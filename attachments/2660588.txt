COCIR Feedback 
Commission proposal for a European Artificial Intelligence Act 
Executive summary 
The proposed Artificial Intelligence Act defines high-risk AI systems so broadly that almost 
all medical device software may be considered a high-risk AI system. The Medical Device 
Regulations, especially in combination with the GDPR, already include an extensive, often 
more detailed, set of requirements related to various aspects of the new Act. However, the 
Act's definitions and requirements are not aligned, and the Act refers to risk and harm in 
complex and inconsistent ways. For specific devices, the Act's requirements conflict with 
the safety and performance requirements of the Medical Device Regulations. These 
misalignments increase complexity, legal uncertainty, and implementation costs, 
ultimately paid for not only by the manufacturers but also healthcare systems and patients. 
Certain requirements may even prevent European patients and citizens’ access to specific 
state-of-the-art digital health innovations. 
Instead, COCIR strongly supports a targeted, sector-specific, and risk-based approach to the 
regulation of artificial intelligence. 
Introduction 
Artificial Intelligence (AI) in healthcare is already a reality. Healthcare providers have 
embedded the technology into their workflows and decision-making processes. ​COCIR collection of Artificial Intelligence Use Cases ​  The 
introduction of AI in healthcare has brought improvements for patients, providers, payers, 
other healthcare stakeholders, and society at large, also in the fight against the COVID-19 
pandemic. ​The vital role of Artificial Intelligence in a time of COVID-19 ​  
It is essential to have a robust regulatory framework that provides certainty to all actors as 
new technologies are being developed and introduced into the market. ​COCIR Analysis on AI in Medical Device Legislation ​  Medical Device 
manufacturers have long-standing experience of operating in a highly regulated sector. 
COCIR members take the responsibility towards end-users, especially patients, very 
seriously, to place safe and performing products on the European market. 
The following sections contain our detailed feedback to the Commission proposal for an 
Artificial Intelligence (AI) Act. ​Artificial intelligence – ethical and legal requirements (europa.eu) ​  We are looking forward to engaging with the EU institutions 
in the upcoming legislative process, together with our technical experts. 
Detailed feedback 
1. 
Scope & definitions 
The proposed risk classification and broad definition of AI system means that any software 
that qualifies as a medical device and that is placed on the market or put into service in its 

own right or as a component of a hardware medical device falls under the scope of the 
Artificial Intelligence Act. As most medical device software requires notified body 
involvement during the conformity assessment, almost all medical device software would 
be considered a high-risk AI system. The definition includes logic and knowledge-based 
approaches which can be interpreted so broadly that essentially any basic software is 
included. Examples of medical software falling in the scope of the proposal range from 
medical device software embedded in electronic thermometers to alert the user when the 
temperature corresponds to fever, blood glucose meters and patient ventilators, migraine 
or asthma episode prediction apps, to medical image analysis software for tumour 
detection. 
COCIR is a strong advocate of a targeted, sector-specific, and risk-based approach to the 
regulation of Artificial Intelligence. The Medical Device Regulations, especially in 
combination with GDPR, already have an extensive, often more detailed, set of 
requirements related to various aspects of the new Act (e.g., risk management, Quality 
Management System, transparency and user information or cybersecurity). 
The proposed Act is largely based on the New Legislative Framework (NLF) and product 
legislation derived from the NLF, including the MDR and IVDR. While we believe that using 
the NLF is appropriate, the AI Act modifies many of the concepts and definitions in ways 
that are contradictory and confusing for manufacturers placing devices on the market that 
will need to comply with several regulations. Examples include the concepts of ‘importer’, 
‘putting into service’, ‘provider’ or ‘user’. 
In addition, the proposed Act does not define risk – a fundamental concept for conformity 
assessment. Instead, its provisions include the term ‘risk’ and the related ‘harm’ in various 
meanings. This causes interpretation issues within the proposed Act and across its 
referenced legislations (see Figure 1), increases the Act’s complexity and implementation 
cost and results in legal uncertainty. 
We see a definite risk of duplications and additional unnecessary administrative burden on 
AI-based medical device software manufacturers. The different definitions and, for some 
applications, conflicting requirements create an incoherent framework, possibly requiring 
two sets of technical documentation rather than one. Furthermore, parallel incident 
reporting channels and communication with different authorities will create inefficient 
information flows. We expect that different technical standards will be harmonised under 
the two legislations and fear that they will likely overlap. Medical device software has 
specific needs. We need to avoid contradictory technical requirements resulting from e.g., 
potentially different risk definitions or conflicts between MDR safety and performance 
requirements and the AI Act’s human oversight requirements. The European Union needs 
to ensure legal certainty and consistency. 
Despite the enormous capacity for healthcare innovation in Europe, we have already noted 
over the last years that other jurisdictions like the United States have become the preferred 
location to first place innovative medical software on the market. We are concerned that 
the additional complexity introduced by the AI Act will accelerate this development. This in 
turn will reduce the European Union’s competitiveness and innovation potential at a global 
level. Ultimately, European patients will then have later access to digital health innovation. 
Recommendation: The Medical Device Regulations should be removed from Annex II 
Section A. In addition, the Act should include a provision to amend the General Safety & 
Performance Requirements (Annex I) of the existing Medical Device Legislations. The 

Medical Device Coordination Group should develop detailed guidance for AI-based medical 
devices to support these new requirements and address any last gaps. 
2. Requirements for high-risk AI systems 
Most of the proposed requirements are appropriate to ensure that only safe and performing 
AI systems are placed on the market. However, some of the requirements require further 
discussion to align with state-of-the-art software development practices and ensure that 
no barriers are created for highly innovative medical software. Some examples of these 
requirements are provided here below. 
Data and data governance (Article 10) 
A clear data governance framework is crucial. However, the requirement to use error-free 
and complete data for training, validation and testing is neither practicable nor desirable if 
testing takes place under real-world conditions. Datasets are often somewhat inaccurate. 
Accuracy consists of trueness (proximity of measurement results to the true value) and 
precision (repeatability or reproducibility of the measurement). Trueness and precision 
represent a certain margin of error. In medical imaging, the image data is reconstructed 
from the values measured by the system’s detectors, and the measurement itself is done 
based on a system setting that is neither 100% right nor wrong but developed by trained 
professionals. The resulting measurements can differ for the same patient at the same 
stage depending on the model of imaging system and the chosen parameters – one can 
never ensure that the data (i.e. the images) is “completely error-free”. We are also highly 
concerned that the proposed wording would completely prevent the use of Real-World 
Data, an area with huge potential for healthcare. 
Recommendation: Article 10 (3) should require that datasets are sufficiently accurate and 
complete to meet the intended purpose rather than ‘error-free and complete’. 
Human oversight (Article 14) 
The concept of human oversight proposed in the AI Act could be interpreted so restrictively 
that there are undesirable ethical consequences. In practice, human oversight could be 
continuous or intermittent or retrospective. High-throughput computing allows systems to 
monitor hundreds of data points simultaneously. The system can act much faster than the 
human brain can let a signal pass from eye to brain to hands. Any device that ultimately 
relies solely or primarily on human attention and oversight cannot possibly keep up with 
the volume and velocity of algorithmic decision-making or will necessarily be outmatched 
by the scale of the problem and hence be insufficient and potentially harmful to patients. 
In some cases, the only effective oversight possible will be before using the device or after 
the use through retrospective periodic performance review for individual patients or patient 
cohorts. A restrictive interpretation could prevent certain applications of Artificial 
Intelligence in healthcare (e.g., AI enabled eye surgery robot​Urias MG, et al. Artificial intelligence, robotics, and eye surgery: Are we overfitted? Int J Retin Vitr. 2019;5;52. ​ ). 
Recommendation: Article 14 (1) should be modified to ensure that human oversight is 
guaranteed where necessary to reduce risks as far as possible and achieve performance in 

consideration of generally acknowledged state-of-the-art and technological/scientific 
progress. 
In addition, ‘full’ understanding would be very difficult to define and implement. As part of 
a risk-based approach, it is important that the software provider makes available 
information that is actionable to the user, as required to ensure the safe and effective use 
of the device. 
Recommendation: Article 14 (4) should require that information be made available, so the 
user sufficiently understands the AI system to ensure intended functioning. 
Medical device manufacturers generally place the human 'in' or 'on the loop.' However, in 
specific applications, limited in number today but expected to grow in the future, the 
variability and limitations introduced by human factors can result in the safety and 
performance of autonomous AI outperforming that of the human-AI team. Several medical 
applications are known today where regulators require the human to be taken out of the 
loop to ensure clinical safety and performance peaks and to avoid user intervention does 
not harm patient outcome (e.g., AI to read certain qPCR curves, AI to segment brain 
tumours, AI-enabled eye surgery robot). For such applications, a requirement for 
continuous human oversight conflicts with the Medical Device Regulation’s requirements 
to reduce risks as far as possible and achieve performance considering generally 
acknowledged state-of-the-art and technological/scientific progress. 
Recommendation: Art. 14.4(e) should be modified to ensure that users be able to intervene 
on the operation of the high-risk AI system or interrupt the system through a ‘stop’ button 
or similar procedure except if human interference increases patient risk and/or reduces 
patient outcome. 
Access to training data (Article 64 & Annex VII Conformity based on assessment of 
quality management system and assessment of technical documentation) 
Requiring that market surveillance authorities and notified bodies get full access to training 
data sets is problematic where (1) providers/manufacturers do not have direct access to the 
training data, i.e., where the training data remains behind security and privacy shields (cf. 
federated learning), (2) copyright or privacy reasons do not allow providers/manufacturers 
to store training data themselves (cf. personalized medicine) and (3) where the quantity of 
training data is so vast that storing it causes a disproportionate cost and impact on the 
environment (cf. GPT-3 which was trained on the entire internet). 
Recommendation: Remove the requirement to provide access to training data. Access to 
testing data sets should be sufficient, especially as testing data sets cover more sources of 
bias than only those caused by training data. 
3. Transitional provisions and period  
Entry into force and application (Article 85) 
As we have experienced in the transition from the Medical Device Directives to the Medical 
Device Regulation, sufficient transition time is crucial for the entire regulatory system, not 
only manufacturers. Putting the necessary infrastructure in place, ensuring compliance 
with the requirements and guidance documents, as well as adequate notified body 
capacity require sufficient time. 
Recommendation: the transitional period should be extended by at least two years (to 48 
months) to allow for all elements to be in place in time before the Date of Application. 

Figure 1 Illustration of risk and related discrepant meanings of harm in the AI Act and referenced legislation 