July 2021 
Proposal for a 
Regulation laying down harmonised rules on artificial 
intelligence (Artificial Intelligence Act) and amending certain 
Union legislative acts (COM/2021/206) 
Position Paper on the EU Commission’s Proposed Act 
Intesa Sanpaolo, one of the top banking groups in Europe, welcomes the opportunity to respond 
to the European Commission’s (EC) Proposed Artificial Intelligence Act (COM/2021/206). This 
position paper is intended to present its key recommendations to the European Commission in 
the context of the have your say procedure on the Artificial Intelligence Act (AIA). 
The paper is divided into the following three sections: (i) general comments on the application 
of the Proposed Act to Financial Services and Codes of conduct; (ii) technical comments on 
the requirements laid down in Title III of the Proposal; and (iii) detailed remarks concerning the 
definitions provided for by Article 3 and some requests for clarification. 
I. 
GENERAL COMMENTS 
Integration into the Capital Requirements Directive 
The conformity assessment procedure and some of the providers’ procedural obligations under the 
AIA Proposal are integrated into the procedures under Directive 2013/36/EU (the Capital 
Requirements Directive, or “CRD”). We request further clarity as to how the interaction with the CRD 
is intended. 
In particular, Recital 80 states that “In order to avoid overlaps, limited derogations should also be 
envisaged in relation to the quality management system of providers and the monitoring obligation 
placed on users of high-risk AI systems to the extent that these apply to credit institutions regulated 
by Directive 2013/36/EU”. 
In our opinion, a clarification is needed concerning what is meant by “limited derogations”, 
since we do not see any other explicit reference to this in the AIA text. If it is referred to Article 
17.3 and 29.4 the wording should be less ambiguous. 
Moreover, we noticed that some provisions, such as Article 19 and 61, only refer to the specific case 
of “creditworthiness and credit scoring” (Annex III.5(b) AIA) while asking for an integration with the 
CRD. However, other provisions, such as Article 9, 17, 18, 20, 29, do not specify this reference, 
suggesting that the integration with Article 74 of the CRD should apply for all the systems listed in 
Annex III (e.g. recruitment) developed/used by Credit Institutions. 
We believe this omission should be remedied by adding the reference to “point 5(b)” in the 
provisions that require an integration with the CRD. Otherwise, the AIA would be extending 
the material scope of the CRD also to non-credit-related use cases. The AIA Regulation could 
introduce derogations from the rule but within the scope of the CRD only. For this reason, in 
our view, all the references to the CRD should be explicitly limited to “creditworthiness and 
credit scoring” to determine access to financial resources. 

Creditworthiness and credit scoring to determine access to financial resources 
We agree with the European Banking Federation response that the scope of this use case is not fully 
clear. Some AI systems, such as the valuation of collateral, are just background tools in the 
creditworthiness assessment process, consequently the risk level of those tools is varied. 
Moreover, it is not very clear whether other financial products, different from consumer credit and/or 
mortgage credit, are also included in the scope of Annex III.5(b). 
In addition, Recital 37 states that creditworthiness and credit scoring are considered as high-risk since 
they determine customers’ access to financial services. 
To ensure legal certainty, we suggest clarifying the concept of creditworthiness assessment 
and credit scoring in the Proposal and to specify which consumer credit product are in scope 
of Annex III.5(b). 
We believe that the reference to “the access” should be also explicitly clarified in the text of 
Annex III.5(b). In other words, we consider that, in line with Recital 37, it should be considered 
as high-risk only those systems used to evaluate the access to credit lending and not the 
system put into service for phases following the initial disbursement of the loan. 
Voluntary codes of conduct for non-high-risk AI systems – Article 69 AIA 
The proposed approach raises strong concerns. Article 69.1 states: “The Commission and the Member 
States shall encourage and facilitate the drawing up codes of conduct intended to foster the 
voluntary application to AI systems other than high-risk AI systems of the requirements set out in Title 
III, Chapter 2 (…)”. 
The requirements for high-risk AI systems are likely to be disruptive applied to non-high-risk AI systems 
and impose disproportionate obligations. The adoption of such codes of conduct could lead to an 
excessive burden that can discourage the adoption of AI systems, since it involves the voluntary 
alignment of non-high-risk systems with high-risk ones, in areas where there is almost no threat for 
fundamental rights. 
We therefore suggest replacing the reference to “Title III, Chapter 2” in the first paragraph of 
Art. 69 with proportionate and tailored requirements for non-high-risk AI systems, agreed on a 
voluntary base. Otherwise, we could consider leaving to stakeholders an open choice on 
how to pursue AI trustworthiness while adopting the codes for non-high-risk AI systems. 
II. 
TECHNICAL COMMENTS 
AI techniques and approaches and amendments to Annex I – Annex I and Article 4 AIA 
The techniques and approaches mentioned in Annex I seem to go far beyond what is commonly 
considered as Artificial Intelligence. Each technique and approach mentioned in letter a, b and c​Annex I AIA “(a) Machine learning approaches, including supervised, unsupervised and reinforcement learning, using a wide 
variety of methods including deep learning; (b) Logic- and knowledge-based approaches, including knowledge 
representation, inductive (logic) programming, knowledge bases, inference and deductive engines, (symbolic) reasoning 
and expert systems; (c) Statistical approaches, Bayesian estimation, search and optimization methods.” ​  
has different characteristics, especially in terms of explainability. Therefore, it seems unrealistic to put 
them at the same level even more when it comes at identifying potential discriminatory practices. In 
the literature about AI, the techniques listed in letter (c) are usually not considered as AI system. ​McCarthy, J. (2007). “What is artificial intelligence”. Russell, Stuart, and Peter Norvig. "Artificial intelligence: a modern 
approach." (2002). Turing, Alan. "Computing machinery and intelligence” (1950). ​  

We suggest the EC to eliminate letter (c) Annex I in order to exclude at least the statistical 
approaches from the definition of AI. 
We appreciate that the EC acknowledges the need to update the list of AI techniques, as set out in 
Article 4. However, the introduction of new techniques should not in our opinion have a retroactive 
effect for the systems already in place, while it should cover the systems under development. 
We recommend the EC to establish a non-retroactive application of the law in order to avoid 
disservices and inefficiencies. 
Data set requirements – Article 10 AIA 
Firstly, we would like to point out that it could be difficult to distinguish between personal and nonpersonal data due to the overall complexity of AI systems. With reference to this, we recall that Article 
2 paragraph 2, Regulation (EU) 2018/1807, as well as “Guidance on the Regulation on a framework 
for the free flow of non-personal data in the European Union”, states that: “(…) in a case of a dataset 
composed of both personal and non-personal data: (i) the Free Flow of Non-Personal Data 
Regulation applies to the non-personal data part of the dataset; (ii) the General Data Protection 
Regulation’s free flow provision 26 applies to the personal data part of the dataset; and (iii) if the 
non-personal data part and the personal data parts are ‘inextricably linked’, the data protection 
rights and obligations stemming from the General Data Protection Regulation fully apply to the whole 
mixed dataset, also when personal data represent only a small part of the dataset 27”. 
Therefore, we encourage the EC to avoid duplications and inconsistencies with the 
abovementioned Regulations. 
Secondly, regarding the requirements provided in Article 10.2(f) and (g), the Commission asks for i) 
examination in view of possible biases and for ii) identification of any possible data gaps or 
shortcomings. These requirements seem to demand providers to avoid bias amplification on one 
side, without giving any practical clue on how to technically achieve this result on the other side. 
These requirements represent a concern in our view, since it is still highly disputed, and somehow 
unsolved, also at scientific level, how to achieving such a result. 
Standards that are now under development on bias mitigation should be shared by the 
Commission. 
Thirdly, Article 10.3 seems to be excessively burdensome, asking for a ‘qualitative’ assessment that is 
very difficult to be implemented. 
The Commission should specify the meaning of “relevant, representative, free of errors and 
complete” as well as the “appropriate statistical properties” and how to concretely execute 
it. 
In particular, we would like to ask the EC to specify the meaning of “free of errors” and 
whether it is referred to ‘data collection and errors recording’ or instead to ‘detection of 
potential biases’. In any case, it is technically almost impossible to have a dataset free of 
errors. 
Moreover, we would like to ask for a clarification on the meaning of “Relevant, representative 
and complete”: for instance, if “representative” is to be intended with respect to protected 
groups/classes of people or with respect to the use case. 
Finally, 
We encourage the EC to clarify the text of the Proposal to help providers to identify which 
data are relevant, representative and how to control errors, following the ‘Privacy by Design’ 
approach, adopted in the General Data Protection Regulation. 
We wonder whether it is feasible to adopt “technical limitations on the re-use”. 
We suggest clarifying Article 10.5 since the provision requirement is not very clear. 

Transparency and provision of information to users and human oversight – Articles 13 and 14 AIA 
Article13.1 asks providers “to enable users to interpret the system’s output”, but no further clarification 
is provided in the AIA text, except for some other prescriptions for transparency (i.e. of disclosure of 
information about features, limits and circumstances that could lead to risk to health, safety or 
fundamental rights). 
Since this is a complex and debated matter, we would like to ask for a clarification on how 
the interpretation of the system’s output should be granted and if there are specific 
techniques that should be preferred to others. 
Human oversight is outlined in the text in five significantly different ways. It is also required that they 
need to be followed “as appropriate to the circumstances”. 
We would like to ask a clarification about the admissible strategies and the cases in which a 
strategy should be preferred over others. Human-oversight could be useful to support the 
monitoring, but it seems unpractical to expect it for every decision made by an AI system. 
III. 
DETAILED REMARKS 
Definitions – Article 3 AIA 
Paragraph 2. The “provider” is defined as a subject “that develops an AI system or that has 
an AI system developed (…)”. The concept of “has an AI system” should be clarified, since it 
is not clear whether it refers to titles, rights, licence, property, etc. 
Paragraph 23. We recommend that Article 3(23) clarifies that ‘supplementary training’ does 
not amount to a “substantial modification”. The current generic definition of “substantial 
modification” could lead to a disproportionate outcome, in particular considering the 
provision under Article 28.1(c). 
Paragraph 36. Regarding the high-risk systems for “Biometric identification and 
categorization”, under Annex III.1(a), we recommend to include a clarification that the use 
of anti-fraud detection system (as used in remote client onboarding techniques or during 
client operation on the on-line channels by collecting information from end-users’ terminal 
equipment, including about its software and hardware), as well as the use of biometric 
functionalities of a mobile device that allows to open an app or authenticate a payment, 
are excluded from the scope of this provision. 
Finally, the definition of “deep fakes” laid down in Article 52.3 should also be included in 
Article 3. 
High-risk AI systems - Classification rules for high-risk AI systems – Article 7 AIA 
The Regulation does not seem to provide for upgrades to the content of Annex II. 
We therefore suggest including in Article 7 the possibility for the Commission to adopt 
delegated acts in accordance with Article 73 to update the Union harmonisation legislation 
listed in Annex II. 
EU Database for Stand-alone high-risk AI Systems – Articles 51 and 60 and Annex VIII 
The Proposal requires providers to disclose “Electronic instructions for use” of the AI system that shall 
be accessible to the public, however the ultimate purpose of this provision is not clear. 
We therefore request further clarity on this provision since it raises concerns from a security 
point of view, as well as in terms of confidentially and commercial sensitivity. 

Reporting of serious incidents and malfunctioning – Article 62 AIA 
The provision under Article 62 is not clear. 
It should be clarified what is meant by breach of obligations protecting fundamental rights. 
Moreover, given the proliferation of regulation at EU level on incident reporting, we ask for an 
alignment, in particular with the Digital Operational Resilience Act, the Network and 
Information Systems 2 Directive and the General Data Protection Regulation. 
Penalties – Article 71 AIA 
The penalty set out in Article 71.3 for the violation of Article 10 on data sets is burdensome, especially 
when compared to the wording of the rule which, as seen above, appears to be truly generic, open 
to different interpretations and extremely difficult to implement. 
Therefore, we propose to delete letter (b) of paragraph 3 from Article 71, so that the violation 
of Article 10 will be addressed in Article 71.4. 
Entry into force and application – Article 85.3(b) AIA 
It should be noted that Penalties (Article 71) should not be applied before the Regulation itself 
applies. Therefore, we suggest deleting the following sentences “Therefore the provisions on penalties 
should apply from (…)” in Recital 88 and “Article 71 shall apply from [twelve months following the 
entry into force of this Regulation]” in Article 85.3. 