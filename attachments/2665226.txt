COMMISSION EUROPEENNE 
PROPOSITION DE RÈGLEMENT DU PARLEMENT EUROPÉEN ET DU CONSEIL ÉTABLISSANT DES 
RÈGLES HARMONISÉES CONCERNANT L’INTELLIGENCE ARTIFICIELLE ET MODIFIANT CERTAINS 
ACTES LÉGISLATIFS DE L’UNION 
L’Association française des Sociétés Financières (ASF) représente les métiers de financement 
spécialisé en matière de crédit ainsi que les services financiers et d’investissement. Les établissements 
membres de l’ASF financent plus de 20 % des crédits au secteur privé. ​Il s’agit des établissements de crédit et les sociétés de financement ayant pour activité le crédit-bail, le crédit à la 
consommation ou encore l’affacturage ou les services de caution, établissements tous régulés et supervisés. ​  
Observations liminaires 
L’Association française des Sociétés Financières (ASF) accueille avec intérêt la consultation publique 
lancée en avril dernier sur le projet de règlement établissant des règles harmonisées concernant 
l’intelligence artificielle. 
Ce projet de règlement est selon nous un point positif dans la mesure où il prend en compte 
l’utilisation des systèmes d’IA dans la relation entre les entreprises et leurs clients. 
Néanmoins, l’ASF rappelle que l’utilisation des données personnelles nécessaires au fonctionnement 
des IA est déjà encadrée quant aux conditions de collecte et finalités de traitement et de 
conservation des données, ainsi qu’à l’exercice des droits des personnes (droit à l’information, droit 
d’opposition, droit d’accès, droit de rectification, …) et ce, afin de protéger la vie privée et les libertés 
des personnes concernées. 
Enfin, des dispositions juridiques interdisent différentes formes de discrimination, de sorte que nous 
disposons déjà d’un arsenal apportant une sécurité juridique dans l’utilisation des IA. 
Définition de l’IA (article 3) 
La définition proposée nous parait trop large car elle intègrerait toutes catégories de score sans 
distinction, y compris ceux destinés à l’analyse préalable à un octroi de crédit. 
En effet, les établissements du secteur bancaire et financier, afin d’optimiser les contraintes 
réglementaires et prudentielles qui pèsent sur eux, sont de plus en plus amenés à une exploitation de 
la donnée « avancée », système qualifié d’IA. 
L’exploitation de données peut avoir plusieurs types de finalité : 
Pour l’efficacité opérationnelle : gestion des méls au service après-vente, de « bots » pour 
les relations clients ou souscription digitalisée, le développement de la fidélité ; 

Pour la gestion du temps nécessaire à la décision d’octroi avec une automatisation 
des contrôles dans la phase d’étude et d’acquisition ; 
Pour la lutte contre la fraude, avec une recherche de modèles de fraude complexes ; 
Pour la gestion des risques par les scores d’octroi, mais aussi les scores au recouvrement. 
Un équilibre doit être recherché entre d’une part, l’adoption d’exigences minimales pour encadrer 
les risques et les problèmes liés à l’IA avec un cadre suffisamment souple et d’autre part, la 
promotion du développement technologique, sans augmentation disproportionnée des coûts de 
mise sur le marché pour les établissements quelle que soit leur taille. 
Systèmes d’IA interdites (article 5) 
Le texte prévoit l’interdiction de l’utilisation de systèmes d’IA susceptibles d’exploiter les éventuelles 
vulnérabilités dues à l’âge ou au handicap physique ou mental d’un groupe de personnes donné pour 
altérer substantiellement le comportement d’un membre de ce groupe d’une manière qui cause ou 
est susceptible de causer un préjudice physique ou psychologique à cette personne ou à un tiers. 
Cette définition particulièrement large pourrait viser indirectement les scores de crédit qui utilisent 
le critère de l’âge parmi d’autres critères et au même titre que ces autres critères. 
Ces pratiques d’IA interdites devraient être plus précisément définies. 
Classification de systèmes d’IA comme systèmes à haut risque (annexe III) 
Parmi les IA à risque élevé (annexe III), on relève les IA dans les domaines suivants : l’identification 
biométrique et la catégorisation des personnes physiques (les IA utilisées en temps réel et à distance 
permettant une identification des personnes physiques sont dans le périmètre), l’emploi, la gestion 
des salariés et l’accès au travail indépendant, l’accès aux services privés essentiels et aux services 
publics. 
Les IA utilisées pour évaluer la solvabilité des personnes physiques ou établir leur score de crédit 
sont dans cette catégorie, ce qui nous paraît contestable. 
Les scores de crédit 
Les établissements spécialisés en France les utilisent depuis de nombreuses années en vue de 
répondre notamment à un triple enjeu : 
respecter les règles de solvabilité auxquelles les établissements spécialisés sont assujettis ; 
évaluer la solvabilité de l’emprunteur ; 
élaborer leur modèle interne. 
Dans ce cadre, ils recourent aux IA pour l’élaboration, l’actualisation et l’utilisation de modèles de 
score pour l’attribution de crédit, ce qui permet de : 
mesurer le risque statistique de défaut de remboursement qui correspond aux demandes de 
crédit (ou de moyen de paiement adossé à un contrat de crédit) présentées par des clients 
personnes physiques ; 
apporter une aide à la sélection des demandes dont le niveau de risque, ainsi évalué, permet 
la conclusion d’un contrat de crédit ; 
constituer des modèles de score ; 

vérifier la pertinence des modèles de score mis en œuvre et leur actualisation ; 
évaluer le risque de défaut de remboursement qui est attaché à chaque demande de crédit 
en vue d’apporter une aide à l’instruction de la demande. 
L’encadrement des usages de l’IA dans le secteur bancaire et financier 
L’IA est un moyen et non une fin. Seuls les usages de l’IA doivent être encadrés. Ce n’est pas parce 
que la donnée collectée et utilisée est sensible que le risque est nécessairement élevé dès lors que 
son usage est strictement encadré. La qualification de « haut risque » se définit en fonction de 
l’usage qui est fait de l’IA par l’utilisateur. 
Les usages sont souvent déjà encadrés par des réglementations spécifiques. 
Pour les services bancaires et financiers, la réglementation prudentielle définit déjà le processus de 
contrôle et d’évaluation des systèmes d’octroi. Cette réglementation limite les risques liés à l’IA dans 
le domaine. 
Les algorithmes d’octroi sont strictement régis par un principe de non-discrimination (avec 
en France, des sanctions pénales) qui interdit de les fonder uniquement sur un critère de 
discrimination prohibé (âge, nationalité, handicap…), ou de les contourner en donnant plus 
d’importance à l’un de ces critères par rapport aux autres. Un seul critère ne peut à lui seul 
emporter la décision d’octroi du crédit. 
Ces algorithmes exempts de discrimination, sont exempts de risques pour le client en 
fonction de l’usage qui en est fait. Cet usage est l’évaluation de la solvabilité du candidat à 
l’emprunt qui lui permet d’éviter le risque d’endettement excessif au regard du crédit 
envisagé. L’objectif de l’utilisation du score est donc bien de limiter le risque financier pour 
le client comme pour le prêteur. 
Le Comité de Bâle impose aux établissements bancaires et financiers des contraintes 
prudentielles. Pour estimer leurs besoins en fonds propres associés aux créances, ces 
établissements peuvent avoir recours aux IA. 
Par ailleurs, les usages et les procédures de monitoring sont déjà largement encadrés par un 
corpus réglementaire (exigence de représentativité, d’exhaustivité des données, de 
monitoring des éventuels contournement, définies notamment par les orientations de l’EBA 
sur l’octroi et le suivi des prêts « Guidelines on loan origination and monitoring » - 
EBA/GL/2020/06 du 29 mai 2020.) 
La qualification de ce type d’IA en « IA à haut risque » nous semble inadaptée et disproportionnée 
au regard des réglementations déjà existantes et des obligations à respecter. Ce type d’IA est par 
ailleurs utilisé par les établissements bancaires et financiers depuis un grand nombre d’années. Ces 
algorithmes ne devraient donc pas entrer dans la catégorie des modèles à risque élevés. 
Renvoi à des règles sectorielles 
Dans certains cas, l’évaluation de la conformité est réalisée dans le cadre de la procédure de 
la directive 2013/36 sur le processus de contrôle et d’évaluation prudentiels. 
Pour éviter une double règlementation qui risquerait de générer des difficultés d’articulation, l’ASF 
préconise d’exclure du champ de la proposition de règlement les IA utilisées par le secteur bancaire 
et financier, les spécificités de ces IA étant déjà prises en compte. Le texte devrait prendre davantage 
en compte la réglementation applicable en matière bancaire et financière. 

Exigences applicables aux systèmes d’IA à haut risque / Obligations des fournisseurs et des 
utilisateurs (articles 8 à 15) et obligation de transparence (article 52) 
S’agissant de la transparence et de la fourniture d’informations, l’article 13 prévoit que la conception 
et le développement des systèmes d’IA à haut risque sont tels que le fonctionnement de ces systèmes 
est suffisamment transparent pour permettre aux utilisateurs d’interpréter les résultats du système 
et de l’utiliser de manière appropriée. Un type et un niveau adéquats de transparence permettent de 
veiller au respect des obligations pertinentes incombant à l’utilisateur et au fournisseur. 
Ces exigences nous semblent redondantes avec la règlementation bancaire et financière. En effet, 
les orientations de l’EBA sur l’octroi et le suivi des prêts (« Guidelines on loan origination and 
monitoring » - EBA/GL/2020/06 du 29 mai 2020), prévoient déjà un cadre de gouvernance et 
d’exigence en ce qui concerne l’utilisation de scores de crédit pour évaluer la solvabilité des 
emprunteurs. 
Par ailleurs, cette exigence de transparence dans le fonctionnement de l’IA fait doublon avec 
le RGPD. 
Le RGPD et les textes français imposent en cas d’utilisation de scores de crédit, notamment pour 
évaluer la solvabilité des consommateurs candidats au crédit, de permettre à ces derniers de : 
de demander et d’obtenir une intervention humaine de la part du prêteur pour réexaminer 
la décision ; 
de demander et d’obtenir du prêteur une explication claire de l’évaluation de la solvabilité 
réalisée, notamment de la logique et des risques associés au traitement automatisé 
des données à caractère personnel, ainsi que sa signification et ses effets sur la décision ; 
d'exprimer leur point de vue et de contester l’évaluation de la solvabilité et la décision. 
En outre, les établissements financiers sont assujettis à des règles de lutte contre la fraude, 
le blanchiment de capitaux et le financement du terrorisme. Les IA permettent de générer des alertes 
qui font l’objet d’une analyse, assurant ainsi une fluidité et une sécurisation du processus de 
traitement manuel. Le bénéfice de ces systèmes a été démontré en termes d’efficacité et de rapidité. 
L’ASF souhaite que les obligations de transparence en matière d’IA n’aient pas pour effet de lever 
le secret des affaires et d’entraver le respect des obligations de lutte contre la fraude et le 
blanchiment et le financement du terrorisme pour les établissements. C’est un point crucial car l’IA 
est également utilisée dans ces domaines. ​https://acpr.banquefrance.fr/sites/default/files/medias/documents/20200612_gouvernance_evaluation_ia.pdf ​  
Enfin, l’articulation des obligations et des responsabilités entre fournisseurs et utilisateurs des 
systèmes d’IA pourrait être améliorée. Dans certains cas, le fournisseur et l’utilisateur sont une même 
personne, il convient donc dans ce cas de ne pas dupliquer les obligations applicables. 
Robustesse et cybersécurité (article 15) 
La sécurité informatique constitue un enjeu majeur pour le secteur bancaire et financier. Ce domaine 
est déjà très réglementé au niveau européen (Directive 2013/36/UE, DSP2, NIS). 

L’ASF préconise l’application de ces textes plus adaptés au secteur bancaire et financier afin d’éviter 
le « doublonnage de règlementations ». 
Autorité en charge du contrôle (article 59) 
Des autorités nationales compétentes seraient établies ou désignées par chaque État membre aux fins 
d’assurer l’application et la mise en œuvre du règlement. Les autorités nationales compétentes 
seraient organisées de manière à garantir l’objectivité et l’impartialité de leurs activités et de leurs 
tâches. Chaque État membre désignerait une autorité de contrôle nationale parmi les autorités 
nationales compétentes. L’autorité de contrôle nationale agirait notamment en qualité d’autorité de 
surveillance du marché. 
Dans le domaine bancaire et financier aux niveaux européen et national, plusieurs autorités sont 
d’ores et déjà amenées à intervenir. L’ASF s’interroge sur l’articulation entre ces autorités en 
matière de contrôle et d’incident (autorité compétente en matière de protection des données, 
autorité compétente pour se conformer à la Directive concernant les services de paiement 2, 
autorité compétente en matière de sécurité dans le cadre de la directive NIS notamment). 
Sanctions (articles 71 et suivants) 
L’ASF estime que les sanctions prévues sont trop élevées et pourraient avoir pour effet de freiner 
l’innovation. Un équilibre devrait être trouvé d’autant qu’il existe un problème de cumul de 
sanctions pour certains acteurs. 
Application des règles 
Aucune précision n’est apportée quant à l’application dans le temps de ce texte aux IA déjà mises en 
œuvre au sein des Etats membres (par exemple pour l’exigence d’une certification). 
ASF le 2 juillet 2021 