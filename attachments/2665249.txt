Bld. du Roi Albert II 5, 1210 Brussels | +32 2 226.00.50 
info@industriall-europe.eu | www.industriall-europe.eu 
On 21 April 2021, the European Commission presented its Proposal For A Regulation Of The European 
Parliament And Of The Council Laying Down Harmonised Rules On Artificial Intelligence (Artificial 
Intelligence Act) And Amending Certain Union Legislative Acts. IndustriAll Europe welcomes the European 
Commission’s initiative, as it is the first proposal of its kind and will set new standards when it comes to 
addressing the challenges of high-end technology and human rights implications. We appreciate the 
opportunity to provide feedback on the draft regulation. 
Although we share doubts, with other trade unions and civil rights organisations, that the risk-based 
approach is fit for purpose, and although we think that a rights-based approach to the Regulation would 
have been preferable, we acknowledge that the risk-based approach is thoroughly applied in the proposal. 
In that sense, we welcome that “AI systems used in education or vocational training, notably for 
determining access or assigning persons to educational and vocational training institutions, or to evaluate 
persons on tests as part of or as a precondition for their education should be considered high-risk (...)” 
(35). We further welcome that “AI systems used in employment, workers management and access to selfemployment, notably for the recruitment and selection of persons, for making decisions on promotion and 
termination and for task allocation, monitoring or evaluation of persons in work-related contractual 
relationships, should also be classified as high-risk (...)” (36). 
The current proposal, however, does not take into consideration the impact of AI on workers rights and 
the need to anticipate change. Human rights considerations in the wider sense are also neglected. The 
potential adverse impact of Artificial Intelligence and Machine Learning systems on the environment is 
missing altogether. The current proposal should therefore be complemented by additional and tailored 
Regulations that can address the gaps that pose risks to those who are subjected to AI/ML systems. 
Overview 
IndustriAll Europe appreciates that the draft Regulation is more ‘hands-on’ than the White Paper and 
follows a horizontal approach. This is more applicable than specific sectoral legislations, which would open 
the door to inconsistencies and loopholes, and which would probably cause controversies that can now 
be prevented. The European Union rightfully aims at leading the global race for AI, for technological 
development and for its regulation. But this must under any circumstances be accompanied by a strong 
industrial base which keeps pace with the technological developments. This requires a reliable 
IndustriAll Europe’s contribution to the public consultation on the 
Brussels, 03 August 2021 
What we like about the proposal 

commitment to high-skilled labour and high value-added industries as the core of the European economic 
model. 
IndustriAll Europe further welcomes that the underlying message of the draft Regulation is clear: that 
European values should be at the core of this Regulation and that not everything that is technically feasible 
should be allowed. We therefore specifically welcome the prohibition of certain AI practices and 
applications, as well as the introduction of mandatory transparency measures. We further welcome the 
proposed EU database on stand-alone, high-risk AI systems, which should be publicly accessible. This will 
contribute to foster trust in AI technologies and make the technology more transparent and reliable. 
IndustriAll Europe’s assessment: 
IndustriAll Europe believes that, even though the draft Regulation is quite comprehensive, there are still 
a number of points that should be made more precise, corrected, or added altogether. 
First of all, we are of the opinion that the proposal is full of loopholes and exceptions that should 
be closed. Too many vague formulations and definitions leave too much room for interpretation. 
Relevant categories when discussing data are not addressed at all, namely that of ‘inclusivity’, 
‘non-discrimination’ and ‘fairness’. An ambitious landmark legislation, such as the current 
proposal, should deal with all relevant categories that the subject it regulates touches upon. 
Secondly, we think that the definition of ‘high-risk’ AI is too narrow and ignores too many use 
cases which affect workers and citizens in their everyday life. Annex III, which lists high-risk 
applications, reads “AI systems intended to be used for...” which, again, leaves too much room for 
interpretation. In addition to the used wording, the different risk categories are too broad. A fully 
differentiated risk pyramid with more risk layers would be helpful to regulate the different types 
of applications in question in a more application-oriented manner. One example for a more 
granular approach has been cited already in the EU Commission’s White Paper on AI (COM(2020) 
65 final) when referring to the five-level risk-based system as proposed by the German Data Ethics 
Commission. We would like to underline, however, that the risk-based approach is not fit for 
purpose and that a rights-based approach to the Regulation would have been preferable as this 
would allow for tailored regulations for AI/ML applications.  
Thirdly, it is highly problematic that the definition of ‘unacceptable’ AI seems to be final, with no 
mechanism in place to introduce new kinds of ‘unacceptable’ AI to the list. This would, in the worst 
case, lead to a situation in which new types of harmful AI are being developed without an adequate 
legal mechanism in place to prohibit its use or it being put on the market. There is also no provision 
in place to prevent AI/ML applications from developing features that would be considered 
‘unacceptable’, and it is not clear how those applications should be dealt with. This adds to our 
observation that the language used is often insufficient. Terms such as “disproportionate” or 
“unjustified” to describe “detrimental treatment” in the context of social scoring are not fit to 
contribute to a robust regulatory framework. Instead, legal terms that clearly indicate scope and 
intention of the prohibition should be introduced. 
We criticise the fact that the ban on remote biometric identification systems is only halfhearted, 
and only for law enforcement purposes. And even this already narrow ban contains a number of 
problematic loopholes, as it leaves wide discretionary power to the authorities, i.e. by including 
What we criticise about the proposal 

the possibility to seek authorisation for the use of such systems ex-post. The exception to deploy 
remote biometric identification to prevent ‘terrorist attacks’ will most certainly be misused by 
authoritarian member states, i.e. to supress strikes and protests, and to control social movements. 
As a minimum requirement, remote biometric identification systems should therefore be 
prohibited for all public authorities and private actors working on behalf of public authorities. Yet, 
together with numerous other organisations “we call for an outright ban on uses of facial 
recognition and remote biometric recognition technologies that enable mass surveillance and 
discriminatory targeted surveillance” and have signed the open letter initiated by AccessNow. ​https://www.accessnow.org/cms/assets/uploads/2021/06/BanBS-Statement-English.pdf ​  
We think that the challenges of bias are not adequately addressed. High quality data sets alone 
cannot prevent discriminatory bias and unfair outcomes of algorithmic decision making. 
Technological solutions will not be enough to guarantee bias-free AI, but it will need robust sociotechnical processes to help to tackle discriminatory practices. In that sense, Article 10, which states 
that “training validation and testing data sets shall be relevant, representative, free of errors and 
complete. They shall have the appropriate statistical properties, including, where applicable, as 
regards the persons or groups of persons on which the high-risk AI system is intended to be used” 
should be made more precise and amended by a definition of what “representative”, “free of 
errors” and “complete” actually mean. The mere referral to the “appropriate statistical 
properties” is obviously insufficient. The general rule - especially for data sets used at the 
workplace and in the context of HR - could be defined by the Regulation, but the concrete design 
could be agreed upon by the management and the works council, or the trade union. The same is 
true for provision (45). Trade unions should be considered as ‘certain actors’ and “should be able 
to access and use high quality datasets within their respective fields of activities which are related 
to this Regulation.” Works councils should be provided with the means to hire software engineers 
to support them in their analyses of the AI/ML systems and to support them in the analysis of the 
algorithms and the inherent biases. 
From a trade union perspective, one of the most pressing issue is that the draft Regulation only 
touches on the deployment of AI at the workplace. It is certainly most welcome that AI systems 
used for recruiting and to determine access to social benefits, among others, are considered to be 
of ‘high-risk’. The Regulation, however, does not entail a comprehensive section on employment 
which would be able to cope with this important field. The current proposal risks leaving many 
areas unregulated and opens too many doors to proliferation and malpractice. In that sense, it is 
also highly regrettable that the proposal does not address the questions of liability and redress, 
especially since the definition of relevant categories, such as ‘adverse impact’ and ‘harm’ remains 
vague. The reference to the revision of the Product Liability Directive, which is due later this year, 
is admittedly justified, but it would still make sense to set a number of fundamental principles 
already in the Regulation on AI, as too many questions on liability in the context of AI have been 
open for too long. In the same way, it is regrettable that there is no mechanism addressed through 
which the decisions of algorithms, at least at the workplace, could be contested. In that sense, the 
issues of collective bargaining and the important role it plays upon addressing technological 
change, a safe and trustworthy work environment and quality employment, should be included in 
the Regulation – in the absence of a stand-alone Regulation on AI at the workplace. 

The proposal is built around an overly large reliance on industry self-assessment, which is clearly 
an inadequate approach in the context of high-risk uses of AI technology. A mandatory third-party 
assessment subject to harmonised assessment procedures for all high-risk AI applications, and 
certainly all AI applications dealing with sensitive personal data, should instead be installed to 
guarantee full compliance of the technology with fundamental and privacy rights. The current 
proposal risks that dangerous applications that are initially classified as ‘low-risk’ are not subject 
to proper oversight as the combination of an industry self-assessment and a weak mechanism to 
retroactively add applications to the ‘high-risk’ list is error-prone. A mandatory human rights 
impact assessment for every AI application would be the more feasible approach to guarantee 
trustworthy and reliable AI.  
The definition of ‘low-risk’ AI is too broad and does not allow for increments. AI applications, such 
as chatbots, biometric categorisation systems and emotional recognition systems, are all treated 
alike, although they have dramatically different impacts on the individual, and the latter ones are 
especially highly unreliable. They should therefore be subject to a fully differentiated legal 
framework. A fully differentiated risk pyramid with more risk layers would, again, be helpful to 
regulate the different types of AI/ML applications in a more application-oriented manner. 
We doubt that the proposed enforcement mechanism is fit to contribute to a sound and 
harmonised deployment of AI across the member states. It is highly unlikely that all member states 
will have the sufficient resources and competences at their command to contribute to a consistent 
implementation of the Regulation at national level. As the draft Regulation foresees “1 to 25 fulltime positions” for the ‘national supvervisory authority’, it is obvious that the quality of the 
implementation of the Regulation will be subject to the priorities that are given to it by the national 
governments. This puts a harmonised regulation at risk. It is also likely that the competencies of 
the ‘national supervisory authority’ will overlap with those of other authorities, such as the 
national data protection authorities. The role and competency of the ‘national supervisory 
authority’, as well as the appointment procedures, should therefore be further clarified. 
We welcome the proposed creation of a new European Artificial Intelligence Board (EAIB), yet, we 
think that it is highly problematic that there is no clear nomination procedure for the national 
authorities in place which will represent the member states on the Board. This might, again, lead 
to a situation where the national data protection authorities are dislodged. We further 
recommend that social partners should be full members of the EAIB, especially since the 
workplace is very much neglected in the proposed Regulation. 
We would like to emphasise that such an ambitious proposal can only be a success if it is properly 
funded. Artificial Intelligence and Machine Learning become more and more common, and it is no 
longer only a few large companies which heavily invest in AI/ML development, but often SMEs and 
start-ups, which develop new fields and technologies. These smaller companies will very likely lack 
the means to ensure that their business model and technologies play by the rules that are included 
in the draft Regulation. To prevent companies from having to drop out, the EU should provide 
SMEs especially with significant resources to help them develop sound and trustworthy AI which 
complies with the rules. Regulatory sandboxes are obviously an important step in that direction, 
but it should be made sure that companies have area-wide and easy access to ensure that 
technological development is not hampered. 

What we would propose to change in the draft Regulation 
In the light of the above-mentioned points, industriAll Europe suggests the inclusion of the following points 
in the AI Regulation: 
A comprehensive chapter on AI at the workplace in the Regulation, or a stand-alone Regulation on 
AI at the workplace, which should be drafted upon consulting cross industry social partners as well 
as sectoral social partners; 
Any chapter on AI at the workplace must include the roles of trade unions and works councils, as 
well as that of collective bargaining. In the absence of a stand-alone Regulation on AI at the 
workplace, or of a dedicated chapter, we suggest the inclusion of at least the following provisions: 
o Article 14 “Human oversight”: The Article rightfully refers to a set of capabilities that the 
person to whom ”human oversight” is assigned should have at their command. Trade 
unions and/or works councils must be involved in the selection of the people who have to 
perform the “human oversight”, as well as in the definition of the criteria that his person 
has to fulfill. The persons who shall be responsible to perform the human oversight should 
receive the necessary training and they should be protected from negative consequences 
when they carry out their duty.  
o Article 69 “Codes of conduct”: Trade unions should be involved in the drafting of those 
“Codes of conduct”, and they should be mandated by collective bargaining where 
applicable. 
o Article 61 “Post-market monitoring by providers and post-market monitoring plan for 
high-risk AI systems” and Article 62 “Reporting of serious incidents and of 
malfunctioning”: Trade unions and works councils should be involved in the monitoring 
and reporting procedures and also have a comprehensive overview of those processes. 
Workers should under all circumstances be involved in the processes of introducing new 
technology to the workplace and to conduct impact assessments. Shop stewards should 
be provided with the necessary training and competences to engage in these processes. 
This could be included in Article 13 “Transparency and provision of information to users”. 
o It should be clarified that trade unions and/or works councils must be involved in the 
introduction of Artificial Intelligence/Machine Learning applications at the workplace, and 
collective agreements, at least for high-risk applications, should be mandatory. If the 
company wishes to introduce a high-risk AI/ML system, the management should approach 
the competent public authority and explain their intention. In the absence of a works 
council and/or trade union, both parties (the management and competent public 
authority) would need to sign an agreement that acts in place of the collective agreement. 
This formal agreement can be replaced by a collective agreement at a later stage, if a 
works council or a trade union is established later on; 
o Algorithmic Decision Making systems which have the capability to end a work contract or 
to conduct other types of human resource management which negativley affect the 
employment relationship should be prohibited; 
o Performance control systems should only be allowed under a collective agreement. In the 
absence of a works council and/or a trade union which could sign such a collective 

agreement, a competent public authority should, again, be the contracting party. Part of 
such an agreement should be:  
(a) the nature of the data being collected on workers, the frequency of its 
collection and the duration of its storage; 
(b) the explicit algorithms or the machine-learning system used to process this 
data; 
(c) the metrics used to evaluate work and the performance values required from 
workers; 
(d) the teaching data, its biases and the means implemented to overcome them; 
(e) the reliability and accuracy statistics of any implemented machine learning 
system; 
(f) the acceptable means to supervise work and to detect, store and process 
circumstances of non-compliance with work prescriptions; 
(g) the procedeures for workers or their representatives to detect errors or unfair 
treatment in this automated processing, report them and obtain redress. 
o Every worker should be aware of the exact nature of such a system monitoring their 
performance, and of the parameters used to evaluate them; 
o Works councils should be provided with the means to hire software engineers to support 
them in their analyses of the AI/ML systems; 
o Explainability must be guaranteed by using a language that is understood by those 
subjected to AI/ML systems; 
o Consent to the processing of worker-related data should only be given collectively; 
individual consent should not be considered sufficient in a situation of employment or of 
dependent work; 
o It should be clarified that platform workers should also be covered by the relevant 
collective agreements.  
Introduce a provision to include new types of ‘unacceptable’ AI to the list included under Title II of 
the draft Regulation; 
Clearly define how AI applications that have been falsely labelled as ‘low-risk’ can be re-classified; 
Clearly define the questions of liability and redress in accidents and incidents involving AI systems. 
The current general rule, whereby the employer is by default liable for any accident in the 
workplace (in the absence of any wrongdoing by the worker) should remain; 
Ban all types of emotional recognition software, as they are highly unreliable and their outcomes 
have the potential to be more harmful than helpful; 
Any AI application dealing with personal data, with workers’ data and/or which affects working 
conditions should be classified as ‘high-risk’ and subject to a third-party conformity assessment. 
Collective agreements should be fostered to further regulate the processing of personal data in 
the employment context (similar to Article 88 of the GDPR); 
Mandate third-party assessment for all high-risk AI applications, instead of self-assessment 
procedures; 
Provide for independent and competent notified bodies at national level assigned with conducting 
the third-party assessment; these should be capable to advise the user, to test and examine the 
AI/ML application and to accept complaints. Those notified bodies should be equipped with 

sufficient means to timely and thoroughly meet their responsibilities (similar to the provisions 
discussed in the proposal for a Regulation on Machinery Products, Article 28); 
Address clearly the fact that regulatory sandboxes (Article 53) are a tool to help develop technical 
solutions in the narrow sense. AI systems operating in the employment context should not be 
allowed to run in regulatory sandboxes, as their development needs a broad and interdisciplinary 
approach in which social and racial, as well as gender aspects, are taken into account. Regulatory 
sandboxes will not be fit for such a purpose; 
Address ‘bias’ in more detail, and clearly define categories such as ‘representative’ or ‘complete’ 
when discussing data sets; 
Address the challenges of AI literacy and how the necessary digital skills can be acquired; 
Clearly define “intended use” when discussing high-risk AI applications, e.g. in Annex III, or find a 
more suitable wording, i.e. when discussing “AI systems intended to be used for...”, as the current 
formulation creates too many loopholes; this revised wording should also take into account that 
the “intended” use may evolve over time and that the initially “intended” use does not prevent 
data from being used for other purposes. The nature of the data (potentially) being collected 
should therefore be taken into account as well, and any impact assessment should take the 
potential further development of the system into account; 
Include a provision that makes sure that the right of workers to information, consultation and 
participation is respected on the introduction of any kind of AI at the workplace: there must be 
‘nothing about us, without us’; 
Clearly define and strengthen the role of the European Artificial Intelligence Board, and invite 
social partners to join the Board as full members.  