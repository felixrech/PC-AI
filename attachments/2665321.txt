Position Paper 
of the German Insurance Association 
ID Number 6437280268-55 
on the Proposal of the European Commission 
for a Regulation laying down harmonised rules on 
artificial intelligence 
(Artificial Intelligence Act) 
Introduction 
The insurance industry welcomes the proposed Regulation of the EU Commission on “artificial intelligence”. The EU Commission’s objective to increase people’s trust in AI is to be supported. Only when people trust in the 
use of AI, will artificial intelligence be able to realise its innovation potential. 
The risk-based approach chosen by the EU Commission is the right approach in this context. 
The objective must be to create a legal framework in which trustworthy and 
human-centric AI will contribute to prosperity, innovation, and economic 
growth within Europe. It is therefore of utmost importance to strike the right 
balance between the mitigation of risks on the one hand and the encouragement of technological innovation on the other hand. 
An innovation-friendly single market for AI will enable European companies 
to be globally competitive, and thus will make a significant contribution to 
growth and employment in the European economy. AI can be designed 
Gesamtverband der Deutschen 
Versicherungswirtschaft e. V. 
German Insurance Association 
Wilhelmstraße 43 / 43 G, 10117 Berlin 
P.O. Box 08 02 64, 10002 Berlin 
Phone: +49 30 2020-5000 
Fax: +49 30 2020-6000 
Rue du Champs de Mars 23 
B - 1050 Brussels 
Phone: +32 2 28247-30 
Fax: +49 30 2020-6140 
ID number 6437280268-55 
Contact: 
Legal Affairs / Compliance / Consumer 
Protection; European Office 
E-Mail: recht@gdv.de; 
bruessel@gdv.de 
www.gdv.de 

such that it complies with European standards so that both goals, consumer 
protection and innovation and societal progress, can be reconciled. 
Striking the right balance in regulating AI is indeed something of a balancing 
act: unacceptable risks associated with the use of AI need to be prevented 
while desirable innovation must not be stifled by the regulation. The future 
Regulation should therefore follow the principle of “as much as necessary but as little as possible”. Any regulation of non-high-risk AI applications, disproportionate requirements for AI applications and the consequential overregulation would result in a weakening of research and innovation in the EU, and thus in a weakening of European growth potential and 
international competitiveness of European companies. The consequences 
would be significant given that the draft Regulation is intended to apply 
across all economic and industrial sectors. 
In detail: 
Risk-based approach is welcomed 
The insurance industry welcomes the risk-based approach. Differentiation 
according to the risk potential of the AI application is the right strategy. Taking account of the potential for harm makes sure that only applications for 
which the existing regulations do not provide sufficient protection will be 
subject to additional regulation. 
This approach, however, is not adequately reflected in the actual wording 
of the provisions. As a result, several non-high-risk AI applications would be 
classified as high-risk systems. This applies to applications in the area of 
“employment, workers management and access to self-employment” (Annex III, No. 4), for instance. Insurance companies are also among those 
using AI systems for the purpose of task allocation (Annex III, No. 4(b)), 
which do not pose any risk of harm at all. They solely serve to increase the 
efficiency of procedures and to reduce costs and, as a result, to enhance 
customer satisfaction. With regard to the interplay of the broad definition 
and the list of high-risk AI applications, in particular, this can lead to a significant extension of the scope of application of the Regulation. For instance, according to the current definition, companies’ own job portals 
would also be classified as high-risk AI systems even though they only have 
a notification function on vacant positions which comply with the criteria previously set by the individual user (location, job description, seniority level, 
etc.). 

For the Commission to achieve its declared and desirable objective “to create a legal framework that is innovation-friendly, future-proof and resilient 
to disruption (…) without unduly constraining or hindering technological development or otherwise disproportionately increasing the cost of placing AI 
solutions on the market”, not only the general area of a system’s application 
should be considered, but also on an individual level its specific purpose. 
For this the criteria of Article 7 (2) should be applied. Furthermore, it should 
be stated clearly that the AI applications already referred to in Annex III also 
need to fulfil the conditions set out in Article 7(1) to be classified as high 
risk. In addition to the requirement that the application is used in one of the 
areas specified in Article 7(1)(a), it is required that the respective application 
represents one of the risks mentioned in Article 7(1)(b). Only then is it a 
high-risk AI application. In terms of substance, this also already follows from 
the reference made in recital 32. 
Before new provisions are being adopted, it should be noted that a comprehensive legal framework which regulates the business of insurance companies and also provides for adequate consumer protection when AI is being 
used does already apply. This includes, amongst others, the Solvency II 
Framework Directive, the Insurance Distribution Directive, the GDPR, the 
Product Liability Directive, anti-discrimination directives, and the Unfair 
Commercial Practices Directive as well as numerous laws adopted by the 
EU Member States. The Act against Unfair Competition (UWG, Gesetz 
gegen den unlauteren Wettbewerb) and the General Act on Equal Treatment (AGG, Allgemeines Gleichbehandlungsgesetz), for instance, provide 
for the protection of consumers against discrimination in Germany. Consumers’ right to information is ensured by the EU General Data Protection 
Regulation (GDPR). For instance, pursuant to Article 13(2)(f) GDPR, consumers shall be informed about the use of fully automated decision-making, 
including meaningful information about the logic involved, as well as the 
significance and consequences of such processing. The principles of data 
minimisation and purpose limitation together with accountability make sure 
that only demonstrably required data can be processed for legitimate purposes. Article 22(3) GDPR provides protection against possible discrimination in the event of automated individual decision-making. Moreover, Article 
5(1)(d) GDPR prohibits the use of inaccurate data. 
With regard to an additional regulation of AI, an evidence-based approach 
is required which helps to carefully review the existing regulation and identify potential shortcomings in the use of AI and the related risks. Only where 
risks for individual persons or the society are not sufficiently covered by 
existing legislation, should it be supplemented by a risk-based legal framework. 

List of high-risk applications 
According to the proposed Regulation, certain AI systems (so-called “standalone AI systems”) are, based on their risk potential, classified as high-risk 
applications and are listed in Annex III. The specific requirements for highrisk applications should apply to these AI applications. This shall mean AI 
applications which, in the light of their intended purpose, pose a high risk to 
the health and safety or the fundamental rights of persons. Both the severity 
of the possible harm and its probability of occurrence as well as the area in 
which the AI applications are used should be taken into account in this context. Future changes to the list can also be made based on these criteria. 
This objective method is welcomed. Specified and verifiable criteria provide 
legal certainty to both consumers and companies. The principle of proportionality, which takes potential impacts as well as the probability of risks into 
account, is explicitly welcomed. It is essential that these standards are being complied with when extending the list in the future. 
The Proposal provides for an extension of the list of high-risk applications 
through delegated acts adopted by the EU Commission. This is to be 
viewed critically, in particular with regard to the broad AI definition. Even 
though the power to make amendments through delegated acts is limited to 
the Annexes to the Regulation, the provisions provided therein are essential. 
Furthermore, in the current regulatory Proposal, different time intervals are 
being provided with regard to the review of the list pursuant to Article 84(1) 
and the review of the Regulation itself pursuant to Article 84(2). As a result, 
a dynamic emerges where it is possible that amendments are made to the 
applicable Regulation at least once a year. In addition, there are the effects 
of Level 2 and Level 3 legislation as well as the work of the European Board 
for AI. This comprehensive and dynamic regulation might lead to legal uncertainty. Small and medium-sized companies, in particular, may incur high 
consulting fees if the necessary expertise in dealing with the AI Regulation 
needs to be brought in from outside. In order to counteract these effects, to 
provide companies with the necessary certainty to make plans and not to 
undermine incentives for investments in AI, the review of the list pursuant 
to Article 84(1) should therefore be integrated into the review of the Regulation pursuant to Article 84(2) and take place every four years only. 

Proportionate requirements tailored to the respective high-risk AI application 
To make sure that desirable innovation will not be impeded, the requirements should also be proportionate with regard to high-risk AI applications. 
The provision stipulated under Article 8(2), which provides for taking into 
account the intended purpose of the high-risk AI system as well as the risk 
management system, goes into the right direction in this context. The requirements cannot be the same for every AI application, but should instead 
take account of the specificities and the needs for protection. 
AI definition only for types of machine learning 
The definition of AI is crucial for the scope of application of future regulations. Algorithms which do not include any type of machine learning or selfoptimisation should, by definition, not be subject to AI regulations. Linear 
models, supporting methods from the area of explainable AI and established statistical methods should not be subject to the scope of application 
either. Otherwise, there is the risk that the Regulation will take the form of 
a general software regulation, which would clearly not be in line with the 
declared purpose and the recitals. Therefore, in order to make sure that the 
Regulation meets the intended purpose, paragraphs (b) and (c) in Annex I 
should be deleted since they do not describe any techniques or approaches 
of artificial intelligence. 
Similarly to the list of high-risk AI applications, which the EU Commission 
can extend by means of adopting delegated acts, the list of techniques and 
approaches of artificial intelligence can also be amended. Accordingly, 
there is also no legal certainty for companies with regard to the definition of 
AI. Since the Commission can subsequently add to both the list of AI techniques and concepts in Annex I and the list of high-risk AI applications in 
Annex III, there is a risk that the scope of the regulation will be disproportionately expanded. This would be contrary to the right approach to only 
make high-risk AI applications subject to additional high requirements. It 
would therefore be desirable for the EU Commission to exercise its power 
to supplement the lists with restraint. 
Liability insurance for notified bodies 
Article 33(8) proposes a requirement for notified bodies to take out appropriate liability insurance for their conformity assessment activities. It is our 
understanding that entities acting as notified bodies under the proposed 
Regulation and in a corresponding capacity foreseen by other EU 

legislation, for instance the Medical Devices Regulation (2017/745), will 
continue to be able to cover all of their conformity assessment activities 
under a single contract of liability insurance. Further, it is our understanding 
that such insurance will be written on the basis of standard market terms 
and conditions in accordance with applicable insurance contract law. 
Encouragement of voluntary codes of conduct is welcomed 
The EU Commission’s approach to rely on soft law solutions such as selfregulation with regard to non-high-risk AI applications is welcomed. Voluntary codes of conduct represent a reasonable supplement to existing laws, 
which make sure that essential safety standards are being fulfilled. 
The proposed tool of voluntary codes of conduct can tell potential users – 
such as citizens, companies as well as public authorities – which applications meet particularly high standards. 
This would create incentives for companies to go beyond the existing requirements and develop particularly trustworthy solutions. It would benefit 
not only consumers but also companies which can use this tool to distinguish themselves from competition. Such an approach can give European 
companies a first-mover advantage in AI in the global competition. As a 
result, European companies can set themselves apart from global competitors as the trustworthy alternative. 
However, the Regulation provides that the requirements for high-risk AI systems referred to in Title III, Chapter 2 shall also be completely fulfilled within 
the scope of voluntary codes of conduct for non-high-risk AI systems. Calling for the same requirements for non-high-risk AI systems would go too 
far, which is why the requirements for these AI systems within the scope of 
voluntary codes of conduct should be adequately reduced or the companies 
should be given more leeway in developing these codes of conduct. 
Taking adequate account of the risks arising from supervisory access 
to training, validation and testing data 
In the course of increasing digitalisation it is understandable that the supervisory authorities are considering new methods and procedures to carry out 
their mandate. One of these innovations concerns the data transfer between supervisory authority and company by means of application programming interfaces (APIs) or other technical solutions. In the AI Regulation, 
APIs are provided for the access to training, validation and testing data. 

However, for these data, in particular, using APIs poses some risks that 
need to be adequately considered. Attacks or manipulation attempts regarding AI applications through so-called “adversarial examples” show how 
important it is to protect training, validation and testing data as well as the 
source code of the AI application. Knowledge of the data or the source code 
makes it possible to deliberately manipulate the predictions or decisions of 
AI applications. Against this background, each transfer of such data and 
every interface which enables direct access to these data pose an additional 
safety risk. Furthermore, implementation and maintenance efforts will be 
required with regard to the demanded application programming interfaces, 
which would place a heavy burden on small and medium-sized companies, 
in particular. 
No weakening of the level playing field in regulatory sandboxes 
In order to ensure innovation-friendly framework conditions and to encourage innovation it is fundamental to prevent overregulation of AI applications 
and legal uncertainty. In addition, measures to encourage innovation may 
also play an important role. Numerous countries within the EU have already 
established innovation hubs or regulatory sandboxes such as the “European Forum for Innovation Facilitators” as a platform to share information 
and coordinate efforts at European level to facilitate innovation in the financial sector. The supervisory infrastructure and expertise built in this context 
should also be used to encourage innovation in the area of AI applications. 
With regard to measures to encourage innovation it is crucial that a level 
playing field continues to be ensured for all providers, following the principle 
of “same business, same risk, same rules”. This is the only way to allow for 
fair competition in innovation between different types of market participants 
(e. g. traditional providers and start-ups). Providing small and medium-sized 
companies with priority access to regulatory sandboxes, as proposed by the 
EU Commission under Article 55, would mean an unreasonable discrimination against large-scale companies and is therefore to be rejected. 
Preventing duplication in the design of governance structures 
The possible use of existing supervisory structures, where available, and 
the consideration of national distributions of competences are welcomed. 
As a result, well-functioning structures will be preserved and a duplication 
of supervisory activities will be prevented. For industries which are already 
subject to regulation and comprehensive oversight, such as the insurance 
industry, additional regulation and additional supervision would not seem 
justified, considering the efforts to be required and the benefits to be gained. 

There would be a high risk that, as a result of the bureaucratic requirements, 
many innovations benefitting customers and the society might be stifled and 
that European insurers, for instance, would be placed at a competitive disadvantage to foreign providers. 
Furthermore, it is crucial for the design of the supervisory structures that 
harmonised law enforcement is being ensured within the EU. The AI Regulation operates with numerous uncertain legal terms which must be interpreted in a harmonised way across Europe. This is particularly challenging 
with regard to cross-border matters. In addition to the national supervisory 
authorities, the “Artificial Intelligence Board” will play a major role in this 
respect. The establishment and structure of the Board, as provided for in 
the Regulation by the EU Commission, is welcomed. In its function as 
guardian of the Treaties, the EU Commission pursues the fundamental objective of harmonized Union law enforcement. It is therefore only logical that 
essential tasks of AI regulation are in the Commission’s hands and that the 
Commission is being assisted and advised by the Board in carrying out 
these tasks. From the point of view of the insurance industry, it is therefore 
the right approach that the Board is not given any power to issue guidelines 
or monitor them. The supporting and advisory role of the Board is evident 
from the fact that it is authorised to issue opinions, recommendations or 
written contributions on matters related to the implementation of the Regulation. 
It is crucial in this context that the “Artificial Intelligence Board” does not 
follow the blueprint of the European Data Protection Board (EDPD). According to recital 139 GDPR, the European Data Protection Board should also 
“contribute to the consistent application of this Regulation throughout the 
Union (…) and promoting cooperation of the supervisory authorities 
throughout the Union”. Pursuant to Article 70 GDPR, the Board can issue a 
whole series of “guidelines, recommendations, and best practices” for this 
purpose. While the decisions by the EU Commission are directly subject to 
judicial control, the powers of the EDPB have a quasi-binding effect. However, they are not subject to direct judicial review. Furthermore, the Board 
interprets the GDPR very broadly, sometimes contrary to the text of the law 
and without taking account of what is relevant in practice, and to the detriment of controllers. 
The governance structure of the Board provided in the Regulation is therefore appreciated. Due to the fact that the EU Commission chairs the Board, 
it is ensured that the results of the Board are based on common decisionmaking. The governing function of the EU Commission is also reflected by 
the fact that the Board’s rules of procedure require the Commission’s approval. This way it is being prevented that the Board becomes independent, 

which could lead to decisions that are not in line with the Commission’s and 
thus to a hardly justiciable “secondary legislation”. 
Legal basis for the processing of personal data to prevent discrimination 
The introduction of a legal basis under Article 10(5) for the processing of 
special categories of personal data to the extent that it is strictly necessary 
for the purpose of preventing discrimination is appreciated. Preventing discrimination through AI applications is crucial for the creation of a trustworthy 
AI landscape, and the processing of special categories of personal data 
thus is a matter of substantial public interest. In contrast to the general provisions stipulated in the GDPR, Article 10(5) provides a clear legal basis for 
AI training in this respect. In practice, however, the training of AI for the 
purpose of eliminating or preventing discrimination cannot be separated 
from training for other specific purposes. In addition to the objective to treat 
different populations in a non-discriminatory manner, AI also needs to optimise the accuracy of the prediction during the training (Article 15). This also 
applies to the objective that AI systems should be resilient as regards inconsistencies and errors. Article 10(5) should therefore be extended to data 
processing for these purposes as well. 
Furthermore, is seems worth considering that legal bases for data processing for the purpose of improving AI systems should not be limited to 
high-risk applications only. The objectives pursued with the training are also 
relevant for AI applications that have a lower risk. 
Berlin, 30 July 2021 