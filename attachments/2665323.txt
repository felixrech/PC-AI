Opinion of the LNE on the proposal for a 
Regulation of the European Parliament and of the 
Council laying down harmonised rules 
on artificial intelligence 
Introduction 
In the context of the European Commission's call for opinions on its proposed regulation on artificial 
intelligence (the so-called "AI Act"), we thought it would be useful to communicate a number of 
remarks. 
For the past four years, artificial intelligence has been at the top of the European and international 
technological and industrial development agenda. Professional and domestic sectors are gradually 
becoming automated. The health crisis we are facing is contributing to this transformation towards a 
more 'virtualized' society, less exposed to biological vulnerabilities. While Honda has been marketing 
its level 3 autonomous vehicle Honda Legend in Japan since March 2021 and the FDA (Food and Drug 
Administration) in the United States is beginning to authorize the marketing of AI-enabled medical 
devices, developers are still at a loss as to the conformity assessment that underpins authorizations 
for deployment within our common market. This situation obliges the public authorities and the LNE, 
whose 2017 Villani mission on AI proposed that it be designated as the competent authority for AI 
assessment in France, to get involved in the issue. The European authorities, which are ahead of their 
international partners on the subject of trusted AI, play an active role in removing the current 
obstacles to the deployment of these technologies. We propose to insist on the need to 
systematically use certification to establish a European framework conducive to the development of 
high-risk AI that is efficient, safe and acceptable. This would mean moving in two directions: 
Ensure the competence of Notified Bodies in AI assessment, in particular regarding the 
development of assessment methods (protocols, metrics, etc.), the implementation of test 
environments (virtual, physical, mixed, database, etc.) and the modalities of conformity 
assessment (tests, audits, literature reviews, etc.). This could be achieved if Notified Bodies 
rely on test centers competent in AI assessment, or if they entrust the latter with the task of 
defining the assessment methodologies (protocols, tools, test benches, etc.). 
Encourage developers of non-critical AI solutions to use voluntary certification according to 
standards that include the requirements of the AI Act; the LNE will produce an updated 
version of its certification standards in early 2022 to cover all these requirements. 
The LNE remains at the disposal of the European Commission for discussions on how to implement 
these measures. 
Section 2.1 – Taking into account the regulations in force 
Some issues, such as personal data protection or cybersecurity, are not specific to AI and are already 
covered by different regulations (Article 16 TFEU, etc.). It is for example the case for cybersecurity 
(Article 42 of the proposed AI Act, paragraph 2). In order to avoid confusion on the part of 

developers of AI solutions, it is important that the requirements of the AI Act do not duplicate these 
existing regulations. It is therefore necessary to focus this regulation on the intrinsic vulnerabilities of 
AI, particularly in terms of performance, robustness and explainability. 
Articles 8-15 (Title III, chap. 2) – Requirements for high-risk AI systems 
With regard to the requirements set out in Articles 8 to 15, the LNE has noted the following: 
Some of the data requirements (Article 10) need to be clarified, such as the notion of 
"relevance" of processing operations (Article 10(2c)), the "appropriateness" of the statistical 
properties of training, validation and test datasets (Article 10(3)), etc. This is also true for 
other requirements: AI systems are expected to have an "appropriate level of accuracy" (cf. 
Article 15(1)), without it being made clear how this level is to be determined. 
Certain steps in the development process of an AI system, such as the annotation (see Article 
10) of training and test data for machine learning algorithms, could be subject to more 
specific requirements to ensure that they are carried out properly. For example, it could be 
required that the quality of annotations be assessed by means of inter- and intra-annotator 
qualification. 
The elements to be presented in the “instructions for use” could also include a description of 
the infrastructure (hardware, operating system, software), the types of deployment (public 
or private cloud, on-premise etc.) supported by the AI functionality and the dependency on 
underlying AI technologies, as well as the interfaces required for the use of the AI system, the 
main factors influencing the performance of the AI system (weather conditions, etc.), the 
contraindications and non-conditions associated with the use of the AI. 
The requirements on post-market monitoring of the AI system (see Article 61) could be 
extended to the need to ensure "maintenance in operational condition", which could be 
achieved through a dedicated set of requirements. 
The question arises as to whether the version to be indicated in the technical documentation 
(see Annex IV(1a)) should be updated whenever the parameters of an AI system with 
continuous learning change. 
The requirement for the automatic recording of events of the high-risk AI system (see Article 
12(1)) could be extended to include a replay capability, for example for accident situations. 
Certain characteristics of AI systems, such as their robustness and resilience, which are 
required by the draft AI Act (see Article 15), need to be defined, for example by clarifying 
how to measure them. 
Article 43.2 
Article 43(2) of the draft Regulation provides that for the high-risk AI systems referred to in Annex III, 
points 2 to 8, providers shall follow the conformity assessment procedure based on internal control 
as referred to in Annex VI, which does not provide for the involvement of a notified body. As a result 
of this provision, the assessments of a large majority of high-risk AI systems will not involve an 
independent third party body. In order to increase confidence in the compliance of AI systems and to 
enhance the robustness of the assessment mechanisms used, we recommend greater use of the 
conformity assessment procedure involving a notified body, described in Annex VII. 
The use of Notified Bodies is common to many regulations within the scope of the new legislative 
framework. It has proven to be effective at a limited cost and adapted to the structure of the 
assessed companies. 
It should be noted that by considerably limiting the scope of intervention of notified bodies, the draft 
regulation hampers the emergence of independent conformity assessment bodies, which are 
indispensable actors in controlling the development of high-risk AI systems. 

Article 43.3 and article 38– Areas covered by existing notified bodies 
In order to facilitate the development of the competence of the notified bodies that will be involved 
in the high-risk AI systems to which the legal acts listed in Annex II, Section A, apply, and to 
harmonise the assessments to be carried out by these bodies, we recommend the establishment of 
assessment guides. 
Similarly, there should be communication between the sectoral group of notified bodies referred to 
in Article 38 and the sectoral groups of notified bodies provided for in the legal acts listed in Annex II, 
Section A. 
Article 43.4 – Lifelong learning 
It is proposed in the current version of the AI Act that high-risk AI system modifications resulting 
from post-deployment learning are not considered substantial (i.e. do not require a compliance 
reassessment) if they were predetermined by the provider at the time of the initial assessment (cf. 
Article 43(4) and Whereas 66). However, for a system with an incremental or continuous learning 
capability after deployment, this notion of predetermined modification is unclear to us, while the 
risks of degrading the performance of such systems are significant. We therefore recommend that 
this point be clarified and that a compliance assessment process be developed for systems with 
lifecycle learning. 
Article 69 (Title IX) - Codes of conduct 
With regard to Article 69, we wish to inform the European Commission of the creation by the LNE of 
a process certification for AI. The document on which this certification is based defines criteria for 
the design, development, evaluation and maintenance processes of artificial intelligence systems. 
This document was developed thanks to the reflections of a working group bringing together 
developers, evaluators and users of all sizes and from various backgrounds (AXIONABLE, ARCURE, IRT 
RAILENIUM, KICKMAKER, ORANGE, PROXINNOV - A2D, SCHNEIDER ELECTRIC, SCORTEX, THALES, 
TOSIT, etc.) 
This document is intended to evolve to take into account the state of the art, new regulations or 
standards in the field. 
We therefore suggest that Article 69 should not limit the possibility of setting up codes of good 
conduct to individual suppliers of AI systems or organisations representing them. 
For more details on LNE certification: https://www.lne.fr/fr/service/certification/certificationprocessus-ia 
To download the standard: https://www.lne.fr/fr/recevoir-referentiel-certif-IA 
Annex II section A 
The list of legislative texts in section A of Annex II could be completed by Directives 2014/31/EU and 
2014/32/EU. Indeed, the use of AI systems in regulated measuring instruments is now envisaged by 
the manufacturers of these products, with implications in particular in the field of energy metering or 
measurements of pollutant emissions from vehicles. 