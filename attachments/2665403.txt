1 
4 August 2021 
Feedback on the European Commission’s 
proposal for a Regulation laying down 
harmonized rules on Artificial Intelligence 
(Artificial Intelligence Act) and amending certain 
Union legislative acts 
Established in Dublin in 1990, the Association of Consumer Credit Information Suppliers (ACCIS) 
represents the largest group of credit reference agencies in the world. ACCIS brings together 40 
members across 28 European countries and 11 associate and affiliate members from all other 
continents. 
Introduction 
Credit Reference Agencies (CRAs) provide credit-related information as well as products and services 
derived from this information, including credit scores, to help credit institutions lend responsibly and 
borrowers to get access to fair and affordable credit. CRAs also provide other services to lenders, 
including helping them build their own credit scorecards, detect and prevent economic crime, helping 
to monitor and predict markets trends, or refining their lending and counter-fraud strategies. The 
clients CRAs work with primarily provide financial services, energy and telecommunication services 
and some also work with public administrations. 
CRAs’ clients in financial services are licensed and supervised organisations at EU level, and their 
activities are covered by the Consumer and Mortgage Credit Directives, as well as by guidelines from 
the European Banking Authority. CRAs clients in the energy and telecommunications also operate 
within an extensive framework of EU and national level regulations. 
CRAs have pioneered the development, operationalization and governance of credit scoring models 
and their underlying techniques for decades. While “conventional” models using traditional statistical 
techniques (e.g. weight of evidence, logistic regression, discriminant analysis, etc.) are still the most 
common, CRAs are developing more “advanced” models using different kinds of algorithms, such as 
neural networks. ​EBA Report on Big Data and Advanced Analytics (2020) ​  These advances in Artificial Intelligence (AI) - and specifically in Machine Learning 
(ML) - are improving existing models and driving new approaches to credit scoring and, in turn, to 
lending, which allow creditors to lend more responsibly to more consumers and SMEs, while 
strengthening protection against over-indebtedness and non-performing loans. 
CRAs have found that the same expertise and robust governance that they have honed over many 
years to control their traditional algorithmic models are proving effective in controlling the risks from 
models that use AI/ML. These governance arrangements of CRAs already ensures accountability, 
transparency, human oversight and effectiveness to manage the risks to data subjects and product 
users. 
Building on the vast experience of the credit reference industry with data management and data 
analytics, ACCIS acknowledges the Commission’s efforts to address both the benefits and challenges 
of AI in its draft regulation laying down harmonized rules on this technology. We believe, however, that 
more work is needed to strike the right balance between promoting innovation and protecting 
European citizens and their rights. We fear that the current draft adopts a disproportionate approach 

2 
that could curtail the use of socially beneficial applications of AI/ML, in particular as regards 
creditworthiness assessments and credit scoring. 
In this paper, we have identified three main critical issues in the draft Act and recommended solutions: 
1. the definition of AI systems should be narrowed so that it does not include low risk, 
understandable and explainable techniques such as logistic regression; 
2. AI for credit scoring and creditworthiness assessments should not be deemed high-risk 
because borrowers are already adequately protected by existing legislation and regulations; 
3. the concept of creditworthiness should be clarified; 
We would like to also share our views and recommendations on three additional issues: 
4. the proposed obligations on providers and users of AI systems intended to be used to 
evaluate the creditworthiness of natural persons or establish their credit score are not 
appropriate; 
5. the role of voluntary codes of conduct should be strengthened; and 
6. the governance framework should ensure harmonized supervision and enforcement of 
obligations. 
Main critical issues 
1. The definition of AI systems should be narrowed 
According to Annex I, referred to in Article 3 (1), the definition of AI techniques and approaches 
includes under (c) “statistical approaches, Bayesian estimation, search and optimization methods”. 
This definition in Annex I: 
a. contrasts with the definition put forward by the High-Level Expert Group (HLEG) on Artificial 
Intelligence – set up by the European Commission: 
“AI includes several approaches and techniques, such as machine learning (of which deep 
learning and reinforcement learning are specific examples), machine reasoning (which includes 
planning, scheduling, knowledge representation and reasoning, search, and optimization), and 
robotics (which includes control, perception, sensors and actuators, as well as the integration of all 
other techniques into cyber-physical systems).” 
The HLEG on AI did not include “statistical approaches” under its definition of AI. 
b. is far too wide and too vague. It covers techniques (such as logistic regression) that have been 
used extensively and safely in loan origination models for decades and which cannot be 
reasonably be described as novel or high-risk techniques. It is also too vague – simply averaging 
three numbers is a “statistical technique” and the most basic decision tree based on business 
experience could be an “expert system”. 
Annex 5 of the impact assessment sets out five characteristics of AI that logistic regression (and 
other techniques captured by the definition in Annex I) do not have: 
Complexity is defined as parameters that “are not in practice understandable for humans, 
including for their designers and developers”. Logistic regression is fully understandable, ex 
ante. The report by the EU Agency for Human Rights, cited by the impact assessment, 
categorises “regression analysis predictions for fully automated credit scoring” as medium, not 
highly, complex. 
Transparency / opacity “with respect to how exactly the AI system functions as a whole 
(functional transparency); how the algorithm was realized in code (structural transparency) 

3 
and how the program actually run in a particular case, including the hardware and input data 
(run transparency)”. Logistic regression is fully transparent in each case. 
Continuous adaptation refers to a “process by which an AI system can improve its own 
performance by learning from experience”, and “can give rise to new risks that were not 
present when the system was placed on the market”. Logistic regression models do not have 
self-learning capabilities and thus do not continuously adapt. 
Autonomous behaviour, which “can affect the safety of the product, because certain AI 
systems increasingly can perform tasks with less, or entirely without, direct human intervention 
and in complex environments this may lead to situations where AI system may [take] actions 
which have not been fully foreseen by their human designers with limited possibilities to 
override the AI system decision”. All steps in a logistic regression model are programmed and 
controlled by humans so there are no autonomous outputs. 
Data where “the dependence of AI systems on data and their ‘ability’ to infer correlations from 
data input can in certain situations affect the values on which the EU is founded, create real 
health risk, disproportionately adverse or discriminatory results, reinforce systemic biases and 
possibly even create new ones”. While this risk does apply to credit scoring models that use 
logistic regression, the risk has existed and been addressed throughout the decades for which 
these models have been used. The risk has not grown. 
Including logistic regression and similarly understandable and transparent techniques in the 
definition of AI is completely at odds with the purpose of this regulation. As explained in the impact 
assessment, the driver of the problem are the specific characteristics of certain AI systems which 
make them qualitatively different from previous technological developments. It would then be 
inappropriate to include in the AI definition traditional techniques that don’t share such 
characteristics and thus their associated risks can be, and are, currently addressed differently as 
explained below. Otherwise, the AI regulation would be disproportionate, not cost-effective and 
unnecessarily deter innovation with regards to such traditional techniques. 
Recommendation: To align the definition of AI systems in the proposal with the stated aims of the Act, 
the Commission should delete the following from the definition in Annex 1: “expert systems” and 
“statistical approaches, Bayesian estimation, search and optimization methods”. Alternatively, the 
definition should explicitly state that AI systems using logistic regression and similarly explainable and 
non self-learning techniques are excluded.  
2. AI for CWAs and credit scoring should not be classified as high-risk 
According to Annex III referred to in Article 6(2), “AI systems intended to be used to evaluate the 
creditworthiness of natural persons or establish their credit score” are considered “high risk” (point 5 
(b)). 
The criteria to classify them as such is that if, in the light of its intended purpose, those systems pose 
“risk of harm to the health and safety” or a “risk of adverse impact on fundamental rights” (Recital 32). 
Recital 37 then explains that, in particular, AI systems used to evaluate the credit score or 
creditworthiness of natural persons should be considered high-risk since they “may lead to 
discrimination of persons or groups and perpetuate historical patterns of discrimination, for example, 
based on racial or ethnic origins, disabilities, age, sexual orientation or create new forms of 
discriminatory impacts”. 
We believe that the existing legislative framework already addresses the identified risks, so subjecting 
these systems to new regulation would impose a major and disproportionate burden on firms, in the 
financial sector and beyond and would go counter the Commission´s goal of promoting innovation. 
In this regard, we would like to make the following considerations: 

4 
The processing of personal data revealing racial or ethnic origin, political opinions, religious or 
philosophical beliefs, or trade union membership, and the processing of genetic data, biometric 
data for the purpose of uniquely identifying a natural person, data concerning health or data 
concerning a natural person’s sex life or sexual orientation is prohibited by Article 9 GDPR. CRAs 
do not process ‘sensitive’ data. 
Over the past decades, creditors and CRAs have developed model governance processes to 
understand and manage risks, in particular the risk of bias, in line with the EU Charter of 
Fundamental Rights and secondary legislation such as regards data protection as well as 
international good practices and the expectations of consumers, clients and regulators. 
Even though CRAs are not generally subject to financial regulation or financial supervision 
directly, they need to comply indirectly with financial regulation as lenders need to ensure that any 
credit risk models they integrate into their decision-making processes are compliant. ​CRAs are subject to strict supervision by data protection authorities and in some countries such as Denmark and Norway there 
are additional licensing requirements. ​  
We, therefore, believe that any risks that may arise from the use of AI in CWAs are already 
effectively mitigated by financial and data protection regulation at EU and member state level, with 
routes for consumers to seek manual review of decisions and redress under the GDPR and more 
recently the proposal for a revised Consumer Credit Directive. The Guidelines from the European 
Banking Authority on loan origination and monitoring also specifically cover credit decision-making 
processes that use automated models. All these legal instruments can be updated if necessary. 
It is also important to note that other sectors beyond financial services, such as telco and energy, 
also benefit from improved tools to make creditworthiness evaluations, including advanced credit 
scoring, to better serve European citizens. 
Finally, it is important to recall that AI is still an innovative technology in the financial service 
sector, so investment in its development is critical and not granted. European policy should 
promote, not deter, its development and uptake and regulation should only be introduced when 
required. Indeed, as recognised by the Commission, technological progress using AI / ML 
approaches has the potential to generate significant benefits for consumers, the economy and 
society including in the financial service sector. AI models tend to be more accurate than 
traditional models, therefore they make fewer mistakes (e.g. false positives and false negatives) 
which actually results in the AI model having better performance metrics in terms of fairness to the 
customers. AI can, therefore, be fairer to the consumer than the traditional methods. In particular, 
AI better helps the inclusion of individuals (and businesses) that have previously been invisible to 
or underserved by credit markets and it supports risk management techniques. Algorithmic credit 
scoring also lowers the costs of lending and provides an opportunity to inspect and re-optimize 
lending decisions. ​Nir Kshetri (2021) The Role of Artificial Intelligence in Promoting Financial Inclusion in Developing Countries, Journal of Global 
Information Technology Management. ​  
Recommendation: To remove uses of AI for CWAs and credit scoring from the list of high-risk 
applications, at least not until credible evidence emerges to justify such classification based on the 
criteria mentioned in Article 7 and after a robust and transparent consultation process with all 
stakeholders concerned is conducted. As the Commission will have to power to add use cases to 
Annex III, it should wait now until it is clearer to what extent further regulation would be beneficial. 
3. The concept of CWAs should be clarified 
In case the Commission would still consider that AI systems intended to be used to evaluate the 
creditworthiness of natural persons or establish their credit score should be classified as high risk, the 
concept of ‘creditworthiness assessment’ should be clarified. 
In this respect, we would like to explain certain aspects of the credit decision-making process: 

5 
There are three main steps in the creditworthiness assessment process​See, for example, the European Parliament report on “Mis-selling of Financial Products | Consumer Credit“ (2018). ​ : 1) the creditor gathers 
relevant information about the consumer’s financial situation; 2) once the creditor has collected the 
necessary data, he makes a judgement about the consumer’s creditworthiness; 3) Once the 
creditor has made such a judgment, he can make a decision on the consumer’s credit application. 
The creditworthiness assessment (step 2) and the final decision to grant or not credit (step 3) are 
two different decision-making processes. They are both done by the lender, not by CRAs. 
The judgement about a consumer’s creditworthiness assessment (step 2) is only one factor in a 
creditor's decision to lend (step 3). Other factors include affordability assessments, ID verification 
and the lenders’ portfolio risk management and business models. This means, for example, that a 
person may have a good creditworthiness assessment, but still be rejected for a certain product 
because the lender´s policies require that person, for example, to be a homeowner. This can also 
happen the other way around, after a negative creditworthiness assessment, the lender may still 
decide to grant credit to a person, as recognised in the recent proposal for a revised Consumer 
Credit Directive. 
To conduct a creditworthiness assessment (step 2), a lender may use different types of input 
about the consumer (step 1), including credit scores. Such an assessment may or may not be AIbased. Similarly, each of the inputs, including the credit scores, may or may not be AI-based. 
As regards scores, there are different types of scores. Typically, lenders create their own scores. 
While they may use information obtained from CRAs, they will also use their own information and 
algorithms to help calculate their own scores. Lenders may also use CRA scores as the only 
scores that they use as part of their decision-making process. 
In light of the above, an AI system which is used for an input in the credit decision-making process - 
with a non-significant impact in such a process - should not be covered by the regulation. Indeed, AI 
systems can be used for different inputs at different stages of the process, and which can be very far 
from the final decision. 
Recommendation: To clarify that an AI system which is used for an input in the credit decision-making 
process - with non-significant impact in such a process - should not be covered by the regulation. 
Additional issues 
4. Transparency and documentation requirements should be adjusted 
Regarding the provisions on transparency and provision of information to users (Article 13 AI 
Regulation), we believe that transparency should be done under the terms and conditions already 
regulated by the GDPR. Article 13.2.f of GDPR states that "the existence of automated decisions, 
including profiling, referred to in Article 22, paragraphs 1 and 4, and, at least in such cases, 
meaningful information about the logic applied, as well as the significance and expected 
consequences of such processing for the data subject, must be reported". We believe that this is a 
concise regulation already in place in all Member States and that, therefore, transparency in AI must 
necessarily be linked to what is already stipulated in the GDPR. 
Technical and auditing requirements for high-risk AI systems are very detailed. The Commission 
should generally try to reduce the documentation effort, which would be a significant cost for the 
industry and barrier for innovation, and, especially, it should try to reduce “eventualities” (e.g., such as 
the fact, that providers should not only consider the intended use of scores but also what they might 
be used for). 
Recommendation: To align the requirements on transparency and provision of information to what it is 
already stipulated in the GDPR. 

6 
5. Voluntary codes of conduct should not only be for ‘non high-risk’ AI 
Article 69 introduces the possibility of a voluntary code of conduct for non high-risk systems to be 
compliant with requirements foreseen for high-risk systems. This tool should also be foreseen for highrisk systems too. As per the experience of our industry with the GDPR, codes of conduct can facilitate 
compliance for providers of AI systems and address sector-specific issues and processes whilst they 
foster trustworthiness and a level of guarantees among various stakeholders that assure that all 
relevant risks are identified and addressed. 
Recommendation: To introduce the possibility to adhere to codes of conduct for all AI providers – 
including for high-risk systems. 
6. The governance system should be improved 
Title VI sets out the governance systems at EU and national levels. With respect to the national level, 
article 59 provides that Member States will have to establish or designate one or more national 
competent authorities (NCAs) for the purpose of supervising the application and implementation of the 
regulation. 
Not being a ‘regulated financial institution’ under Union legislation on financial services (mentioned in 
recital 80), CRAs would fall under the supervision of the NCA that each Member State would establish 
or designate. This may lead to inconsistencies and /or discrepancies across the EU Single Market in 
relation to conformity assessment procedures and the enforcement of other procedural obligations 
placed on AI providers under the proposal. There is a need for a proper harmonized enforcement 
system to ensure consistency and also to avoid overlaps with other existing authorities at Member 
State level (such as the data protection authorities). There is also a need for a mechanism 
guaranteeing a single point of contact for companies in case of cross-border cases. 