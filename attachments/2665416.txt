European Commission Proposal for a Regulation on Artificial Intelligence (AI)
Feedback from NEC Corporation
1.
Introduction
1.1
This document contains feedback from NEC Corporation (“NEC”) on the European 
Commission’s Proposal for a Regulation on Artificial Intelligence (COM(2021)206) dated 
21 April 2021 (the “Draft Regulation”).
1.2
NEC’s feedback in this document comprises:
information regarding NEC’s AI business;
general comments on the Draft Regulation;
comments on Title III of the Draft Regulation relating to requirements for highrisk AI systems; and
(D)
brief comments on other provisions of the Draft Regulation. 
1.3
NEC would welcome the opportunity to discuss its feedback with the Commission and, 
more generally, to provide further input on and participate in the development of the Draft 
Regulation. ​NEC can be contacted for these purposes at overseas-dta@manage.jp.nec.com​ 
2.
NEC’s AI business
2.1
NEC is a Japanese multinational technology firm, specialising in the development of a 
range of technologies including AI technologies. NEC has been involved in researching 
and developing AI technologies for over 50 years. NEC’s AI technologies comprise 
“visualisation” solutions (including image clarification and biometric technologies), 
“analysis” solutions (including data and textual analysis technologies) and “prescription” 
solutions (notably, predictive technologies). NEC’s vision is that, by combining these 
forms of technology, humans can maximise the economic and social value that AI is 
capable of delivering.
2.2
NEC’s AI technologies are deployed in a range of sectors and contexts, including public 
safety (for example, in law enforcement and border control contexts) and infrastructure 
(for example, in public utilities and power contexts). NEC prides itself on developing 
robust and accurate AI technologies; by way of example, its facial recognition software 
has consistently been ranked as the most accurate of its kind by the US National Institute 
of Standards and Technology. ​https://www.necam.com/AdvancedRecognitionSystems/NISTValidation/FingerprintFacial/​ 
2.3
NEC understands that it is likely to be a “provider” rather than a “user” under the Draft 
Regulation (this distinction is discussed further below). As a result, and as one of the 

world’s largest AI developers, NEC is likely to be significantly impacted by the Draft 
Regulation. 
2.4
As a final preliminary remark, it should be noted that, whilst NEC develops a range of AI 
technologies, it is generally the customer / user who determines the domain or context 
in which the AI system is deployed. The customer / user may also tweak or make final 
configurations to the AI system to determine, ultimately, how it will operate when it is 
deployed (for example, the customer might train the AI system on specialised data sets). 
This point is relevant to some of NEC’s feedback comments below. 
2.5
In broad terms, whilst NEC appreciates that the Draft Regulation builds upon the EU’s 
New Legislative Framework regime (relying on similar concepts and terms used in that 
regime), NEC is concerned that this regime is not, in all circumstances, appropriate for 
AI systems and that a more flexible legal regime is required. 
3.
General comments on the Draft Regulation 
NEC supports the purpose behind the Draft Regulation
3.1
NEC fully supports the Commission’s purpose and intention behind the Draft Regulation. 
In particular, NEC endorses the Commission’s comment in paragraph 1.1 of the 
Explanatory Memorandum to the Draft Regulation (echoed in Recital (5)) that “Rules for 
AI available in the Union market or otherwise affecting people in the Union should… be 
human centric, so that people can trust that the technology is used in a way that is safe 
and compliant with the law, including the respect of fundamental rights”.
3.2
NEC shares the same vision in its role as a leading developer of AI technologies. As 
noted above, NEC’s work in this area (including its AI research and development work) 
seeks to complement human capabilities and improve social welfare. NEC firmly 
believes in the protection of human rights in the context of AI technologies; a philosophy 
which is enshrined in the NEC Group AI and Human Rights Principles.
3.3
NEC therefore welcomes the Commission’s proposal for a legal framework governing 
the development and use of AI, particularly so as to ensure a high level of protection for 
the public. In its comments below, NEC hopes to provide useful feedback on various 
aspects of the Draft Regulation which, in NEC’s view, could be improved so as to meet 
the Commission’s aims in proposing this Draft Regulation. 
Striking the right balance between protection, innovation and competition
3.4
NEC notes that, in proposing the Draft Regulation, the Commission (rightly) seeks to 
“foster the development, use and uptake of artificial intelligence in the internal market”​Recital (5).​ 
and to propose a legal framework “that is limited to the minimum necessary requirements 
to address the risks and problems linked to AI, without unduly constraining or hindering 

technological development or otherwise increasing the cost of placing AI solutions on 
the market”. ​Paragraph 1.1, Explanatory Memorandum. ​  The Commission also aims to “[strengthen] Europe’s competitiveness and 
industrial basis in AI”. ​Paragraph 2.2, Explanatory Memorandum.​  
3.5
NEC recognises that balancing the need to encourage innovation and competitiveness 
in the field of AI, whilst at the same time protecting the public from the possible harm 
that AI can cause, is a difficult exercise. However, as a broad comment, NEC is 
concerned that the Draft Regulation does not strike the right balance between these 
competing interests and that, in its current form, it is likely to disincentivise AI innovation 
and weaken the EU’s competitiveness and global position in fostering the development 
and use of AI.
3.6
In particular, and as is described in more detail below:
The requirements of Title III of the Draft Regulation (relating to high-risk AI 
systems) are very onerous. They are likely to require significant work and a large 
financial burden, particularly for “providers” of high-risk AI systems.
This burden is likely to stifle AI innovation and discourage firms from placing AI 
systems on the EU market. The EU is likely to be seen as a market with high 
barriers to entry, which will ultimately reduce its competitiveness on a global 
scale.
This burden is also likely to disproportionately affect SMEs (particularly those 
involved in the development of AI technologies), given that they are unlikely to 
have available to them the resources needed to comply with these onerous
requirements. This will inevitably have an anti-competitive effect on the AI 
developers market, which will be dominated by larger tech firms that are better 
able to comply with these requirements. 
3.7
NEC hopes that, during the legislative process of the Draft Regulation, there will be an 
opportunity for the Commission (and also the European Parliament and Council) to 
reflect on the points noted above and the balance that the Draft Regulation seeks to 
strike between protection, innovation and competition. 
3.8
NEC would be happy to engage in further dialogue with the Commission regarding this 
point (and the Draft Regulation more generally). In light of the potentially significant 
impact that the Draft Regulation is likely to have on organisations (globally) involved in 
AI development and use, NEC respectfully suggests that further discussions with 
relevant stakeholders regarding the Draft Regulation would benefit the legislative 
process and further iterations of this important legal framework.

Wide definition of “AI system” and related risks
3.9
The Draft Regulation adopts a very wide definition of “AI system”. This is a core concept 
that will have a profound effect on the scope and application of the Draft Regulation. In 
particular, under Article 3(1) and Annex I of the Draft Regulation, an AI system appears 
to include any algorithmic system which uses a “knowledge- or logic-based” approach
or any “statistical” approach. 
3.10
Whilst there is not yet any universally accepted definition or understanding of what 
constitutes “AI”, the definition of “AI system” under the Draft Regulation seems to capture 
technologies which, in NEC’s view, are not conventionally understood as being AI. In 
fact, as a result of this definition, many software solutions – which are not considered as 
“AI” by their user or provider – would fall under the ambit of the Draft Regulation.
3.11
In NEC’s view, this exacerbates the issues regarding innovation and competition noted 
above, as a result of the burden that the Draft Regulation places on organisations 
involved in the development or use of AI (in particular the Title III requirements for highrisk AI systems). It also creates a hidden risk for organisations that are not obviously 
impacted by the “high-risk” category of AI systems. For example, a financial institution 
which uses AI technologies in consumer-facing products or services is unlikely to be 
subject to the Title III requirements in respect of those technologies, but if it uses a 
deterministic software programme (which is not conventionally understood to be AI) in 
its recruitment processes (per Annex III), then it could be subject to the onerous Title III 
requirements in respect of that software, due to the wide definition of “AI system”. 
3.12
NEC appreciates that the Commission has proposed this definition in order to be “as 
technology neutral and future proof as possible, taking into account the fast 
technological and market developments related to AI”​Paragraph 5.2.1, Explanatory Memorandum. ​ , but the scope of this definition is
likely to create the issues noted above.
Extra-territoriality / scope of Draft Regulation 
3.13
NEC appreciates the rationale for the Draft Regulation having “extra-territorial” effect, as 
explained in Recital (11) and as provided for in Article 2(1). However, NEC believes that 
the relevant provisions which give the Draft Regulation this extra-territorial effect could 
benefit from further clarity. For example:
The definition of “provider” in Article 3(2) does not appear to contemplate a 
scenario in which multiple organisations contribute to the development of an AI 
system (as is often the case). For example, if three organisations jointly develop 
an AI system with a view to placing it on the market under none of their names, 

then it appears that none of them would be considered a “provider” under Article 
3(2). 
Article 2(1)(a) provides that the Draft Regulation applies to providers “putting into 
service AI systems in the Union”, irrespective of where they are established. The 
definition of “provider” also refers to the concept of “putting into service” an AI 
system. NEC appreciates that this is a concept adopted by the EU in the New 
Legislative Framework, but the definition of “putting into service” (and the
interplay between this definition and Article 28(1)(a)) could benefit from further 
clarity in the context of AI systems. For example, it could be made clearer how 
an organisation could “supply” an AI system “for own use on the Union market 
for its intended purpose”. 
On a related note, the Draft Regulation should make clear (perhaps by 
addressing this in a separate provision) the legal implications of an organisation 
using AI in its internal processes (for example, in a manufacturing process). As 
currently drafted, this organisation could, in these circumstances, be both a 
“provider” and a “user” (it could be putting into service an AI system for its own 
use and, at the same time, “using an AI system under its authority”). However, it 
seems unnecessary and overly burdensome for the organisation to have to 
comply with the Title III requirements for high-risk AI systems in this scenario.
(D)
Article 2(1)(c) provides that the Draft Regulation will apply to providers and users 
of AI systems located in a third country “where the output produced by the system 
is used in the Union”. This concept is not defined and may not be straightforward 
to apply in practice. 
3.14
As the Commission will appreciate, the distinction between providers and users is 
crucial, particularly in the context of the Title III requirements. NEC respectfully suggests 
that this distinction – and the provisions noted above pertaining to the scope of the Draft 
Regulation – would benefit from further clarity in order to provide more certainty. 
Allocation of responsibility between “providers” and “users”
3.15
NEC endorses the Commission’s proposal to impose obligations on all participants 
within the value chain of an AI system. NEC also recognises that, consistent with the 
EU’s New Legislative Framework, the Draft Regulation places greater responsibility on 
the manufacturer / provider of an AI system, as compared with other participants in the 
value chain. 
3.16
Unlike other products subject to the New Legislative Framework, however, the user of 
an AI system may ultimately have more control about the end use of the AI system and 
the provider may not be able to articulate or influence its “intended purpose”. As noted 
above, for example, NEC might develop a generic AI system, but it will be the customer 

/ user who would determine the domain or context in which that AI system is deployed 
and the customer / user might also further configure the AI system before it is deployed.
3.17
In light of this, NEC is concerned that the Draft Regulation imposes a disproportionate 
burden on “providers” versus “users”, particularly in relation to high-risk AI systems. For 
those systems, NEC understands that providers are subject to the obligations in Article 
16 (which includes compliance with the requirements of Title III, Chapter 2), whereas 
users of high-risk AI systems are subject only to the more limited obligations in Article 
29. 
3.18
There are also practical issues arising out of this allocation of responsibility. For 
example:
In cases where the user configures the AI system (which it may wish to do without 
involving the provider (e.g. for financial reasons)), it would be difficult for the 
provider to satisfy the requirements of Article 10 (for example). NEC appreciates 
that, in these circumstances, the user might be deemed to be the provider under 
Article 28 (if it has made a “substantial modification” to the AI system); however,
this raises further practical challenges. For example, if, as it is assumed it would, 
the user would then be responsible for satisfying the Title III, Chapter 2 
requirements, it would be practically difficult (or even impossible) for the user to 
satisfy all of those requirements given that it was not involved in the initial 
development of the AI system.
On a related note, and in light of the above, NEC considers that the definition of 
“substantial modification” in Article 28(1)(c) would benefit from further detail in 
the context of an AI system, especially if it has the potential to render a user 
subject to the Title III, Chapter 2 requirements.
Various provisions in Title III, Chapter 2 refer to the persons or groups of persons 
on which the high-risk AI system is intended to be used. ​See, for example, Article 10(3), Article 13(3)(b)(iv) and Annex IV, paragraph 3 (referred to in Articles 
11(1) and 18).​  As previously noted, 
the provider may not know the persons or groups of persons on which the user 
intends to use the AI system and it may be practically or commercially difficult for 
the provider to stipulate this in the “intended purpose” of the AI system. 
3.19
In short, NEC believes that the allocation of responsibility between providers and users 
of high-risk AI systems should be made more proportionate to reflect the fact that AI 
systems do not always function like other products which fall under the EU’s New 
Legislative Framework e.g. machinery, medical devices or toys. For those products, 
there is generally less flexibility for the end-user in terms of how they use the product 
and it is easier for manufacturers / providers to be able to state the intended purpose of 
the product. AI systems require a more flexible legal regime – users might configure the 

AI system and they will be able to determine where and how the AI system is deployed. 
On the other hand, it would be impractical and overly burdensome to make the user 
subject to the Title III, Chapter 2 requirements in those circumstances because they will 
not have been involved in the development of the AI system. 
4.
Comments on Title III of Draft Regulation 
Article 10: Data and data governance
4.1
NEC welcomes the Commission’s proposal in Article 10 of the Draft Regulation to 
stipulate requirements for data and data governance, and it agrees that the training, 
validation and testing of data sets is important in minimising the harm caused by AI 
systems. However, NEC considers that the requirements of Article 10 would benefit from 
further clarity, particularly given that the Commission has proposed the highest level of 
fine for violations of Article 10. ​As set out in Article 71(3)(b). ​  
4.2
In particular:
Article 10(2)(f) refers to practices concerning “possible biases”. NEC considers 
that the Draft Regulation could provide further clarity on this important point; for 
example, techniques or suggestions on detecting or removing bias from data 
sets.
Article 10(3) requires that “Training, validation and testing data sets shall be 
relevant, representative, free of errors and complete”. NEC notes, first, that these 
requirements are likely to be highly subjective and difficult to assess objectively 
(particularly the requirements as to “relevant”, “representative” and “complete”), 
and, second, that the requirement that a data set is “free of errors” is unrealistic 
in practice. Furthermore, there may be circumstances in which errors are used 
deliberately in a training or testing data set in order to improve the accuracy of 
the AI system. NEC respectfully suggests that further detail is required regarding 
these requirements and, possibly, some qualification or “materiality” threshold. 
Articles 10(3) and (4) pre-suppose that the provider of a high-risk AI system
knows, during the training of that system, in what context an end user is going to 
use the AI system. As noted above, this does not reflect the commercial reality 
of an AI value chain and NEC therefore reiterates the point noted above 
regarding the appropriate allocation of responsibility between providers and 
users. 
Article 15: Accuracy, robustness and cybersecurity 
4.3
Articles 15(1) and (2) suggest that an AI system must achieve a consistent level of 
accuracy – which must be declared in the relevant technical documentation – throughout 

its lifecycle. NEC does not believe that it is realistically possible for the provider of an AI 
system to guarantee that the system will achieve a specified level of accuracy throughout 
its lifecycle. 
4.4
This requirement could also expose AI providers to private law actions by users because 
it forces providers to promise that the AI system will achieve a specified level of accuracy 
throughout its lifecycle, which is not feasible. 
Articles 16(b) and 17: Quality management system
4.5
NEC has three points of feedback on the quality management system (QMS) provisions 
in Article 17 of the Draft Regulation:
The fact that the user may have control over the end purpose or use of an AI 
system suggests that, in these circumstances, the user should also have some 
legal responsibility for the QMS of the AI system (perhaps in relation to Articles 
17(1)(f), (h) and (i), for which the provider may find it difficult to include adequate 
information).
There are international standards which are either already well-established (for 
example, ISO 9001) or in development (for example, ISO/IEC AWI 42001 or 
ISO/IEC JTC1/SC 42) which cover or are likely to cover similar areas to the 
requirements of Article 17(1). Has the Commission considered the interplay 
between those requirements and the existing / future standards? There is a risk 
that, in the future, there could be multiple (and potentially conflicting) rules or 
standards regarding QMS for AI systems, which should be avoided if possible.
The requirement in Article 17(2) that “The implementation of aspects referred to 
in paragraph 1 shall be proportionate to the size of the provider’s organisation” 
would benefit from further clarity. 
Article 43 and Annex VII: Conformity assessment process
4.6
NEC notes two principal issues arising from the fact that the Draft Regulation appears 
to allow notified bodies to access information relating to an AI system under the Article 
43 and Annex VII conformity assessment process: 
Article 8.73(1) of the Agreement Between the European Union and Japan for an 
Economic Partnership prevents the EU from requiring the transfer of, or access 
to, the source code of software owned by Japanese entities. This appears to be 
inconsistent with paragraph 4.5 of Annex VII which allows the notified body to be 
granted access to the source code of an AI system in certain circumstances. 
Paragraph 4.3 of Annex VII allows the notified body to be granted “full access to 
the training and testing datasets used by the provider”. NEC would welcome 

further clarity on how any data protection / GDPR requirements can be managed 
in light of this where those data sets contain personal data. 
Article 50: document retention 
4.7
NEC assumes that the Draft Regulation, when it comes into force (following the relevant 
“grace period”), will apply to existing AI systems as well as new AI systems, although 
clarification on this point would be welcomed. 
4.8
Assuming this to be the case, but in any event, NEC considers that the requirement to 
retain the various documents set out in Article 50 of the Draft Regulation for 10 years is 
unduly onerous and is likely to increase the administrative costs of compliance with the 
Draft Regulation. This is likely to exacerbate the issues noted above regarding 
innovation and the competitiveness of the EU AI market.
5.
Brief comments on other provisions of Draft Regulation 
Title VIII, Chapter 1, Article 61: Post-market monitoring
5.1
NEC’s principal point of feedback on the post-market monitoring requirements in the 
Draft Regulation reflects its comments above on various other provisions of the Draft 
Regulation – these requirements appear to assume that, from a practical or commercial 
perspective, the provider has some control or that it retains some involvement in the use 
of the AI system. Given that providers may not be able to articulate or influence the 
intended purpose or end use of an AI system, NEC would query whether it would make 
more sense to place at least some responsibility on users for post-market monitoring.
5.2
NEC welcomes the indication in Article 61(3) that the Commission will adopt further 
detailed measures clarifying the requirements of the post-market monitoring plan, which 
will be important in providing further guidance on the Article 61 requirements.
Title VIII, Chapter 2, Article 62: Reporting of serious incidents and of malfunctioning 
5.3
Whilst NEC fully endorses the Commission’s proposal for serious incidents or any 
malfunctioning in the use of an AI system to be reported to the relevant market 
surveillance authority (under Article 62(1)), again, for the reasons noted above, NEC 
would query whether it would make more sense to place at least some responsibility on 
users for reporting serious incidents or malfunctioning. 
Title V, Article 53: AI regulatory sandboxes
5.4
NEC welcomes the provision in the Draft Regulation for AI regulatory sandboxes in order 
to foster AI innovation. Given that the Draft Regulation is extra-territorial, has the 
Commission considered making AI regulatory sandboxes available to AI companies 
outside of the EU (as currently drafted, it appears that this would not be possible under 
the Draft Regulation)?