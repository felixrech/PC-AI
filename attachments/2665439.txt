Response to the European Commission proposal
for an AI Regulation (WIP)
Executive summary
We welcome this opportunity to provide feedback to this important regulatory proposal that
will undoubtedly influence the future development of AI in the EU. Although the proposed
Regulation is quite balanced, we have identified some areas where there is room for
improvement:
1. Legal uncertainty. Some provisions in this draft regulation are not specific enough,
which could give rise to different interpretations by authorities and market operators
and, therefore, create legal uncertainty and hinder the compliance with this Regulation.
Definitions are one of the main sources of uncertainty, either due to their vagueness or
their absence.
In particular, the definition of AI proposed in Article 3 and the techniques listed in Annex
I can bring any automatic system or system automating some basic tasks in scope. Other
terms such as ‘natural persons’, ‘output’ or ‘(unfair) bias’ should be defined in order to
make clear what is intended to be solved and ensure that the final text applies to any
legal or natural person leveraging high-risk AI systems for their business activity.
With reference to the scope of the Regulation, the Commission should clarify that use
cases such as collections, Anti-Money Laundering controls and biometric identification
on device are not considered high-risk.
In any case, when some ambiguity remains and interactions with other regulations such
as GDPR exist, guidance, recommendations or binding opinions to be developed by the
Commission or the European Al Board seem paramount.
2. Supervision and governance. It is highly important to ensure that non-financial firms
competing with credit institutions and providing or using the same high-risk AI systems
as financial entities are subject to similar regulatory and oversight requirements.
Also, in order to ensure a level playing field between companies providing and using AI
systems for the same purpose, the opinions and recommendations issued by the
European AI Board should be binding for national authorities and be based on the
criteria established by the authority(ies) with the highest expertise on each particular
activity.
Finally, APIs should not be imposed as a mechanism for authorities to exercise their
access rights, since this would probably require large investments without bringing
significant benefits to market surveillance.
3. Bias and discrimination detection. We would prefer the Regulation referred to ‘unfair
bias’, not bias, and a definition of the term were provided. Additionally, requirements on
the examination of possible biases and the identification of any possible data gaps or
shortcomings should be realistic and take into consideration the current state of the art.
Regarding the provision of a legal base for processing special categories of personal data
page 1 of 11

for detecting and correcting biases, we think it is not necessary as this is not the ‘silver
bullet’ of bias elimination and could reduce citizens’ trust on AI technology.
4. Requirements on high risk AI systems. Some of the proposed requirements are not
sufficiently specific or do not reflect the actual state of the art.
For instance, it is not clear if full automation of some processes embedding AI would be
possible, and criteria on how to decide the type of human oversight that is needed are
not provided.
Moreover, further clarity is needed on what is meant by requiring the data has
“appropriate statistical properties”, the scope of the technical documentation to be
provided or what is understood by a substantial change in a system.
On the other hand, record keeping obligations should be more flexible and acknowledge
that logging capabilities are not the only option to satisfy them.
Detailed response
We welcome the opportunity to provide feedback to this important regulatory proposal that
will undoubtedly influence the future development of Artificial Intelligence in the European
Union.
The proposed Regulation is in general balanced, focusing only on a few AI applications that are
well identified in the legal text and reducing the regulatory uncertainty that other approaches
could have entailed.
Nevertheless, we hereby identify some aspects where we feel there is some room for
improvement. We believe that addressing these issues would increase regulatory certainty and
make compliance with this regulation easier.
For the sake of clarity we have grouped our comments in 4 broad topics: issues creating legal
uncertainty, supervision and governance, bias and discrimination detection, and requirements
on high-risk AI systems.
Issues creating legal uncertainty
Some provisions in this draft regulation are not specific enough, which could give rise to
different interpretations by authorities and market operators and, therefore, create legal
uncertainty and hinder the effective compliance with this Regulation.
Definitions are one of the main sources of uncertainty, either due to their vagueness or the
absence of a definition.
In particular, the definition of AI proposed in Article 3 and the techniques listed in Annex I could
bring any automatic system or system automating some basic tasks in scope. For that reason,
we think that some techniques in Annex I should be removed to ensure that only advanced AI
techniques are in scope. More traditional approaches such as linear regressions or expert
systems that have been widely used for decades, are based on well understood techniques, do
page 2 of 11

not present major interpretability issues and have not led to significant negative impacts on the
society are excluded from the application of these new rules.
Thus, point c and the reference to “expert systems” in point b of annex I should be removed,
so that only the most advanced Machine Learning and knowledge-based techniques that are
currently witnessing an exponential adoption and are at the center of the public debate on AI
trust are in scope of this regulation.
In addition, we suggest including some definitions in article 3 in order to ensure a common
understanding when these terms are referred in the legal text:
‘Natural person’ means a human being when non-performing a professional activity.
For the purposes of this Regulation, when natural persons are acting in their
professional capacity, they should have the same consideration as ‘non-natural’ legal
persons.
‘Output’. Although definition (1) refers to examples such as content, predictions,
recommendations or decisions interacting with the environment AI systems interact
with, a more complete definition should be provided that sets clearly the criteria to take
into consideration to decide which results of an AI systems are to be considered output.
‘Bias’. It is important to note that not all bias implies discrimination. For that reason, we
think all occurrences of the term ‘bias’ in this regulation should be replaced with ‘unfair
bias’. Taking the Ethics Guidelines for trustworthy AI drafted by the High-level Expert
Group on Artificial Intelligence as a reference, we propose to add the following
definition of unfair bias to article 3:
‘unfair bias’ means an inclination of prejudice towards or against a natural person
that can result in discriminatory and/or unfair treatment of some natural persons
with respect to others.
The terms ‘robustness’ and ‘resilience’ are frequently used in the text, sometimes as
similar terms. For the sake of clarity a definition of each of these terms should be
provided.
In addition, the definition of ‘reasonably foreseeable misuse’ is vague and some other
concepts are difficult to define univocally (‘generally acknowledged state of the art’,
‘foreseeable risk’, ‘appropriate type and degree of transparency’, ‘appropriate level of accuracy’, ‘
the interaction with an AI is obvious from the circumstances and the context of use’), what could
lead to different interpretations by competent authorities and market players.
In order to reduce the legal uncertainty that a lack of a common understanding can cause, those
vague terms should ideally be removed from the final text or, at least, their use should be
minimized and complemented with guidance, recommendations or binding opinions to be
developed by the Commission or the newly created European Artificial Intelligence Board to
ensure that supervisory expectations and the understanding of market operators are aligned.,
page 3 of 11

Moreover, there are some generic references to other ‘Union laws intended to protect
fundamental rights’ that are not identified in the text and can increase the burden on subjects
bound to comply with this regulation.
The Commission shall strive to develop a ‘self-contained’ text where generic references to
other Union laws are removed or at least replaced with specific references to EU legal texts.
With reference to the scope of the Regulation, although the AI systems subject to this
regulation are listed in Annex III (and II), the Commission should clarify that some use cases
having similarities to those identified in Annex III such as debt collections, Anti-Money
Laundering controls and biometric identification on device are out of scope of this Regulation.
Finally, the Commission should clarify how requirements in this regulation interact with those
established in other regulations. In particular, the interaction between this proposed regulation
and GDPR. For instance, model calibration requires a volume of data that conflicts with the data
minimisation principle in GDPR.
Supervision and governance
It is highly positive that the European Commission acknowledges the strength of financial
regulation and supervision by designating financial authorities as Competent Authorities for
AI systems used and provided by credit institutions as well as by deeming some requirements in
this regulation to be fulfilled by credit institutions complying with CRD and its second-level
regulations.
However, the above could also lead to fragmentation in competition, regulation and
supervision as non-credit institutions providing or using the same high-risk AI systems as
credit institutions (and competing with them) would be overseen by other authorities , which
might lead to differences in interpretation and/or enforcement. This is specially concerning if
non-banks involved in credit granting and are supervised by different national competent
authorities or not supervised at all, as it would be the case for small-scale providers.
For that reason, we would propose the Regulation to be amended so that all firms involved in
the activities referred in Annex III 5b are supervised by financial authorities and, as far as
possible, subject to requirements that reflect those established in CRD (articles 97 to 101 and
internal governance guidelines, fundamentally), when applicable. This objective could be
achieved by amending Article 43.2 as follows:
“For high-risk AI systems referred to in points 2 to 8 of Annex III, providers shall follow
the conformity assessment procedure based on internal control as referred to in Annex
VI, which does not provide for the involvement of a notified body. For high-risk AI
systems referred to in point 5(b) of Annex III, placed on the market or put into service by
credit institutions regulated by Directive 2013/36/EU, the conformity assessment shall
be carried out as part of follow the procedure referred to in Articles 97 to 101 of that
Directive 2013/36/EU”.
page 4 of 11

Also, we propose point 5 (b) of Annex III to be amended to ensure any firm using or providing AI
systems involved in creditworthiness assessment or credit scoring is bound to comply with this
regulation. Thus, point 5(b) of Annex III should be reworded as follows:
“AI systems intended to be used to evaluate the creditworthiness of natural persons or
establish their credit score, with the exception of AI systems put into service by small
scale providers for their own use;”
In addition, given the (sub)consolidated application of prudential regulation and the
continuous references to the Credit Risk Directive and its second-level legislation, it is not clear
if all bank subsidiaries (involved in credit granting or other financial activities or not) within our
prudential perimeter providing or using high-risk AI systems would always be supervised by
financial authorities.
In order to ensure a level playing field between banks, bank subsidiaries and non-banks
providing and using AI systems for the same purposes, the requirements and expectations to be
met by any company involved in the activities referred in annex III should be based on the
criteria established by the authority(ies) with the highest expertise on that activity. as it is the
case of financial authorities in activities related to credit granting.
Furthermore, for the sake of a harmonized application of this Regulation by different
authorities and across Member States, the opinions and recommendations on the
implementation of this Regulation issued by the European Artificial Intelligence Board
shall be binding for authorities of Member States. The binding nature of these acts shall be
reflected in the legal text through the addition of the following paragraph 2 to article 58:
“The opinions and recommendations referred to in paragraph c of this article, shall be
binding on National Competent Authorities”
An area where this pan-European harmonization seems especially necessary is in the
application of the sanctions framework. Article 71.1 foresees that Member States lay down
their own rules on penalties applicable to infringements of this Regulation. As this could lead to
significant fragmentation, the issuance of binding recommendations by the European Artificial
Intelligence Board would promote a harmonized enactment of the Regulation and avoid
regulatory arbitrage by market operators.
Also, in order to ease the identification of the National Competent Authorities involved in the
implementation and supervision of this Regulation, it would be useful that the European
Commission publishes in a dedicated website the list of all the authorities designated by
Member States and its remit (name of the authority, type of authority - market surveillance,
supervisory authority, notifying authority, AI applications / firms under their remit, …). For that
reason, we propose adding an additional subparagraph to article 64.4:
“The European Commission shall publish in a dedicated website the list of all the
Competent authorities designated by the Member States in accordance with this article”
page 5 of 11

With reference to the exercise of the delegation of powers established in Article 73 by the
European Commission, a 3-month public consultation on non-urgent modifications of the
annexes should be granted to ensure any stakeholder impacted by the proposed modification
can express its views. Also, once those modifications are adopted, market operators shall have
enough time to meet any new requirements.
In line with the timelines proposed for the entry into force of this Regulation and its
applicability to high-risk AI systems already in operation, we propose the following
modifications of this draft regulation:
Addition of a new paragraph (3) to article 83: “In case Annexes I and III are modified by
the Commission following the procedures described in articles 4 and 7, this Regulation
shall apply to the high-risk AI systems affected by such modifications that have been
placed on the market or put into service before the entry into force of those
modifications, only if, from that date, those systems are subject to significant changes in
their design or intended purpose”
Addition of a new paragraph (4) to article 85: “This Regulation shall apply to high-risk
systems affected by the modification of Annexes I and III from 24 months following the
entering into force of the delegated Acts defined in articles 4 and 7”
Regarding access rights of market surveillance authorities, art 64.1 establishes that they "shall
be granted full access to the training, validation and testing datasets used by the provider,
including through application programming interfaces (‘API’) or other appropriate technical
means and tools enabling remote access". The development of APIs or other technical means
for remote access by competent authorities would probably require large investments without
bringing significant benefits to market surveillance authorities that can always perform their
duties on the providers/users' premises.
Consequently, we think that article 64.1 should be redrafted as follows:
"Access to data and documentation in the context of their activities, the market
surveillance authorities shall be granted full access to the training, validation and testing
datasets used by the provider, including through application programming interfaces
(‘API’) or other appropriate technical means and tools enabling remote access."
Finally, the European Commission should clarify the following issues related to the
conformity assessment and governance procedures:
what has to be informed in the EU database in case that high-risk AI system is just a
component of a wider process or system,
where to place the CE marking when AI is used for creditworthiness assessment,
credit scoring or recruitment purposes, and
how this Regulation applies to the cross-border provision of high-risk AI systems.
Among other issues, it is not clear whether the provider of an AI system will have to get
the approval of more than a notified body, draft a conformity letter in different Member
page 6 of 11

State’s official languages or be supervised by more than one authority when it (intends
to) provide its AI system in more than one Member State.
Bias and discrimination detection
As it has been noted above, we would prefer the term ‘bias’ be replaced with the term ‘unfair
bias’ and a definition of ‘unfair bias’ be provided to make clear the issue(s) that is(are)
intended to be controlled or solved.
Additionally, Article 10 (2f, g) asks for i) examination in view of possible biases and ii)
identification of any possible data gaps or shortcomings. These requirements seem to demand
providers to avoid bias amplification, without giving any practical clue on how to technically
achieve this result. This is worrying, particularly as the issue of how to achieve such a result is
still widely discussed and largely unsolved, even at the scientific level.
On the other hand, although article 10.5 provides a legal base for processing special
categories of personal data for detecting and correcting biases, it should be noted that such
processing is not the ‘silver bullet’ of bias elimination and could reduce the citizens’ trust
on AI technology for the following reasons:
collecting this data will be difficult as it will cause hesitation on data subjects that will
be concerned of providing such information to the high-risk AI system provider or
user; and
even when this information is available and the ex-post detection of bias in a high-risk
AI system is possible, bias does not always imply discrimination, but sometimes
reflects ‘statistical facts’ and this cannot be easily discerned.
In conclusion, we think that the current practice of not feeding special categories of personal
data in AI models in order to minimize potential biases is effective enough and less intrusive
for citizens than an approach based on collecting and processing special categories of
personal data. For that reason, we think that article 10.5 of the proposed Regulation should
be removed.
Requirements on high risk AI systems
In addition to the vagueness of some definitions or the lack of definition of some concepts
already commented, some of the proposed requirements for high risk AI systems are not
sufficiently specific or do not take into consideration the actual state of the art. Indeed, it
appears that the Commision sometimes assumes that the development and use of AI at global
level and, in special, in the EU is more advanced than actually is, pointing out some issues that
are still far from being real problems and laying down some criteria that cannot be met within
the current state of the art. This increases the risk of competent authorities and market
operators to have different expectations and understanding and, therefore, increases the risk
of non-effective compliance.
For instance, article 9 requires providers to communicate any residual risk to users, but does
not state how these residual risks shall be communicated to the user.
page 7 of 11

Also, article 10.3. requires training, validation and testing data sets to have “appropriate
statistical properties” and be “relevant, representative, free of errors and complete” . However,
what is meant by “appropriate statistical properties” is not explained and asking for full
representativeness, completeness and accuracy in data is not realistic. For that reason, we think
this requirement should be redrafted as follows:
“Training, validation and testing data sets shall be sufficiently relevant, representative,
free of errors and complete. They shall have the appropriate statistical properties,
including, where applicable, as regards the persons or groups of persons on which the
high-risk AI system is intended to be used. These characteristics of the data sets may be
met at the level of individual data sets or a combination thereof.”
In case the Commission prefers to maintain the reference to “appropriate statistical properties”,
it should state clearly the meaning of this concept and its expectations thereon.
Furthermore, we believe that data and data governance requirements in article 10 (in
special art. 10.6) should be deemed fulfilled by credit institutions complying with internal
governance requirements in CRD and/or having implemented BCBS239
Regarding the technical documentation, article 11 should clarify if this documentation shall
just cover the AI software component or the whole system, including processes, user interfaces
and other elements
Moreover, point 5 in annex IV requires the technical documentation to include a description of
any change made to the system through its lifecycle. We feel only substantial changes should be
documented and, therefore, point 5 in annex IV should be redrafted as follows:
A summary description of any substantial changes made to the system through its
lifecycle;
With reference to record-keeping, article 12 establishes automatic logging and logging
capabilities as the cornerstone of record-keeping and system traceability.
However, not all systems include automatic logging capabilities and those that have automatic
logging available, usually provide it for identifying and solving temporary technical issues, not to
keep a historical record of the system functioning.
For the latter, there are other techniques less onerous in terms of processing and storage that
can ensure appropriate record-keeping capacities and reproducibility of the system (although
full reproducibility of the system is usually not possible), such as periodically backing up the
code of the system and the data processed.
Therefore, we would like the European Commission to make the following modifications in the
proposed regulation text:
Art 12.1. High-risk AI systems shall be designed and developed with capabilities
enabling the automatic recording of events (‘logs’) while the high-risk AI systems is
page 8 of 11

operating. Those logging capabilities shall conform to recognised standards or common
specifications.
Art 12.2. The logging capabilities shall ensuring a level of traceability of the AI system’s
functioning throughout its lifecycle that is appropriate to the intended purpose of the
system.
Art. 12.3. In particular, logging capabilities shall The system shall include mechanisms
that enable the monitoring of the operation of the high-risk AI system with respect to
the occurrence of situations that may result in the AI system presenting a risk within the
meaning of Article 65(1) or lead to a substantial modification, and facilitate the
post-market monitoring referred to in Article 61.
Art. 12.4. For high-risk AI systems referred to in paragraph 1, point (a) of Annex III, the
logging capabilities record-keeping mechanisms shall provideallow, at a minimum: (a)
recording of the period of each use of the system (start date and time and end date and
time of each use); (b) the identification of the reference database against which input
data has been checked by the system; (c) the identification of the input data for which
the search has led to a match; [..]
Art 16 d. when under their control, keep the logs automatically generated by their
records of their high-risk AI systems referred to in Article 12;
Art. 20Automatically generated logs Records maintenance
1. Providers of high-risk AI systems shall keep the logs automatically generated by their
records of their high-risk AI systems referred to in Article 12, to the extent such logs
records are under their control by virtue of a contractual arrangement with the user or
otherwise by law. The logs records shall be kept for a period that is appropriate in the
light of the intended purpose of the high-risk AI system and applicable legal obligations
under Union or national law.
2. Providers that are credit institutions regulated by Directive 2013/36/EU shall
maintain the logs automatically generated by records of their high-risk AI systems
referred to in Article 12 as part of the documentation under Articles 74 of that
Directive.
Art 23. Providers of high-risk AI systems shall, upon request by a national competent
authority, provide that authority with all the information and documentation necessary
to demonstrate the conformity of the high-risk AI system with the requirements set out
in Chapter 2 of this Title, in an official Union language determined by the Member State
concerned. Upon a reasoned request from a national competent authority, providers
shall also give that authority access to the logs automatically generated by the records
of their high-risk AI system referred in Article 12, to the extent such logs records are
under their control by virtue of a contractual arrangement with the user or otherwise by
law.
page 9 of 11

Art. 25.2.(b) provide a national competent authority, upon a reasoned request, with all
the information and documentation necessary to demonstrate the conformity of a
high-risk AI system with the requirements set out in Chapter 2 of this Title, including
access to the logs automatically generated by the records of their high-risk AI system
referred in Article 12, to the extent such logs records are under their control by virtue
of a contractual arrangement with the user or otherwise by law;
Art. 26.5. Importers shall provide national competent authorities, upon a reasoned
request, with all necessary information and documentation to demonstrate the
conformity of a high-risk AI system with the requirements set out in Chapter 2 of this
Title in a language which can be easily understood by that national competent authority,
including access to the logs automatically generated by the records of their high-risk AI
system referred in Article 12, to the extent such logs records are under their control by
virtue of a contractual arrangement with the user or otherwise by law. They shall also
cooperate with those authorities on any action national competent authority takes in
relation to that system.
Art. 29.5. Users of high-risk AI systems shall keep the logs automatically generated by
the records of their high-risk AI system referred to in Article 12, to the extent such
logs records are under their control. The logs records shall be kept for a period that is
appropriate in the light of the intended purpose of the high risk AI system and applicable
legal obligations under Union or national law.
Users that are credit institutions regulated by Directive 2013/36/EU shall maintain the
logs records of their high-risk AI system referred in Article 12 as part of the
documentation
concerning
internal
governance
arrangements,
processes
and
mechanisms pursuant to Article 74 of that Directive.
In relation to human oversight, article 14.1 requires high-risk systems to include “appropriate
human-machine interface tools, that they can be effectively overseen by natural persons during
the period in which the AI system is in use” and article 14.4 indicates 5 different human
oversight measures to be applied ‘as appropriate to the circumstances’.
It should be taken into consideration that requiring human involvement in each transaction
would prevent the full automation of processes embedding AI capabilities and limit the ability to
reap the benefits that such automation can bring. Moreover, this would probably not improve
the ability to detect the misperformance of high-risk AI systems that are more frequently
identified at aggregated level.
Consequently, the Commission should make clear that full automation of some processes
embedding AI is possible and provide criteria to support providers and users when
deciding which human oversight measures in article 14.4 best suit a given use case.
Regarding article 15 and, in addition to our request for a definition of robustness and resilience,
we would welcome some clarification on the levels of robustness and resilience expected
by supervisors depending on the use case.
page 10 of 11

Finally, we would welcome some guidance from the Commission or the European Artificial
Intelligence Board on how to meet the requirement to inform of the operation of an
emotion recognition system or a biometric categorisation system established in article
52.2.
page 11 of 11