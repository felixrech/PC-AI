Side 1 af 3 
August 5, 2021 
Feedback to the proposal for an AI Act by the 
European Commission 
1 KMD’s position 
1.1 
General remarks to the proposed regulation 
KMD welcomes the proposal for an AI Act by the European Commission. 
KMD finds that regulation of AI will strengthen trust in the solutions and providers of the 
technology, and as such it will benefit the tech sector, authorities, and society at large. As AI 
has achieved a certain level of maturity and is being implemented in a wide array of software 
solutions, the time is ripe for establishing common standards and regulation of the use of AI 
across the EU. Clear and common requirements for AI solutions in EU makes the competitive 
situation equal across the Union. 
KMD also applauds the idea of creating an EU alternative to the implementation of AI in third 
countries which to various extents ignores the need to secure fairness, democratic control, 
transparency, and measures to combat bias and discrimination. As part of the proposed AI Act 
and the attention following from the proposal, KMD expects a more qualified and informed 
public debate about AI. 
KMD, however, finds that there is room for improving the proposal as per our remarks below. 
1.2 
KMD’s concerns and suggestions 
KMD develops IT solutions for every aspect of the changing digital needs of modern societies 
and organizations. AI regulation will affect a great part of the solutions that KMD – as a leading 
supplier of govtech and business-critical solutions – has already placed and plan to place on 
the market. 
As societies grow and evolve, digital solutions play an increasingly vital role in fostering 
efficiency, growth, and welfare. For half a century, KMD’s contributions to Danish society has 
helped shape one of the most modern and progressive public sectors in the world. The high 
degree of trust that Danes have in digital and data driven solutions today stems from decades 
of developing solutions that are reliable, secure, transparent, unbiased, and explainable. KMD 
aims to continue building on this foundation in the future, and AI is a central component of this 
vision. 
An overall concern from KMD is finding the right balance of the regulation, so that innovation 
and development will not be impaired, and the requirements for testing, documentation etc. 
will not stand in the way of using and inventing AI solutions that benefit society and citizens. 
1.3 
AI is weakly defined 
The object of the regulation is AI, but without a more specific definition, even widespread 
statistics software and simple search engines (cf. Annex 1) may fall under the term AI. This 
could potentially mean that systems using simple keyword searches would be classified as 
“high risk”, and that seems to be beside the target. 

Side 2 af 3 
KMD suggests a more concrete definition of AI in the proposal similar to the UN definition of 
AI​Draft text of the Recommendation on the Ethics of Artificial Intelligence - UNESCO Digital Library ​  where AI operates by means of knowledge modelling and representation and by exploiting 
data and calculating correlations. 
1.4 
Standardization 
KMD welcomes the focus on standards, but finds it challenging that the EC in hindsight can 
decide that the relevant harmonised standards are insufficient. As a vendor, it should be 
possible to place a product on the market if that product follows the harmonised standards 
given at the time of development. 
1.5 
Quality measures 
KMD acknowledges that it is possible to maintain very high data quality, as is also expressed in 
the proposal, but it is not possible to guarantee that data is “free of errors and complete”. 
In general, the proposal assumes that AI systems can deliver predictions that are always 
accurate, reliable, and transparent. These concepts seem rather subjective in their nature and 
a subject for interpretation, which is why KMD recommends to specify the requirements for 
how to measure these concepts. 
2 
Clarification of roles and responsibilities 
2.1 
Unclear definition of roles 
In the proposal, concepts like user, provider, and distributor are used with different 
connotations in different contexts. For instance, the provider role shifts between the customer, 
designer, developer, distributor, operator, company, and importer. 
A consequence of the unclear roles and responsibilities regards infringement to the 
requirements, e.g. when determining if the offender is a company or an individual. It is 
complicated because the provider role lacks a unified definition. Moreover, it may be expected 
that entities procuring IT-solutions may want to push the full responsibility of the AI solution to 
the IT-supplier. A clearer role definition would define which responsibilities they cannot 
prevent. 
It must be stressed that the entity putting the service into the market is not necessarily the 
same legal entity as the one placing it on the market, and the proposal would benefit from 
clarifying the roles in greater detail (Recital 53; Title I Article 3(2); Title III Article 18; Title III 
Article 27). 
2.2 
Human oversight 
Human oversight (Title III Article 14) is a means to increase trust in AI solutions and help 
place responsibility to a legal person if the AI system fails. In the proposal, human oversight 
“[…]shall aim at preventing or minimizing the risks to health, safety or fundamental rights that 
may emerge[…]”. However, the requirements described are difficult to meet in a meaningful 
way. 
The individuals assigned to oversee the solutions are not required to have an expertise in the 
field, but would that really be the right type of human oversight? It is not clear how the 
provider may ensure that the user fully understands the AI solution and outcomes, nor who 
defines if the system is “effectively overseen”. 
KMD suggests as an alternative to the human oversight described in the proposal a 
combination of explainable AI and assignment of responsibility to a human legal entity. 

Side 3 af 3 
2.3 
Sandboxes and data spaces 
Another requirement is the so-called AI regulatory sandboxes (Title V Article 53) for testing 
and validating AI systems before placement on the market. 
It is unclear whether these sandbox environments will be provided by member states’ 
authorities themselves or the European Data Protection Supervisor, or if the task will be 
outsourced to private companies. The same goes for the European common data spaces 
(Recital 45). KMD suggests a further clarification in the proposal. 
3 
Clarification of various terms and concepts 
3.1 
High-risk classification 
As for the protection of public interest in high-risk AI systems, the proposal states that 
common normative standards should be established (Recital 13). KMD suggests a clarification 
of whether these common normative standards are part of the report on AI standards. ​Cf. JRC Publications Repository - AI Standardisation Landscape: state of play and link to the EC proposal for an AI regulatory 
framework (europa.eu) ​  
The requirements regarding accuracy, consistency, robustness, appropriateness etc. in highrisk AI systems (Title III Article 15) furthermore lacks specification. 
Title XII Article 83(2) regards the application of the regulation to high-risk AI systems already 
on the market. It states that these solutions will not be subject to the new regulation unless 
there are significant changes to design or purpose. KMD suggests a further elaboration on 
what constitutes a “significant change”. 
3.2 
Placing on the market and putting into service 
The difference between “placing on the market” and “putting into service” (Title 1 Article 
3(9,11)) is subtle but regards first time a solution is made available to the market respectively 
first use. This raises a question about retrained AI systems for a new problem dataset. A 
clarification of whether using retrained AI systems are considered “first use” would be 
beneficial. 