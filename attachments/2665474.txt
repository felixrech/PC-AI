- 1 - 
SPECTARIS Position Paper 
On the proposal for a Regulation of the European Parliament and of the Council laying down 
harmonised rules on Artificial Intelligence (Artificial Intelligence Act) 
Regulatory Affairs 
regulatoryaffairs@spectaris.de 
www.spectaris.de 
SPECTARIS • German Industry Association for Optics, 
Photonics, Analytical and Medical Technologies 
Werderscher Markt 15 
10117 Berlin 
Tel. +49 30 414021-10 
medizintechnik@spectaris.de 
www.spectaris.de 
Stand: Berlin, Freitag, 29. Mai 2020 

- 2 - 
1. Introduction: 
On April 21st 2021, the European Commission published a proposal for a Regulation of the European Parliament and 
of the Council laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act; AIA), as part of a new 
comprehensive European framework that aims at regulating products and services with AI elements. The ever-growing 
use and development of AI technologies requires regulation in order to further research and innovation, while also 
maintaining a high level of safety for EU citizens - SPECTARIS agrees with these objectives and welcomes this 
proposed AI Act. 
Nevertheless, the planned legislation includes additional regulatory requirements for medical devices by rigidly 
classifying most medical devices as high-risk AI devices - regardless of their respectively used AI elements and risk 
class as defined in MDR (Regulation (EU) 2017/745 on medical devices) and IVDR (Regulation (EU) 2017/746 on in 
vitro diagnostic medical devices). Since applicable medical technology regulations have already set extensive AI 
requirements for medical devices and software, this also points to a more general concern: The result of two parallel 
regulatory frameworks regarding AI components in medical devices could lead to regulatory overlapping, discrepancies 
and additional legal uncertainty for medical technology manufacturers. 
Hence, SPECTARIS recommends further alignment of the proposed AI Act with MDR & IVDR as well as additional 
conceptual clarifications and changes. Detailed feedback on key issues – including MDR/IVDR alignment, definitions, 
harmonisation, datasets, human oversight and transitional period - can be found in the following sections. 
2. In detail: 
 Medical devices are already classified by their risk level based on MDR and IVDR – ranging from class I (low 
risk) to class III (high risk) devices. Medium and high- risk devices (starting from class IIa) have to undergo strict 
conformity assessments carried out by Notified Bodies. However, the introduced classification system in the AIA 
draft document seems to automatically categorize all medical devices of medium and high risk with AI elements 
as high- risk devices under the AIA, regardless of their initial MDR/IVDR risk class and the specific AI 
component that is included. In its current form, this AIA classification itself is unclear – it not only lacks examples 
of how to differentiate between low- and high-risk AI systems, but also clearly differs from and undermines the 
well-established MDR/IVDR classification system. In order to prevent legal uncertainity and misunderstandings 
for manufacturers, or - at worst - even direct contradictions or duplications, SPECTARIS suggests fully 
aligning the AI Act with the already existing MDR and IVDR classification system. Also, to offer 
manufacturers a better understanding of the distinction between low- and high-risk devices, more examples 

- 3 - 
need to be provided. 
 SPECTARIS also considers it necessary to further align key requirements and definitions with 
MDR/IVDR. This includes aligned requirements on Quality Management Systems (QMS), cybersecurity, and 
EUDAMED for vigilance and post-market surveillance, as well as alignment of requirements for Notified Bodies 
(e.g. submissions of technical documentation, conformity assessments, CE marking). Only comprehensive 
alignment of the two frameworks will provide legal certainty for manufacturers and thus prevent duplicated and 
unnecessary administrative procedures, such as doubled conformity assessments or two technical 
documentations. This will also help to avoid further capacity issues that Notified Bodies already have to deal 
with today. Furthermore, definitions need to be matched with MDR/IVDR terms, since current terms (e.g. 
“provider” vs. “manufacturer”) clearly diverge. 
 Consistent implementation of the AIA across all EU member states is particularly important – harmonisation 
should be pursued in order to avoid differing national efforts (as in the case of the GDPR) and a fragmented 
Single Market. Harmonisation is not only crucial to maintain a high level of AI safety throughout all of Europe, 
but also for providing suitable grounds for innovation: SPECTARIS welcomes the idea of regulatory sandboxes. 
However, relevant sectoral authorities (e.g., the AI Board and/or MDCG) must leverage the outcomes from 
national regulatory sandboxes at the EU level, so that AI systems created and tested in such regulatory 
sandboxes could be easily moved to and placed on the market in other member states. 
 Additionally, the AIA’s broad definition of an “AI system” (Article 3, point 1 and Annex I) that this legislation 
presents, only complicates the process of distuinguishing between basic medical device software and AI-based 
medical device software. For example, in Annex I (b) and (c) “knowledge bases” as well as software including 
“statistical approaches” or “search and optimization methods” are deemed as artificial intelligence systems. This 
could have the effect of including medical devices into the scope of the Regulation merely if they embed basic 
software functionality that enables users to search for information from a dataset, or rank the results of a query 
in terms of their relevance. Hence, most medical device software would be considered high-risk AI systems, 
even if devices only include basic software components. In effect, this broad definition of AI systems could lead 
to an unnecessarily wide range of medical devices falling under this regulation by falling under the category of 
high-risk devices, despite only using software components that are usually not characterised as actual AI 
elements. SPECTARIS thus recommends to remove statistical approaches, simple databases as well as 
search functions from Annex I, or to alternatively offer a more comprehensive and clear-cut depiction 

- 4 - 
on how to differentiate between basic software and AI systems. 
 Article 10(3) of the proposed legislation requires high-risk AI systems datasets to be “relevant, representative, 
free of errors, and complete.” In practice, however, this is often not possible because the data sets are usually 
not entirely accurate. Completely error-free data cannot always be ensured, which is why the current wording 
could prevent the use of data in healthcare. Article 10(3) should therefore be consistent with Recital 44, by 
adding “sufficiently” to the requirements for training, validation and testing data. 
 Article 14 of the proposed legislation covers human oversight. The AI Act in its current form would require that 
AI systems should be designed and developed so “that they can be effectively overseen by natural persons 
during the period in which the AI system is in use”. However, high-level computing far exceeds human 
capabilities, which means that devices that have to rely on human oversight cannot carry out their functions as 
intended, since their purpose is to provide solutions at a much higher speed and accuracy than humanly 
possible. Tying the AI system’s functionality to human oversight could be restrictive and would introduce new 
risks as functions might not be carried out quickly or sufficiently enough, which would render the development of 
certain medical devices (e.g. AI surgery robots) impossible. Human oversight in itself is certainly warranted to 
reduce risks - nevertheless, it needs to be set at an appropriate level wihout inhibiting the initial function of the 
medical device AI system, as this could create new risks by itself and hamper innovation efforts. 
 The transitional period of 24 months – as proposed in Article 85 - does not allow sufficient time for companies to 
adapt to such a complex and comprehensive regulatory framework. We suggest a transitional period of at least 
48 months. 
Additional technical comments are listed in the following section. 
3. Annex: Technical comments & proposed changes: 
Page Reference 
Comment / Question 
Proposed change 
12 
5.2.2 Prohibited 
Artificial 
Intelligence 
Practices 
For the risk-based approach, there is a 
distinction made between (i) unacceptable 
risk, (ii) high risk, and (iii) low or minimal risk. 
Examples of prohibited practices are provided. 
The risks to be considered are in regards to 
To determine whether a system is defined as 
high risk or low/minimal risk, a scoring process 
should be implemented to support providers of 
AI systems in defining their systems. 

- 5 - 
safety and human rights. However, the 
distinction between high-risk versus low-risk 
systems remains unclear. 
48-49 
Article 10, Data 
and data 
governance 
What type of data acquisition and what 
amount of data is recommended or specified? 
Since this will probably be different for each 
project: Are there statistical methods that are 
recommended for determining the data 
analysis? 
Recommendations: 
Insert references, State of the Art 
Statistical Technology 
53-54 
Article 17, Quality 
Management 
System 
Cybersecurity is not mentioned in this article. 
Cybersecurity assessment and standards for 
cybersecurity are also an aspect of the quality 
management system and may also be 
mentioned in this article. 
58 
Article 29, 
Obligations of 
users of high-risk 
AI systems 
In this article, it is mentioned that users exert 
control over the input data. 
It could be emphasized that the provider of the 
AI system (manufacturer) is responsible for 
defining the input data (giving guidelines 
regarding appropriate data). For high-risk AI 
systems, the provider should also have 
functional measures in place to prevent 
inappropriate data from being used with their 
systems (e.g. plausibility checks) as this can 
have a direct impact on the output of the AI 
system. 
58 
Article 29, 
Obligations of 
users of high-risk 
AI systems 
This article mentions that users shall monitor 
the operation of the high-risk AI system. How 
should this monitoring be implemented? 
Additional mention of the provider of the AI 
system as an actor responsible for 
implementing monitoring capabilities in their AI 
systems. 
SPECTARIS is the German industry association for optics, photonics, analytical and medical technology based in Berlin. 
The association represents 400 predominantly medium-sized German companies. 
The consumer optics (ophthalmic optics), photonics, medical technology and analytical, bio and laboratory technology sectors 
achieved a total turnover of over 71 billion euros in 2020 and employ around 327,000 people. 