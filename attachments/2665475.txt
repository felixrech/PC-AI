RELX welcomes the opportunity to provide feedback on the European Commission’s proposed
regulation on Artificial Intelligence (AI). The regulation sets out how the Commission wishes to
develop harmonised rules on AI and will have a significant impact on the future development and
use of AI systems in Europe. It marks the world’s first specific piece of legislation targeted at
regulating the use of AI. The regulation is an opportunity to strengthen the EU’s position as a first
mover on responsible technology oversight. 
AI is a central issue to the global digital economy and will likely be at the heart of future economic
transformation across the globe. Global competitiveness in AI is high and the development of
effective AI is a priority for countries around the world. The US, UK and China are all direct
competitors to the EU in both the academic and commercial sectors and are looking to lead in AI
development. This regulation is therefore an important milestone in the EU’s bid to be a leader in
responsible AI. 
RELX supports the approach the Commission has taken, not trying to regulate the technology itself
but rather applying a risk-based approach and limiting its application to high-risk AI use cases. That
said, it is important that where there are overlaps with other EU regulations, such as financial
regulation, sector-specific regulation, and GDPR, this regulation should be without prejudice. We
support the encouragement of industry-led standards and innovation friendly measures including
sandboxes. RELX looks forward to working with the EU institutions to finalise this important Act.
RELX is a global provider of information-based analytics and decision tools for professional and
business customers across a range of sectors, including financial services, legal, agriculture, science,
technology, medical, healthcare and energy. We employ 33,000 people worldwide and support
customers in 180 countries. We utilise technology and data to help our customers improve their
decision making across the sectors we serve. We help scientists make new discoveries, doctors and
nurses improve the lives of patients, lawyers win cases, prevent online fraud and money laundering
and insurance companies evaluate and predict risk. 
AI is a key interest for RELX’s businesses. RELX is both a significant consumer and developer of the
latest technologies, investing annually approximately €1.2 billion in technology and employing over
9,000 technologists. Some of these techniques fall within the broad definition of Artificial
Intelligence. Our technology is developed across the world, with a significant number of our data
scientists and developers in Europe, with tech hubs in London and Amsterdam. As a matter of best
practice and corporate mission, RELX focuses on the responsible application of AI and aims for fair
and transparent application of these approaches.
This paper outlines RELX’s initial views on the Commission’s proposed regulation, ahead of further
discussion with the EU institutions. Our goal is to support innovative technologies that benefit
society while ensuring that those technologies respect an individual’s fundamental rights. 
Introduction
RELX response to the proposal for a regulation 
on Artificial Intelligence 

Title I: General provisions
 Article 3: ‘artificial intelligence system’ (AI software system) means software that is developed
with one or more of the techniques and approaches listed in Annex I and can, for a given set of
human-defined objectives, generate outputs such as machine-generated content, predictions,
recommendations, or decisions influencing the environments they interact with; 
 Annex 1(c): ‘statistical approaches’ captures far more activity than is necessary or commonly
Article 2 and Recital 11 - Scope 
We are concerned by the Act’s potential extra-territorial reach as laid down by the third category of
applicability in Article 2, “providers and users...located in a third country, where the output
produced… is used in the Union”. Recital 11 notes that AI systems that are neither placed on the
market, put into service nor used in the EU might fall within its scope, where an activity “would
qualify as high-risk and whose effects impact natural persons located within the Union”. This is a
very broad reach of regulation. There is a risk that EU citizens will not be able to benefit from global
innovations as a result of this. For example, Europeans may not benefit from global medical
research where that research is undertaken outside the EU if the research did not build compliance
with this EU regulation into its model. 
Article 3(1) and Annex 1 – Definition of AI
The way the EU defines AI is extremely important, since this is the world’s first legal framework and
this definition is likely to be used by other organisations/states when they start to regulate. We
recognise that AI is difficult to define, as it is a term which covers a broad range of technologies.
However, we suggest the chosen definition laid down in Article 3(1), which, whilst being based upon
the OECD definition is more extensive and could capture techniques which are not widely
considered to be AI. We would encourage the Commission to tighten up the definition to ensure
legal clarity and consistency. 
We would propose two specific changes to the definition, indicated in bold text below:
1.
2.
 understood to be AI. We would suggest editing annex 1(c) to: ‘statistical approaches in so 
 far as those statistical approaches contribute to a high-risk AI system’, to ensure the 
 definition more accurately reflects the risk involved in using those statistical approaches, 
 rather than suggesting that AI includes traditional mathematical techniques. 
Article 3 – Other definitions 
RELX believes it is helpful that the act distinguishes between providers (Article 3(2)) and users
(Article 3(4)) of AI systems, and think this is a useful way of recognising the different responsibilities
of various players that sit within an AI ecosystem. 
The definition of ‘publicly accessible space’ (Article 3(39)) is extremely broad and we do not think
that Recital 9 which seeks to add some limitations, will be sufficient. We would propose some
limitation to “publicly accessible space”, for example by adding “to which the general public have a
right to be present, without invitation” which would cover examples such as the forecourt of a
restaurant or bar.

We also have reservations with the overly broad definition of “serious incident” (Article 3(44)) which
loosens the link between causation and liability. Liability requires a reasonable standard of
causation, where the product’s defect must be the cause of the serious incident. We would suggest
using the same standards as those of the product liability directive and the (draft) NIS 2 directive to
ensure consistency with other legislative acts. 
Articles 5 and 7 – Additions to prohibited and high-risk applications 
Articles 5 and 7 would empower the Commission to regularly update the list of applications that are
either prohibited or deemed high-risk. This could create significant uncertainty for businesses
operating in a variety of sectors if they face the prospect of finding themselves within scope of the
regulatory requirements which are extensive and require planning. We would like to see a
transparent process, with (i) clear criteria under which the Commission can act, (ii) timelines, and (iii)
how decisions can be challenged. We would ask that any update of the Annex includes consultation
with business. 
Article 5 - Prohibited AI practices
We understand and support the principles and reasoning behind prohibiting certain practices
deemed as unacceptable, in that they contravene EU values, in particular violating fundamental
rights. However, we urge caution not to cast the net too broad by using vague language that creates
uncertainty. This does not lend itself to companies innovating in this space. Indeed, this provision
could inadvertently ban services that private organisations offer which greatly help protect citizens,
for example the fight against fraud. This is because of language such as “beyond a person’s
consciousness” or “likely to cause psychological harm”, both of which are open ended and undefined.
As mentioned above we are also concerned about the powers retained by the Commission to update
this list. 
Article 6 – Classification rules for high-risk AI systems 
We agree with the Commission’s risk-based approach and that the strictest requirements should be
attached to the applications of AI that present the greatest risk to fundamental rights. However, the
current approach is broad and, as set out above in relation to the definition of AI, could capture a
significant number of applications which are not really considered high-risk. For example, within the
RELX Exhibitions business, we seek to optimise an individual’s attendance at an exhibition by
matching them with other attendees. This uses profile data, provided at registration and involves an
18th century Bayesian estimation. Whilst this is not high-risk AI, it risks being caught up in the overly
broad scope of the regulation. 
The regulation classifies high risk AI systems by type of system or sector. We do not believe this
reflects the real risk of harm caused and is therefore disproportionate. Many systems will likely fall 
in-between high risk and minimal risk (i.e outside scope). This could lead to organisations adopting a
precautionary approach and unnecessarily incurring heavy compliance costs, or choosing not to put
the system on the market (in the EU) at all. The risk-based approach could be improved by applying 
a sliding scale of risk.
Titles II and III: Prohibited and high-risk AI

Contact
Your NFP Name
123 Anywhere St., Any City, ST 12345
123-456-7890
www.reallygreatsite.com
hello@reallygreatsite.com
@reallygreatsite
We would suggest that the risk-based approach also needs to recognise the way in which an AI
system is being used in order to determine whether that system is high-risk or not. A critical
criterion is the extent to which an AI system is heavily relied upon by the user, with a distinction to
be made between an AI system used in an advisory capacity, or one used directly to inform
decision-making. For example, where an AI system is one of a number of inputs, including
perhaps other AI systems and human intervention, it would be seen to be merely advisory.
However, where an AI system’s output is the sole determinant factor in a decision, would be an
example of being “heavily relied upon”. There are of course a range of points in between and
exactly where along this spectrum a certain usage sits would be a consideration for the user to
assess. It may be helpful to consider the following decision-tree: 

Contact
Your NFP Name
123 Anywhere St., Any City, ST 12345
123-456-7890
www.reallygreatsite.com
hello@reallygreatsite.com
@reallygreatsite
Considering this extra layer of risk ie the extent to which the AI system is relied upon for the final
outcome, helps confirm exactly where the focus of this regulation should be on targeting high-risk
AI systems without capturing those AI systems which do not lead to high-risk decisions. 
The assessment of whether an AI system is, or is intended to be, heavily relied upon, and therefore
high-risk, should form part of the risk assessment and risk management systems put in place. This
assessment cannot be made based on a specific number of inputting factors but will depend on
context. As noted above, in some contexts, heavily relied on might mean it is the only system being
used, or there could be multiple systems being used but with a particular reliance on one particular
system. We would not foresee a specific threshold for the number of systems being used that
determined how reliant a decision is on a particular AI system. This should be considered as part of
the risk assessment and management system. 
CHAPTER 2: Requirements for high-risk AI systems
Article 9 – Risk management system 
With regard to the testing of high-risk AI systems, referred to in Article 9(7), we agree this is a critical
aspect and would welcome, at the appropriate time, clear and practical guidance from regulators
and the opportunity for provide input to regulators in order to assist with compliance. 
Article 10 – Data and data governance
Article 10(3) refers to training, validation and testing data sets being free of errors. We do not
believe it is possible to comply with this incredibly strict requirement. Developing a programme
which is 100% accurate is an impossible task. It also risks creating a distorted view about the
security of the testing programme given algorithms and data evolve quickly. Our understanding of
what is representative or reliable also evolves over time, meaning a dataset might be considered
100% accurate today but not tomorrow. It also risks presenting users with a false sense of reliability
of a particular AI system. There is also lack of clarity with regard to completeness of data. Data can
be missing or uncollected for a variety of reasons but remain of value. 
We would suggest that instead of a requirement to be free of error, there should be industry-led
agreed standards. We know that where standards provide legal certainty, they can be useful. If the
EU encourages industry-led AI standards which make it easier for businesses to operate in the
single market, then this regulation will be a significant positive contribution to the global
development of AI. This is particularly relevant when striving to achieve the most accurate datasets
possible. For example, one approach could be something akin to “nutrition labelling”, which
demonstrates the health of datasets to those using them. This allows technologists to make
informed decisions about the data they are using in an AI system without restrictive or prohibitive
requirements which are impossible to comply with. Industry is already working on this concept and
should be encouraged to continue with developing common standards. 
We would also suggest that the fines associated with breaching the provisions of this Article are
extremely high and we believe could be disproportionate to the harm caused. They represent a
significant increase from those that have become the recognised norm in EU law. 

Encouraging an industry agreed standard on accuracy with a means to correct data if found to be
inaccurate should be sufficient, with fines reserved for extreme cases. 
Finally, there is no reference to respecting intellectual property (I rights contained within data used
by AI systems. We believe there should be a reference to this to ensure that the AI regulation
requires organisations which utilise third-party data in their training systems do so in full respect of
IP rights. IP provides important incentives to invest in high-quality content, which leads to better AI
outcomes. 
Article 13 – Transparency and provision of information to users 
We agree that transparency is important. Who the transparency is aimed at is also important. The
level of transparency required would depend upon who its audience is. These could be experts at
one end of the spectrum and general users at the other. 
Article 14 – Human oversight
The issue of human oversight of AI systems can be difficult. The proposed regulation requires that
high-risk AI systems must be ‘effectively overseen by natural persons’. Our reading of this Article is
that in designing an AI system, it must be possible for effective human oversight to take place,
including compliance with the requirements in Article 14(4)(a-e). Based on this reading we agree
that the proposal achieves a suitable level of human oversight. 
It is not clear whether the provider needs to have continuing access to the user’s operations in
order to comply with this, although we expect this to be dealt with through contractual and
commercial relationships. 
Human oversight should remain proportional, so that it maintains the benefits of the AI system
while providing important checks to avoid the identified risks. 
Questions around human oversight also reinforce the importance of ensuring the process of
determining high-risk AI systems is suitable. As noted above, when an AI system is playing merely
an advisory role by its very nature there will be human oversight built into the application of the AI
system. However, where the AI is the ultimate decision-maker the requirements in this article for
ensuring human oversight is built into the design of the system become more important. 
Article 15 – Accuracy, robustness and cybersecurity 
AI systems should seek to be as accurate and reliable as possible. However in reality it is not always
possible to anticipate an AI system’s resilience with regard to errors, consistencies or even external
macroeconomic factors which can alter a model. It is also important to remember that humans are
not 100% accurate, which is a relevant factor when comparing AI and human outcomes. Given that
humans design AI systems, inaccuracy may also enter a system due to human error. As written, the
regulation would hold AI systems to higher standards than humans.

Contact
Your NFP Name
123 Anywhere St., Any City, ST 12345
123-456-7890
www.reallygreatsite.com
hello@reallygreatsite.com
@reallygreatsite
Furthermore, accuracy often comes with trade-offs. A highly accurate system may be less
transparent or explainable, while a more transparent system may produce a less accurate result.
For example, sundials are easy to explain and will reliably tell you which hour you are in, but not
accurately so. A hi-tech precision watch will accurately tell you what second you are in but may be
harder to use and more prone to failure if it is not kept in perfect operating conditions. The
regulation should recognise this trade-off and allow proportionality and flexibility for providers and
users to exercise judgement in that trade-off, while also being responsible and validating the
model when necessary. 
CHAPTER 3: Obligations of providers and users of high-risk
systems and other parties
Article 16 – Obligations of providers of high-risk AI systems 
We believe the requirement to notify and register high-risk AI systems with authorities could
become anti-productive and have an impact on competitiveness and innovation. It could prevent
organisations from being agile due to unnecessary bureaucracy, which could impact innovation,
due to the need to constantly register new applications, or update existing ones. 
CHAPTER 5: Standards, conformity assessment, certificates,
registration
Article 42 – Presumption of conformity with certain requirements 
We would like to understand better the meaning of Article 42. Specifically, we would like clarity on
whether it implies that systems trained on EU datasets do not require additional conformity
assessments, meaning therefore that those trained on non-EU datasets would. If this is the correct
interpretation, it would lead to a two-tier conformity system which would undermine the global
nature of data and create an additional barrier to investment in Europe. 
Article 43 – Conformity assessment
RELX welcomes the fact that for the majority of high-risk AI applications conformity assessments will
be conducted via internal control process in line with Annex VI. RELX already carries out selfassessment processes to be compliant with GDPR requirements and will be able to benefit from the
learnings in setting them up, as well as being able to leverage the processes we have already
established.
We do however seek clarity on Article 43.1 which refers specifically to biometric identification and
categorisation of natural persons; specifically as to how would an organisation determine whether
to go for internal control (43 (1) (a) or external notification (43 (1) (b).
Article 49 – CE marking of conformity 
The Commission has proposed the use of CE marking for conformity; however it is not clear how
this would operate in a digital context. Greater clarity is needed on what is expected for the affixing
of and maintaining a CE mark. 

Contact
Your NFP Name
123 Anywhere St., Any City, ST 12345
123-456-7890
www.reallygreatsite.com
hello@reallygreatsite.com
@reallygreatsite
Article 53 – AI regulatory sandboxes
RELX welcomes the inclusion of AI regulatory sandboxes in the regulation which will encourage
innovation in the AI space. However, we would caution that onerous compliance burdens could act
as a damper for companies wishing to innovate in the EU. It is crucially important to get this balance
right.
Title V: Measures in support of innovation
RELX broadly welcomes the approach taken to Governance in the proposed regulation. We have
some concerns that the lack of a consistency mechanism could lead to different interpretations of
the regulation across Member States. 
Articles 56 – 58 – The European Artificial Intelligence Board (the AI
Board)
We would welcome greater industry involvement in the the AI Board so that all relevant
stakeholders can collaborate and co-create the right framework. As currently drafted, industry is
only a by-stander and not part of its permanent structure. In particular we suggest space should be
made on the board for AI developers to ensure the Board has a suitable understanding of the
practical implications of their decisions. There is a concern that the AI Board’s proposed structure
and remit will lend itself towards a system in which an overly cautious approach will be taken which
seeks to prevent AI activities in order to avoid purported risks. Instead, we would encourage that
innovation should be built into the remit of the AI Board so that they also have a requirement to
consider how the EU can realise the benefits of AI, while appropriately mitigating risks through some
of the measures of this regulation. If this is not achieved the reality is likely to be that AI investment
moves away from the EU. 
Title VI: Governance 
Title VIII: Post market monitoring, information
sharing, market surveillance
Article 64 – Access to data and documentation 
We understand the need for surveillance authorities to be granted full access to training, validation
and testing datasets (article 64) as well as (upon a reasoned request) the source code of the AI
system but would suggest guidance should be developed together with industry. With regard to the
source code in particular, this disclosure should be in line with current legislation including the trade
secrets directive. 

Article 67 – Compliant AI systems which present a risk
We are highly concerned by this article which suggests that Member States will have broad powers
to force the withdrawal from market of an AI system even where the AI provider is in full
compliance with the regulation's requirements. We believe this would be disproportionate. 
The phrase ‘other aspects of public interest protection’ is of particular concern as it is open to a
wide degree of interpretation. It is not clear what situations the Commission had in mind when
drafting this provision, however it appears to be designed in such a way that even if an AI provider
meets the regulation’s requirements, they can still face punitive action. 
Conclusion
RELX has been glad to have the opportunity to provide initial feedback on the regulation and
hopes the contents of this paper has been useful in highlighting certain areas where some
improvements could be made to the regulation. AI stands to benefit European citizens
significantly and as noted in this paper, getting this proposal right could help achieve those
benefits. This is an exciting opportunity for the EU to secure Europe’s AI future, and to shape
the global debate on AI while making sure European citizens are able to access the best and
most innovative products and services available. 
We look forward to engaging with this proposal as it makes its way through the legislative
process and stand ready to assist the EU in its aim of ensuring human-centric AI is encouraged
by adopting a fully risk-based approach, which takes into account the context and role of AI
systems in decision-making. 
For further information please contact:
Elizabeth Crossick, head of EU government relations and global AI policy lead, 
Jeremy Lilley, UK government affairs and AI policy
Date: 5 August 2021