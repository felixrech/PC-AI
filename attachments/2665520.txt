Proposal for a Regulation laying
down harmonised rules on AI:
Com/2021/206
Equifax response to the request for feedback
6 August 2021

Areas where we agree with the proposals
We agree with three fundamental aspects of the Commission’s proposals:
Systems to evaluate creditworthiness and establish credit scores that use more advanced machine
learning can outperform traditional methods, with many benefits for borrowers, lenders and the
economy. These benefits include more access to credit for consumers and SMEs, reductions in the
cost of credit and a reduction in the rate of overindebtedness/non-performing loans. For example,
Equifax has successfully submitted an application to the Bank of Spain’s sandbox for a fully
explainable credit risk model that uses machine learning.
Using AI in such systems without high standards of model governance and explainability would
increase risks for borrowers, lenders and the economy. These risks include non-compliance with
GDPR, loss of control over loan origination models and an increase in the rate of
overindebtedness. It would also lead to some consumers who are denied credit getting inaccurate
explanations for lending decisions.
So the regulatory framework for loan origination should promote innovation and use of AI
systems, while maintaining existing high standards for model governance and explainability.
Our main concerns and recommendations
We have serious concerns that some aspects of the proposals will have a significant negative impact on the
price and availability of credit by greatly increasing the costs for the users and providers of all systems that
evaluate creditworthiness and by reducing the pace of innovation in the EU. In particular:
The definition of AI includes fully explainable techniques plus systems that are not highly complex
and that do not continuously adapt. These systems are low risk and in many cases, such as credit
scores, have been used safely for decades. Including them in the definition greatly expands the
scope and burden of the proposed regulations yet delivers little or no additional consumer
protection.
The evidence that horizontal AI regulation designating systems that evaluate creditworthiness as
high risk will improve access to financial services, electricity and housing is far too weak. It is much
more likely that the proposed regulation will have the opposite effect - increasing costs and
holding back innovation that would otherwise improve the accuracy of creditworthiness
assessments and access to responsible, affordable credit. Furthermore, these systems and are
already subject to separate regulation by banking and financial services regulators.
To ensure the Proposal is proportionate and risk-based, we strongly recommend the Commission better
target the definition of AI, and remove from the list of high risk systems those that evaluate the
creditworthiness of natural persons or establish their credit score.
The definition of AI should be more specific and better targeted so the Proposals achieve their
objective in a risk-based and proportionate way
The definition of AI, as well as the list of techniques covered in Annex I, is too vague and too wide.
The definition includes “statistical approaches”, which could encompass the most straightforward
techniques, such as calculating an average from a series of numbers. It also includes “expert systems”,
which could be the most basic decision tree based on business experience.
The Commission’s impact assessment (section 2.2) states that “the main problem driver” is “the specific
characteristics of AI systems which make them qualitatively different from previous technological
advancements”. These characteristics are then described in detail in Annex IV. However the proposed
definition of AI is so wide that it emcompasses techniques that do not have those characteristics and which
cannot be described as quantitatively different from previous advancements.
For example, logistic regression is covered by the proposed definition of AI. Logistic regression is an AI
technique but it is not new or a technique that is “quantitatively different from previous advancements” - it

has been very widely used in credit scoring since the 1950s. In the table below, we show that logistic
regression does not have the characteristics of what the impact assessment defines as a problematic AI
system that requires further regulation:
Characteristic of a problematic
AI system (taken from annex
IV of the impact assessment)
Characteristic of systems that
use logistic regression
Does a logistic regression
system have this
characteristic of a problematic
AI model?
Complexity: AI systems often
have many different
components and process very
large amounts of data. This
amount of parameters are not in
practice understandable for
humans, including for their
designers and developers.
The systems and their
parameters are intrinsically fully
interpretable and explainable, ex
ante.
No.
Transparency/opacity in: how
the AI system functions as a
whole (functional transparency);
how the algorithm was realized
in code (structural transparency)
and how the program actually
runs in a particular case,
including the hardware and
input data (run transparency).
A logistic regression system is
fully transparent in all three
aspects; it is not a “black box”.
No.
Continuous adaptation: the
process by which an AI system
can improve its own
performance by ‘learning’ from
experience. Unpredictability:
where the outcome of an AI
system cannot be fully
determined
Logistic regression systems are
fully completed when they enter
the market; they do not adapt or
self-learn unless they are
continuously retrained with new
data. Logistic regression systems
are also fully predictable
because they are stable and fully
transparent.
No.
Autonomous behaviour:
functional ability of a system to
perform a task with minimum or
no direct human control or
supervision
Logistic regression can be used
in autonomous models, such as
credit scoring. However, the
steps are programmed and can
be fully understood by humans.
Yes, partly.
Data: the dependence of AI
systems on data and their ‘ability’
to infer correlations from data
input can in certain situations
affect the values on which the EU
is founded, create real health
risk, disproportionately adverse
or discriminatory results,
reinforce systemic biases and
possibly even create new ones.
This characteristic could apply to
any method of taking decisions
that use data, even those taken
by a human with no algorithm
involved such as a jury reviewing
evidence in trial. The risk is
surely lower in a logistic
regression system that is far less
complex than some AI
techniques, is fully transparent
and does not continually adapt.
No, not to a significant or
growing extent.

The lack of clarity and the breadth of the definition of AI are a major concern because they greatly increase
the scope of the proposed regulation and the regulatory burden it will place on organisations that use low
risk systems.Organisations that use low risk statistical approaches, and have done so for decades, will fall
into the scope of the AI Act; disincentivizing innovation and increasing their costs even though the low risk
to citizens has not changed.
We strongly recommend that:
The definition be better targeted on high risk techniques so that the proposed AI Act
achieves the Commission’s goal of creating “a proportionate regulatory system centred on a
well-defined risk-based regulatory approach”.
Expert systems, statistical approaches, Bayesian estimation, search and optimization
methods should be removed from the definition.
The definition should exclude logistic regression and other techniques that are as
explainable and as predictable as logistic regression.
AI systems intended to be used to evaluate the creditworthiness of natural persons or
establish their credit score should be removed from the list of high risk AI systems
The evidence for including systems that evaluate creditworthiness and credit scores in the list of high risk
systems is too weak. It also fails to recognise that any risks are, and can be, mitigated more efficiently and
effectively through vertical, sector-specific regulation. Designating systems for evaluating creditworthiness
and credit scores as high risk will increase the price of credit for consumers and SMEs. It will also
disincentivise innovation that would otherwise reduce the rate of overindebtedness and improve access to
responsible credit.
Recitals 32 and 37 states that systems are high risk “if they pose a high risk of harm to the health and safety
or the fundamental rights of persons” and that “AI systems used to evaluate the credit score or
creditworthiness of natural persons should be classified as high-risk AI systems, since they determine those
persons’ access to financial resources or essential services such as housing, electricity, and
telecommunication services”. What evidence there is to support this is set out in Annex IV of the impact
assessment.
Focussing first on the use of AI systems to evaluate creditworthiness and establish credit scores in financial
services, we do not agree that they pose a sufficiently high risk, or sufficiently determine access to financial
resources, to justify inclusion in the list of high risk systems. Our reasons are:
Risks are already effectively mitigated by existing regulation. Financial and data protection
regulation at EU and Member State level mitigates risk, giving borrowers the right to seek
explanations, manual review and redress. This includes existing sector-specific regulation (notably
the Consumer Credit Directive and Mortgage Credit Directive, and the EBA guidelines on loan
origination and monitoring that specifically address models for creditworthiness assessments that
are technology-enabled). There is also horizontal legislation, in particular GDPR. Creditors are
regulated entities and Member States have the option to regulate credit reference agencies too
should they wish to. A 2021 survey of European credit providers found half were regulated at
Member State level and half operated in Member States that have not judged regulation of credit
reference agencies necessary or proportionate.
Based on the proposed definition of AI, it is wrong to say the use of AI systems to evaluate
creditworthiness is growing. The impact assessment (table 7, annex IV) cites “growing use by
credit bureaux and in the financial sector” as an essential reason to designate systems that
evaluate creditworthiness as high risk. Similarly, the EDPB and EDPS partly justify the need for an
AI Act on the basis that AI will lead to more predictions being made and algorithmic decision
making replacing human made decisions. This is not the case in creditworthiness assessments.
The number of predictions about creditworthiness that are done rises and falls with the number of
applications consumers make for credit. If algorithms have replaced human activity in those

assessments then that happened decades ago. Displacement of human activity is not happening
now, or in the future.
Any emerging risks that do arise from the use of AI in creditworthiness assessments and
credit scoring in financial services can be mitigated through new financial services
regulation. This includes new EBA guidelines, changes to financial legislation (such as the
measures in the the new proposed Consumer Credit Directive that prohibit some datasets from
being used in creditworthiness assessments) and through GDPR (enforcement, guidelines and
approved codes of conduct). New financial regulation can impose high standards for model
governance and explainability in systems that use AI techniques, which Equifax supported in our
response to a previous Commission consultation on the AI Act.
The evidence that inaccurate or biased creditworthiness systems or credit scores are a
significant barrier to accessing credit is very weak. The Commission’s own research on financial
exclusion found that “only in Romania, Bulgaria, Slovenia, Slovakia and Portugal do more than 1%
of people live in households for which an application for credit or a loan has been turned down”
and only 2% of people across the EU “think that there is no point in applying for a credit facility
because it would be refused”. Note, that is not to say creditworthiness assessments for those very
small populations are unfair or inaccurate. At no point does that report suggest inaccurate
creditworthiness models are a barrier to accessing credit – rather it finds that those who want to
access credit and cannot do so tend to be on too low an income to afford repayments. In other
words, creditworthiness assessments provide effective protection from overindebtedness.
The use case is incorrectly targeted. Even if the argument were accepted that horizontal
regulation is needed to protect consumers' access to financial services, it is wrong to focus on
systems that evaluate creditworthiness isolation since they do not determine people’s access to
financial services. A creditworthiness assessment is one factor in a lender’s decision to originate a
loan or not. Lenders also consider affordability assessments, ID verification, counter fraud checks
and the lender’s overall risk management and business model. A lender could use an AI system in
any step of that lending process; meaning a consumer may “pass” a creditworthiness assessment
but be denied credit by an AI system the lender uses to choose who to lend to for commercial
reasons or to flag potential frauds. If the Commission’s aim is to protect access to financial services
then it does not make sense to apply a higher bar to one AI system a lender may use in loan
origination decisions and not the others.
The evidence that AI systems for creditworthiness assessments and credit scoring significantly affect access
to “essential services such as housing, electricity, and telecommunication services” is even weaker than for
financial services.
There is no evidence in the impact assessment or the proposals that designating systems for evaluating
creditworthiness assessment or credit scores as high risk will improve access to housing. Nor are such
systems cited as a barrier to accessing housing in more robust reports for the Commission or by groups
fighting homelessness in the EU. If landlords are using credit scores in decisions about which tenants to
accept, it is clear this is insignificant in determining access to housing compared to the real systemic causes
of exclusion cited in the Commission’s rigorous study. The Commission concluded that the actual reasons
for housing exclusion have nothing to do with creditworthiness or credit scores, rather the causes are
problems such as:
A lack of affordable housing
Poverty, unemployment, the low level of welfare benefits, the lack of social protection, the
changing nature of work
Legal obstacles (e.g. lack of documents, regularisation procedures)
Individual and family related causes (e.g. divorce and/or separation, family violence, drug and
alcohol addiction problems, and health and mental health problems)

So if the Commission’s aim is to protect access to housing, designating credit scores and creditworthiness
assessments as high risk AI systems is a disproportionate and misguided response because it will yield no
significant benefit.
As in lending, credit scores and creditworthiness assessments are however widely used by some providers
of electricity and telecommunication services as part of their decision about whether to offer a contract to
consumers on a credit basis (rather than ‘pay as you go’). Again though, it is wrong to say that such credit
scores and creditworthiness assessments determine access to those services – they are at most just one
element in the provider’s decision about which clients to service and on what basis.
Furthermore, the evidence that those systems for evaluating creditworthiness or credit scores are a
significant problem that prevents people from accessing electricity and telecommunication services is very
weak. The EU has carried out detailed work on the digital divide and how to overcome it. There is no
evidence in that work that credit scores or creditworthiness assessments are a significant driver of
exclusion from telecommunications. Similarly, a report for the European Parliament identified the three
main causes of energy poverty as: energy prices, falling household incomes, and living in an energy
inefficient home. Those causes have nothing at all to do with systems for evaluating creditworthiness or
credit scores.
It is also important to note that providers of energy and telecommunications have used these systems for
evaluating creditworthiness for many years and the risks that arise from that activity have not changed. To
protect consumers in these markets, there is sector specific legislation and regulation at EU and Member
State level, as well as protection from horizontal legislation such as GDPR. Any residual or emerging risk can
be tackled through proportionate sector specific regulation and enforcement of GDPR. Designating AI
systems that evaluate creditworthiness as high risk will have no significant impact on access to electricity
and telecommunications. It will however impose a disproportionate burden on firms, discourage innovation
and likely increase costs for the people that use those services as well as all users and suppliers of credit
information.
We strongly recommend:
The Commission remove AI systems used to evaluate the credit score or creditworthiness of
natural persons from the list of high risk systems.
As the Commission will have to power to update the list of high risk systems over time, it can better address
its concerns by first investing in independent, credible research to better understand the actual extent to
which AI systems for creditworthiness assessments and credit scores impact on people’s access to finance,
housing, electricity and telecommunications respectively relative to other factors, and the extent to which
further regulation would be proportionate and beneficial.
Additional concerns and recommendations
Giving the Commission the power to expand the list of high risk AI systems through delegated acts
will create significant legal uncertainty in the market. The unpredictable evolution of the scope of the
regulation could discourage developers from developing and implementing innovative AI solutions in the
EU. The proposals should be changed to deliver a more stable and predictable regulatory framework.
Transparency requirements must more clearly protect intellectual property. For example, were
providers of AI systems forced to reveal too much detail on algorithms then users could copy those
products. That would greatly limit investment and innovation in AI in the EU. As regards the duty of
transparency, this should be done under the terms and conditions already regulated by the GPGR. Article
13.2.f of GDPR states that "the existence of automated decisions, including profiling, referred to in Article
22, paragraphs 1 and 4, and, at least in such cases, meaningful information about the logic applied, as well
as the significance and expected consequences of such processing for the data subject, must be reported".
This is a concise regulation, already in place in all Member States, and so transparency in AI must
necessarily be linked to what is already stipulated in GDPR.

Too many organisations will be responsible for enforcing and administering the AI regulations in the
area of creditworthiness/loan origination. It is not clear how they will work together to apply the
regulations in a harmonious way. Fragmentation, uncertainty and inconsistency in the application of the
regulations will be a barrier to the creation of a single market in credit and digital financial services. It would
also have a negative impact on consumer protection, the cost of compliance for businesses, and on
investment and innovation in AI.
For example, where AI systems are provided or used by regulated credit institutions, the proposals seem to
envisage that Member States’ data protection authorities will enforce the regulations alongside “the
authorities responsible for the supervision of the Union’s financial services legislation” at Member State and
EU-level. It is not clear how these divisions of responsibilities and interfaces will work in practice, how those
authorities would consider relevant EBA guidelines, or how consistency will be achieved for consumers,
providers and users across different Member States. Nor is it clear who would oversee a provider of AI
systems to regulated financial institutions where the provider itself is not a regulated financial institution.
For example, in Spain, credit reference agencies that provide models for creditworthiness assessments are
not regulated by a financial authority but the creditors that use those models are. We also note that the
competencies of data protection control authorities are defined in article 55 of the GDPR. Supervising AI
models would seem to exceed what is regulated in that article.
Some authorities responsible for enforcing the regulations will likely find it difficult to acquire the
capacity, experience and expertise they will need to fulfill their responsibilities effectively and
efficiently. For example, a data protection authority that became fully or partly responsible for enforcing
regulations relating to the accuracy and robustness of a credit score model would have little or no
experience and expertise in this area today. Furthermore, the broader the definition of AI and the more
products that are in scope, then the greater the regulators’ workloads will be. That will make it harder for
regulators to fulfill their duties and reach judgements quickly enough to protect consumers and avoid
becoming a bottleneck on innovation. The Act should be better targeted and its implementation delayed
until authorities are able to implement it efficiently and effectively.
About Equifax
Equifax is a global data, analytics, and technology company and a credit reference agency. Headquartered
in the USA, Equifax operates or has investments in 24 countries in Europe, North America, Central and
South America and the Asia Pacific region. It is a member of Standard & Poor’s 500 Index and employs
approximately 11,000 people worldwide. In Europe, Equifax operates in Spain, Portugal, Ireland and the UK.
For more information, please contact: Stuart Holland, Head of Government Relations - Europe,
stuart.holland@equifax.com