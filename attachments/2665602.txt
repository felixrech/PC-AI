Feedback to the European Commission’s regulation
proposal on the Artificial Intelligence Act
Berlin, August 6, 2021
General
Artificial Intelligence (AI) is a rising digital technology and displays all properties of a
general-purpose technology. Therefore, it enables all kinds of innovative applications leading
to high expectations for the future. However, concerns about risks are similarly strong but
often seem vague and speculative.
Without any doubt, Europe with its diverse and talented AI ecosystem can become a global
competitive player in AI. Yet, “AI made in Europe” serving its society and economy in vast
ways, needs a flexible, understandable, and future-oriented regulatory framework.
Overregulation and too many bureaucratic requirements create an administrative burden and
risk of hindering and slowing AI development and innovation. Fulfilling these requirements is
burdensome for smaller start-ups and SMEs in particular. It could result in a competitive
disadvantage to their global counterparts outside the EU e.g., from China and the US, but
also beyond.
Therefore, we welcome the opportunity to provide herewith our feedback and expert opinion
related to the European Commission’s (Commission) proposal for the regulation of AI in
Europe, dated April 2021:
1. Definition
We are aware that one of the main challenges is to horizontally regulate general purpose
technologies because of their wide footprint. The proposed AI Regulation demonstrates this
challenge: The AI definition (Article 3.1 and Annex 3) is not distinct enough. It classifies
almost any existing and future software as “AI" that would then be covered by the
regulatory framework. It remains open about which risks are to be tackled and misses an
analysis of why AI adds a layer of risks not present before, in particular in areas which are
already regulated.

2. Risk-based approach
We appreciate that the current draft does not directly regulate or ban every AI-system that is
theoretically critical without closer examination. In particular, we welcome the fact that the
Commission has opted for a risk-based approach. However, the definition lacks
comprehensibility, clear terminology, and delimitations.
The tendency to place an entire industry under general suspicion and the subsequent
approach, that improper usage of AI-system can lead to harm is too generalizing and does
not reflect reality. Therefore, we recommend a more detailed consideration of the specific use
case category and associated risk-relevant factors (see, for example, risk classes for medical
devices). We welcome critical considerations of AI applications but their generalized
classification as a high-risk category requires revaluation.
3. Risk-assessment
We question the affordability of self- and 3rd-party assessments, especially for start-ups
and SMEs. In the case of 3rd-party assessments, hiring external experts and specialised
audit companies mean costs that can only be paid by large enterprises. The measures
mentioned in the draft to avoid disadvantages for start-ups and SMEs are not
sufficient and need to be extended. For self-assessments, depending on the scope and
associated liability risk, the question arises as to the feasibility of establishing a quality
management system by SMEs and other companies without significant inspection and
testing infrastructure of their own. Further specification of the requirements for the test
processes and test infrastructure for risk-assessments would be desirable, including
answers to the following questions:
Who bears the liability risk for incidents in the testing process?
How are systematic biases resulting out of systematic testing prevented?
How are transparency and access to the review process guaranteed, especially for
start-ups and SMEs?
What are the requirements for the qualification of the reviewers?

As already touched upon in Point 2, the risk assessment of critical applications should
consider several criteria that need to be cumulatively examined and fulfilled.
There is no (comprehensible) balancing of risk and benefit, in the sense of a
risk-benefit ratio, so that a disproportionate risk assessment can quickly occur. What
is the risk assessment when the AI application itself is high risk, but it significantly reduces
the risk of existing approaches, as in autonomous driving vs. autonomous vehicle? Article 7,
paragraph 2 mentions categories such as severity and reversibility, but further factors of
established risk research, such as probability and controllability of the risk and expected
benefit in relation to it are missing. This should be considered when revising the draft.
We also advocate to create a harmonised sanctioning infrastructure across Europe.
There must not be any differences in the way the same facts are handled between the
individual member states. The regulation proposal is not clear enough about how a
harmonised approach to audit processes can be established and ensured within the
European framework.
4. Guidelines
In many places, the operationalization of definitions, assessment criteria and requirements in
the draft remain unclear. This unclear operationalization creates considerable uncertainties
on the business side and especially among start-ups and SMEs. We therefore call for
the involvement of practical users and relevant institutions in the development and design of
national operationalizations of the draft. We strongly suggest that the current proposal is
complemented with application- and industry-specific guidelines. These should be
less general than the regulation itself, but refer to concrete sectors, industries and use
cases - and are ideally formulated jointly with AI developers.
5. Existing regulation
We do not see any need for additional regulation concerning product liability for AI based
products. As existing product liability regulations are well established there is no need to
introduce further technology specific regulations. Most of today’s software products are
continuously improving with updates and the same shall apply to AI based products. We

strongly support other industry associations' position on that as to the point that we see AI
based products bring huge benefits - also in terms of safety - to people, we therefore
strongly position ourselves against additional market barriers and technology specific
extra regulations. The current law sufficiently covers risks that could be justified by AI
systems.
Furthermore, also with respect to the existing admission procedure, we strongly recommend
taking into consideration the use of quantitative measures. The need to understand - as to
follow up decision making processes in a simplified way - is to be replaced by
systematically measuring the performance of an algorithm or product. Otherwise,
today’s and also future AI developments may not be able to be deployed in society because
the admission procedures were created with focus on technologies present at that point in
time.
6. Data
We welcome the aim of the Commission to create better data sets. Limited availability of
high-quality data can be one of the biggest hurdles in developing AI that accurately
represents a population. However, the question arises for AI software manufacturers on
how to ensure the unrealistic goal of Article 10.3. The most common source of bias is
data that does not sufficiently represent the target population. For example, women and
people of color are typically underrepresented in clinical trials. Many diseases present
differently in women and men, whether it is cardiovascular disease, diabetes, or mental
health disorders such as depression and autism having adverse implications for these
underrepresented groups. However, even a representative sample can be misleading.
What if a sample exactly represents a population distribution considering age, sex, and
ethnicity of people living in Germany? The quotes would be representative, but the
distribution would be skewed because there are more Germans living in Germany than
people from other ethnic backgrounds.
Instead of sanctioning AI developers for the use of historical data, the Commission
should promote the development of fair, robust, well-annotated and curated datasets
across institutions. They should be aggregated in a way that protects patient privacy and
captures diversity between and within demographic groups. This should be clearly more
promoted even at an EU level to push innovation.

7. Bias
We raise concerns in the definition and differentiation between bias and ethics. Given
the line of argumentation in general, one could assume that bias is interchangeably used as
a word to describe societal problems. In fact, a bias can exist in society and manifest in data
but not every bias is societal. For instance, an autonomous car could be biased towards
dashed lines (vs. dotted) when changing lines. In fact, this is highly misleading given the
alarm system even should be biased to prevent accidents by treating both lines the same
way. Because bias lacks a clear definition, it tends to include all types of AI
applications. For example, even an AI that is intended to realize lower-wear operation of an
engine would fall under the EU regulation. Consequently, manufacturers will think twice
before making use of AI techniques. This can have a negative impact on innovation.
If we are to understand bias in the form of discrimination, it must be considered that this is
already regulated in laws such as the German AGG (Allgemeines Gleichstellungsgesetz)
independent of the technology employed. It is already clearly defined that consequently no
one should be disfavoured according to the standards set in the AGG. Moreover, other rights
such as the right to erasure are regulated under the GDPR requirements. Given those
regulations, one could orient on existing legal frameworks. The Commission needs to
demonstrate that through AI these existing frameworks are not sufficient, which it has not
done so far.
We instead understand bias as a method to unveil judgement or decision patterns per
se without any connotation or evaluation about whether they are right or wrong. In
fact, reporting a bias and thus decision patterns is recommended. If we follow this approach,
it would be easier to have guidelines to compare the output of AI models against clearly
defined parameters. Companies could use datasets (e.g., generated from universities) and
gauge their output against them.
Bias, fairness, and ethical questions should be regarded on a case-by-case basis.

8. Remote biometrical identification
We lack the clear prohibition of long-term and large-scale ‘real-time’ remote
biometrical identification in public spaces. The current proposal could be interpreted to
allow mass biometric surveillance. Given the underlying aspiration of the regulation, this is
not acceptable from our point of view. We urge the Commission to set clear temporal,
geographic and personal limitations regarding the use of 'real-time’ remote biometrical
identification.
9. Responsibilities
Based on the experience of newly introduced regulations, like GDPR, we want to further
stress that clear responsibilities should be defined as early as possible. This includes
but is not limited to the question of who will sit on the planned AI panel, who will clarify
existing legal grey areas and cross-jurisdictional harmonization issues, who will be
responsible for granting exceptional permission (e.g., for remote biometric identification
systems), and who will define currently vague terms (such as the "proportionality" of social
scoring systems or representativeness, accuracy, and completeness of data).
10. Recitals
We believe that the current recitals should be rewritten in a less generalized way but
according to the approach of the overall regulation and ideally jointly with AI developers.
One proposal for a reformulation, while it is not limited to that, is Recital 70 ("Moreover,
natural persons should be notified when they are exposed to an emotion recognition system
or a biometric categorisation system."): Apart from the fact that this demand is not actionable,
the underlying risk assessment here is based on generalized assumptions. Suggestion is
that natural persons should only be informed of the use of such systems if harm or
risk to covered persons results from their use. However, members of our organisation
use biometric categorization of people to create synthetic counterparts that are used for
anonymization purposes that simultaneously enable AI-development. As such, the
anonymized data needs to be as close as possible to the original to represent reality in a
qualified way, a person of colour just cannot be given the synthetic face of an artificially
generated white person.

Conclusion
Based on beforehand mentioned points, we strongly recommend avoiding indefinite legal
terms to the utmost extent possible. In addition, we encourage the Commission to precise
any definition used in the regulation as much as possible, for instance, through better drafted
recitals This entails explicitly the definition of AI itself, the often-misunderstood term bias and
the delimitation of the different risk categories. By using best case examples, the regulation
can give a better understanding of their intentions. Moreover, we call for further specification
in the risk-assessment and a more realistic aim for data sets. We could not identify any need
for technology specific liability regulation. For the industry, and especially start-ups and
SMEs, it would be crucial to have clear responsibilities and guidelines.
To allow innovation made in Europe it must be a priority that any approval of a final EU AI
regulation must be followed by timely implementation. Not just, but in particular, to enable
legal certainty for AI start-ups, SMEs and all developers of cutting-edge technologies and
stakeholders involved.
The Taskforce AI Regulation of the KI Bundesverband:
Nicole Formica-Schiller, CEO & Founder Pamanicor Health AG
nicole.formica-schiller@ki-verband.de
Prof. Dr. Patrick Glauner, Professor for AI, Technische Hochschule Deggendorf
patrick.glauner@ki-verband.de
Detlef Eckert, Founder Deep Digital Consulting B.V.
detlef.eckert@deepdigital.eu
Daniel Abbou, Managing Director KI Bundesverband
daniel.abbou@ki-verband.de
Alex Dickmann, Assistant to the Managing Director KI Bundesverband
alex.dickmann@ki-verband.de

About the German AI Association
The German AI Association (KI Bundesverband e.V.) is Germany’s biggest industry
association for Artificial Intelligence (AI) and represents more than 300 innovative SMEs,
start-ups and entrepreneurs that focus on the development and application of AI. We support
AI entrepreneurs by representing their interests in politics, business, and the media.
Our goal is an active, successful, and sustainable AI ecosystem in Germany and Europe.
Because only if the brightest minds and forward thinkers decide to found, research, and
teach in the European Union, we can stand up to global competition.
Our members are committed to ensuring that AI-technology is applied in accordance with
European and democratic values and that Europe achieves digital sovereignty. To achieve
this, the European Union must become an attractive business location for entrepreneurs,
where their willingness to take risks is appreciated and innovative spirit meets the best
conditions.
Contact us via
+49 (0)157 70 41 50 46
info@ki-verband.de
For further information:
www.ki-verband.de
KI Bundesverband e.V.
Im Haus der Bundespressekonferenz
Schiffbauerdamm 40
10117 Berlin