language,id,country,user_type,organization,surname,feedback,status,company_size,first_name,reference_initiative,date_feedback,publication,publication_id,publication_status,tr_number,scope,governance_level
en,2665651,BEL,ngo,Equinet,,"Equinet welcomes the opportunity to provide comments on the European Commission’s Proposal for a Regulation on Artificial Intelligence (AI) systems. Equinet further wishes to reiterate its support for this legislative initiative as a timely and valuable opportunity to ensure that the EU becomes the global leader in regulating for AI-enabled technologies that protect and advance fundamental rights, societal wellbeing and the environment.  

Equality is explicitly and prominently addressed as one of the leading fundamental rights concerns related to the impact of AI systems, which the proposed regulation is going to address. Yet, the current text of the Proposal for an AI Regulation provides insufficient guarantees—especially, in the parts on monitoring, compliance and enforcement — that AI-induced harm on the fundamental right to non-discrimination can be effectively identified, prevented or remedied.  

Please find as an attachment the following key recomemndations for the Proposal by Equinet. 
",PUBLISHED,micro,,COM(2021)206,2021-08-06 23:57:37,anonymous,24212003,closed,,,
en,2665650,AUT,ngo,AI Austria,Gorzala,"AI Austria welcomes the opportunity to comment on the proposed Artificial Intelligence Act. We commend the work of the European Commission in developing a framework for artificial intelligence. In certain areas, we believe there are topics to be clarified and considered, which we seek to bring to your attention. We hope our feedback - submitted as a PDF - contributes to the further refinement of the proposed provisions and the creation of an AI framework of trust. ",PUBLISHED,small,Jeannette,COM(2021)206,2021-08-06 23:55:26,withinfo,24212003,closed,,,
en,2665649,DEU,ngo,Digitalcourage e.V.,,"This submission to the AIA consultation is sent on behalf of Digitalcourage e.V, a German NGO that advocates for fundamental rights, privacy and protecting personal data. Digitalcourage is composed of people from a variety of backgrounds who explore technology and politics with a critical mindset, and who want to shape both with a focus on human dignity.

Digitalcourage strongly believes that this is an important regulation in respect to the protection of fundamental rights in the European Union. The upcoming socio-technical developments that are summarised under the term „Artificial Intelligence“ are likely to  weigh heavily on issues of fundamental human rights, e.g. through the use in law-enforcement, migration control or generalised mass surveillance, unless proper safeguards are implemented. 
We increasingly receive individual reports of people that tell us about the negative impact of AI-powered (biometric) surveillance on them. For example this includes workers, that already during their application procedures get assessed on intransparent and potentially discriminatory datasets or whose performance in the assessment procedures is surveilled by supposedly smart systems.
Similarily, we’ve seen a worring increase in the use of such systems in the field of education. Digitalcourage awarded the 2021 German Big Brother Awards to a proctoring company (see here for the elaborated argument: https://bigbrotherawards.de/en/2021/education-proctorio), that implemented permanent video surveillance of students and that implemented algorithmic control mechanisms, which revoked the general presumption of innocence on exam candidates. Such proctoring systems evaluate biometric criteria, such as gestures and eye movements, through automatic behaviour analysis performed by an AI software, that eludes any control by the affected students or their representatives. 
Unless a comprehensive regulation precludes the public and private use of AI against citizens, we are likely to see similar violations of human rights and democratic control to catch on. The power differential inherent in the relations between developers, users and subjects of these technologies requires some form of societal control.

Digitalcourage shares the analysis and endorses the recommendations provided by our partner organisation, European Digital Rights (EDRi). We attach it to our submission and endorse it for review in the consultation.",PUBLISHED,small,,COM(2021)206,2021-08-06 23:53:39,anonymous,24212003,closed,,,
en,2665648,USA,academic_research_instittution,UC Berkeley Center for Human-Compatible AI,,"The EU AI Act is an important step in the right direction toward developing beneficial AI. We particularly welcome the following elements:

- The broad definition of high-risk systems as those potentially impacting health, safety, and fundamental rights, with flexibility to expand
- The recognition and prohibition of “powerful tools for manipulative, exploitative and social control practices”
- The establishment of a permanent Board for oversight and gradual accumulation of expertise
- The prohibition on undeclared “bots” and false-content generation
- The inclusion of post-market monitoring
- The encouragement of codes of conduct along the same lines
- The requirement that a system has “in-built operational constraints that cannot be overridden by the system itself and is responsive to the human operator”
- The inclusion of potentially substantial penalties

Recommendations
We have identified three priorities, detailed in the following pages: 



1. Make the regulation future-proof and prepare for higher-risk systems

- Update the regulation to address increasingly generalized AI systems that have multiple purposes, such as OpenAI’s Generative Pre-trained Transformer 3 and DeepMind’s AlphaFold system
- Article 3(13) “reasonably foreseeable misuse” and “interaction with other systems”: Explicitly consider the issue of high-risk and societal-scale consequences stemming from the interaction of many low-risk systems
- Article 6 classification rules for high-risk systems: Include recommender systems in the classification rules for high-risk systems
- Article 6 classification rules for high-risk systems: Include the requirements to document perceptual inputs, action outputs, objectives, and the operational environment for high-risk systems
- Article 6 classification rules for high-risk systems: Include the requirements to document time-of-sale properties of systems
- List of high-risk categories in Annex III: Add categories


2. Protect people from psychological harm

- Article 5 (1) (a) on psychological manipulation: Consider expanding the current definition of “subliminal techniques beyond a person’s consciousness”
- Section 3.5 of the Explanatory Memorandum: Include the protection of mental integrity in the Explanatory Memorandum
- Article 53 (AI regulatory sandboxes): Recognize the limit of sandboxes


3. Clarify key components of the regulation

- Article 3 (1) Definition of AI: Delete the mention of “a given set of human-defined objectives”
- Article 3 (16): Modify the notion of “recalling” an AI system
- Article 3 (33, 34, 35): Redefine the notion of “biometric data” more generally
- Article 10 on data governance: Clarify training, validation, and test stipulation
- Article 12: Clarify the primary purposes of logging
- Article 13 on transparency: Transparency requirements should require internal processing to be made available and understandable
- Article 15.3: Clarify the issue of performativity (“feedback loops”)
- Article 64: Clarify the type of access granted to the datasets used by the provider
- Article 5.1(c) on social scoring: Clarify the application of social scoring and reinforce protection from manipulative, exploitative, and social control practices
",PUBLISHED,small,,COM(2021)206,2021-08-06 23:53:31,anonymous,24212003,closed,,,
en,2665647,USA,company,CrowdStrike,,"In response to the European Commission’s request for public consultation on Artificial Intelligence, CrowdStrike offers the following views located in the attached pdf. Thank you. ",PUBLISHED,large,,COM(2021)206,2021-08-06 23:50:16,anonymous,24212003,closed,,,
en,2665646,GBR,ngo,Fair Trials,Min,"Fair Trials welcomes the fact that the EU is taking a much-needed legislative approach to regulate and limit the use of artificial intelligence (AI), and that it recognises that the use of AI in law enforcement and criminal justice can have serious implications for fundamental rights. 

AI systems are used by European law enforcement and criminal justice authorities to predict and profile people’s actions and assess risk, leading to surveillance, stop and search, questioning, arrest and detention, and sentencing and probation, as well as other non-criminal justice punishments such as the denial of welfare, housing, education or other essential services. In doing so, these systems engage and infringe fundamental rights, including the right to a fair trial, privacy, and data protection rights, as well as reproducing and reinforcing discrimination on grounds including but not limited to race, socio-economic status and nationality. As more and more countries are turning to AI in law enforcement and criminal justice, it is more crucial than ever that the EU takes this opportunity to become a leading standard-setter in this area, ensuring the protection of fundamental rights.

However, while recognising the risks to some extent, the AI Act does not go nearly far enough to prevent certain fundamentally harmful uses, particularly in relation to law enforcement and criminal justice, which will have damaging consequences across Europe for generations. 
In order to do this meaningfully, the Act must prohibit AI used by law enforcement, and judicial and criminal justice authorities used to predict, profile or assess people’s risk or likelihood of ‘criminal’ behaviour, generate reasonable suspicion, and justify law enforcement or criminal justice action, such as surveillance, stop and search, arrest, detention, pre-trial detention, sentencing and probation. No amount of safeguards, short of a full statutory prohibition, will protect against these fundamental harms effectively. 

In the absence of a full prohibition, and to prevent additional harms, uphold the rule of law and safeguard justice systems, there are several bare minimum safeguards and requirements that can be adopted to lessen the fundamental rights impact of these AI systems. In particular:

a)	The AI Act contains vague and non-specific ‘bias’ requirements, none of which will prevent discrimination and bias. AI systems used in law enforcement and criminal justice contexts must be subject to mandatory, independent bias testing, but the feasibility of such testing depends on the availability of criminal justice data that is severely lacking in the EU;
b)	The AI Act must require more openness, transparency, and explainability of AI systems and their use, the decisions that are made, and significantly, must focus not just on ensuring transparency to the users of the systems, but also to those individuals impacted by AI or AI-assisted decisions;
c)	Given that AI can have a significant impact on individuals when it is used in law enforcement and criminal justice, it is crucial that there are effective avenues for individuals to challenge not just the AI decisions, but also the system itself. However, the AI Act does not facilitate or provide clear routes for challenge or redress for individuals attempting to contest or challenge AI systems, or their decisions; and
d)	The AI Act includes several exemptions for uses of AI from safeguards. This means that there is a lack of protection against a technology that can engage and infringe fundamental rights, including the right to a fair trial, privacy and data protection rights, as well as result in discrimination based on race, socio-economic status or class and nationality. 
",PUBLISHED,small,Bruno,COM(2021)206,2021-08-06 23:49:16,withinfo,24212003,closed,,,
en,2665645,USA,business_association,American Property Casualty Insurance Association,,Please find attached comments from the American Property Casualty Insurance Association.,PUBLISHED,medium,,COM(2021)206,2021-08-06 23:37:22,anonymous,24212003,closed,,,
en,2665643,BEL,ngo,Confederation of Laboratories for AI Research in Europe (CLAIRE),The Hague,"CLAIRE welcomes the opportunity to provide feedback and supports the European Commission's drive towards a balance between regulation and innovation, where citizens’ rights are well protected, while facilitating investment and innovation. However, we find that the sum of the initiatives, as they are currently planned, does not achieve this balance. 

We see five major issues:

- The proposed regulation shows a problematic focus on a particular, flawed definition of AI technologies. We believe that regulation of technology is important, but that the focus should be on the uses and applications of technology, rather than on a specific set of technologies that are being used. AI technology evolves, and even experts disagree which technologies fall within the scope of AI. Regulation aimed at ensuring the responsible use of AI should therefore be as technology-neutral as possible.

- The proposed regulation is too broad and vague to effectively boost the impactful development and use of ""AI made in Europe"", and to strengthen, rather than weaken, the global competitiveness of the EU economy.

- The proposed regulation does not achieve a reasonable balance between cost, investments and effects for companies, governments and society. It places a regulatory burden on the European economy that is particularly challenging for SMEs, startups and micro-enterprises, while other elements of the coordinate plans do not provide effective instruments to offset this burden.

- The proposed regulation is vague on implications to citizens' rights and provides exceptions in government uses of AI that are likely to negatively impact citizens.

- The coordinated plan is lacking key mechanisms for ensuring global relevance and leadership of ""AI made in Europe"". It encompasses a vast array of instruments and mechanisms, each of which is rather modestly funded in relation to the ambitious goals of the plan. The plan lacks coordination between these mechanisms. It also lacks large-scale signature initiatives that can address key challenges within the fragmented European AI ecosystem.

Overall, CLAIRE believes the intention underlying the proposed regulation is good, but that it does not achieve the intended upsides, while creating downsides of serious concern. Likewise, the coordinated plan, of which the proposed regulation forms an integral part, is pursuing worthwhile and achievable goals, but the level of funding and the array of mechanisms currently foreseen is insufficient for realising the ambition of European leadership in key areas and applications of AI and carries major risks of Europe falling further behind the fast-moving global leaders.

The official feedback from CLAIRE to the European Commission first comments on key aspects of the proposed regulation (Section 2), then discusses the revised version of the coordinated action plan (Section 3), and finally provides some high-level conclusions and a brief outlook on future engagement with the European AI strategy (Section 4). 

CLAIRE deliberately addresses the proposed regulation as well as the coordinated plan, since the former is a key element of the latter (see Chapter 9 of the coordinated plan), and regulation related to AI interacts with other elements of the European AI strategy as outlined in the coordinated plan.",PUBLISHED,large,CLAIRE HQ,COM(2021)206,2021-08-06 23:22:48,withinfo,24212003,closed,205170133342-09,,
en,2665642,NLD,academic_research_instittution,University of Cambridge,janssen,"Dear European Commission,

Please note that this contribution is a personal contribution, and that it does not necessarily reflect the opinion of the institutions I'm employed by.

Thank you for the opportunity to respond to the proposed AI Act. 

I'd like to focus on the risk impact approach taken in the proposed Act. Instead of assigning sectors or specified technical mechanisms as high risk (as currently proposed in the AI Act), it might perhaps be useful to develop a fit-for-purpose *spectrum of risks* occurring from AI systems, that takes fundamental rights, the methods of data processing, the balance between legitimate organisational interests and an individuals's (or group's) fundamental rights protection into consideration in specific use cases. This might better help avoid under- or over inclusiveness of risks and impacts to fundamental rights, and of organisational costs. The approach seeks to apply like and to align with GDPR's DPIA (but it could also be applied independently).  

The Commission has in recent policy reports more than once stipulated that AI systems that AI systems should be designed in a *human-centric* way, and that *innovation and fundamental rights protection are two sides of the same coin*: there is no trustworthy and prospering innovation without fundamental rights protection, and vice versa. Please find enclosed an article (2020) discussing an alternative risk assessment approach that underpins that human centric approach, while respecting innovation ambitions. It seeks to offer organisations approaches for practical tools to identify themselves whether their AI system carries a risk to the rights and freedoms of natural persons, in alignment with obligations arising under Art. 35 GDPR. Art. 29 (6) of the proposed AI Act also obliges organisations to perform a DPIA as per the GDPR, whenever they identify high risk to rights and freedoms.

Dr. H.L. Janssen

University of Cambridge, Compliant & Accountable Systems Research Group
University of Amsterdam, Blockchain Society & Policy Research Group",PUBLISHED,micro,heleen,COM(2021)206,2021-08-06 22:47:41,withinfo,24212003,closed,,,
en,2665641,USA,company,U.S. Chamber of Commerce,O'Dea,The U.S. Chamber of Commerce welcomes the opportunity to comment on the European Commission’s Artificial Intelligence Act (“Act” or “AI Act). Please find our comments in the attached document. ,PUBLISHED,large,Ciara,COM(2021)206,2021-08-06 22:34:42,withinfo,24212003,closed,483024821178-51 ,,
en,2665640,DEU,other,Dr. Bublitz & Prof. Douglas,Bublitz and Douglas,"This feedback on the proposed EU Artificial Intelligence Act draws attention to shortcomings with respect to one specific issue: manipulative influences via AI systems on the thought and behavior of persons. It is jointly authored by Dr. Christoph Bublitz, a scholar on human rights and a co-PI in the research project Hybrid Mind: On the Blending of Artificial and Organic intelligence at the University of Hamburg; and Prof. Thomas Douglas, PI on the ERC project Protecting Minds: The Right to Mental Integrity and the Ethics of Arational Influence at the University of Oxford. 

AI systems can powerfully influence or weaken our control over our thoughts and behaviour, by dynamically adapting stimuli to our behaviour and inferred mental state. These influences may bypass or weaken our capacity to control our own thoughts and actions. When they do, they are manipulative. They may also threaten fundamental rights, including the rights to freedom of thought, freedom of opinion, and mental integrity, as guaranteed by the European Convention on Human Rights and the Charter of Fundamental Rights and Freedoms of the European Union. 

The proposed Regulation of Artificial Intelligence does not address central cases of manipulative influence via AI systems; it covers only the peripheral cases of subliminal interventions and interventions that exploit specific, vulnerable groups. Nor is the problem of manipulative influence via AI systems adequately addressed via other instruments, such as GDPR and consumer rights directives. 

The creation of trustworthy AI – a key objective of the Regulation – cannot succeed without a robust stance against manipulation. We therefore call upon the Commission to address and regulate  manipulative interferences more directly and comprehensively. In particular, we suggest classifying as high-risk AI systems that significantly influence thought or behaviour in ways that bypass rational control or significantly weaken rational control.

",PUBLISHED,micro,Dr.,COM(2021)206,2021-08-06 22:07:32,withinfo,24212003,closed,,,
en,2665639,DEU,academic_research_instittution,Institute for Social Science Research e.V. (ISF Munich),Huchler,"The Institute for Social Science Research e.V. (ISF Munich) (Germany) welcomes the EU proposal on the regulation of AI. From our own research we would like to contribute to the consultation:

On risk assessment (see 5.2.2) as well as classification of the risk of AI systems (Art.7):
The limits of operationalizability of risks must also be considered; in particular regarding latent non-reflective harm to individuals and society.
In addition to potential harm and dependence on the AI system, freedom of action during use (e.g. opt-out/configuration options and decision options for users) and for use (e.g., market structure/plurality of service) are also relevant.

Other suggested indicia of the level of risk:
- Extent of potential (de-)qualification/expropriation of relevant knowledge and skills: The ease of use must be weighed against the impact on individual and societal maturity, agency, and diversity (of opinions, scope of thinking, opportunities to develop interests etc.). One big risk of AI (especially machine learning) is that behavior, thinking, emotions are channeled (e.g. by reducing them to a manageable number of classifications) and based on probabilities and data with past reference. The result is social path dependencies - which have a latent effect on individuals and society. (discrimination is only one aspect of this phenomenon, as are filter bubbles, monopolization)
- Extent of clarity/(in)transparency in the regulation of agency. If AI systems act unseen, it must be clear that they are currently (in)active and with which task they are engaged. This is also important for attributing responsibilities. Especially for interactive AI, it must be clear whether AI is currently (co-)acting or not. The more unclear the question of activity is the more high risk the system is. This is also relevant when human supervision is required for AI systems with high risk. (Art. 14)

Emotion-sensitive AI:
- The risk of using emotion-sensitive AI needs to be considered (beyond law enforcement) in terms of latent effects on social self-conditioning of emotion expressions. Even in voluntary/private use contexts (emo trainers, control in smart homes, etc.), these systems can potentially lead to psychological and social harm through the instrumental use of emotion expressions and the reductive classification to a small number of functionalities. Too little evidence is available in this regard.
- If emotion-sensitive systems are systematically used as a substitute for human social relationships, considerable individual as well as social damage can occur. This concerns not only the decrease of social relationships, the standard of living, and value of life but also the danger of indirect emotional manipulation - be it only in the direction of a (e.g. consumption-oriented) conformism.

Quality of data:
Not only statistical quality criteria apply, but an evaluation of the content is also needed (by experts).

Trustworthiness:
Trustworthy AI systems need to be free of contradictions and shall not confront users with contradictions oder externalize their solution.

Accompanying measures:
Due to the latency of socially particularly relevant risks, the limited operationalizability of a holistic understanding of risk, and the fact that effects of technical systems are often determined by their implementation and use, flanking measures should be taken to the ex-ante risk assessment:
- An observatory of latent social and societal effects on individuals and society to determine linkages and measures of harm
- A impact assessment and employee participation (in companies)
- Low-threshold, timely and effective complaint mechanisms (in companies, in society)

Ultimately, a new category of damage (possibly psycho-social damage + societal damage) may be needed that is sensitive to medium- and long-term individual and societal damage and the reduction of the diversity of thought/language, interest and scope of action.
",PUBLISHED,small,Norbert,COM(2021)206,2021-08-06 21:53:51,withinfo,24212003,closed,,,
en,2665638,BEL,company,World Employment Confederation - Europe,de Boer,"Main points:
-	WEC-Europe welcomes the creation of a regulatory framework of AI that will improve predictability and a level playing field for the application of AI. 
-	Following its Code of Conduct, WEC-Europe is dedicated to improving labour market inclusiveness and fighting (un)conscious human bias from the recruitment process, irrespective of the software used in its services. 
-	WEC-Europe emphasizes that AI is a tool that can be programmed to identify these (un)conscious human biases and minimize them in recruitment procedures. As such, it welcomes the opportunities in the proposal to do so. It highlights that in contrast to this ambition, ‘users’ are unable to do so in the proposal. This prevents users' ability to countercheck providers and/or enhance de-biasing methodologies. Moreover, this shapes market dependencies that will not drive innovation and diversity.
-	WEC-Europe warns that the current definitions in the proposal of high-risk AI as well as recruitment is so broad it covers all software used in recruitment (and employment), including those that do not involve any automated decision-making or machine-learning that impact the risk the proposal seeks to mitigate. As such, the proposal’s definitions diminish the overall high-risk approach. 
-	WEC-Europe brings forward that this broad definition and the administrative requirements of the proposal will benefit large providers that are able to meet these. Thereby raising issues of AI market concentration, competition, innovation, and adoption.
-	WEC-Europe welcomes the decentral oversight mechanisms but emphasizes the need for clarity for business on the competent authority.
",PUBLISHED,micro,Jochem,COM(2021)206,2021-08-06 21:48:45,withinfo,24212003,closed,7438623236-16,,
en,2665637,GBR,company,Getty Images (UK) Ltd,Reinitz,Please see the attached recommendations which mainly focus on Article 52(3).,PUBLISHED,large,Paul,COM(2021)206,2021-08-06 21:48:22,withinfo,24212003,closed,428103943814-51,,
en,2665634,BEL,ngo,Amnesty International,westby,"The attached position paper sets out Amnesty International’s concerns around the key gaps and shortcomings in the EU’s proposal for an Artificial Intelligence Act. As it stands the proposed regulation falls far short of the measures that will be required to meaningfully protect people from harmful AI systems in the EU and globally. Amnesty International puts forward several recommendations for strengthening human rights protections in the final regulation, including: 
• an outright ban on uses of facial recognition and remote biometric recognition technologies that enable mass surveillance and discriminatory targeted surveillance. 
• third-party verification for all high-risk AI systems. 
• mandatory human rights due diligence for all AI systems. 
• measures to strengthen transparency and oversight of AI systems. 
• a complaints and redress mechanism for individuals that have suffered human rights harm linked to AI systems.",PUBLISHED,medium,joe,COM(2021)206,2021-08-06 21:38:04,withinfo,24212003,closed,11063928073-34,,
en,2665633,BEL,business_association,DIGITALEUROPE,Chasserieau,"Please find attached our detailed feedback, and a summary below.

DIGITALEUROPE welcomes the European Commission’s legislative proposal for a Regulation laying down harmonised rules on artificial intelligence (‘AI Act’). We have been a key partner to EU institutions on AI topics for years, having notably participated in the work of the Commission’s AI High-Level Expert Group. We are committed to keep supporting EU policymakers and to actively participate in the policy and technical discussions surrounding AI.

In the attached document, we present our initial assessment of the proposed AI Act, which will be complemented by an in-depth analysis with concrete recommendations at a later stage.

Find below an overview of our initial findings:
- The overall objectives and focus on high-risk cases is appropriate.
- The scope needs to be further refined to ensure legal certainty.
- Some of the requirements will be difficult to implement, especially if no harmonised standards are available.
- The allocation of responsibilities between providers and users should be reassessed to best reflect the complexity of AI systems and their value chain.
- Applying the EU product safety approach (New Legislative Framework) to AI will be challenging for most companies, particularly smaller software providers.
- Effective safeguards and coordination measures would help mitigate fragmentation, regulatory divergence and differing implementation by national authorities.
- High compliance costs and paperwork are expected, which would negatively impact businesses, particularly SMEs and start-ups.

DIGITALEUROPE looks forward to working with the European Parliament, the Council and the Commission to discuss and assess how to best improve and implement the proposed AI Act, so that it achieves its ambition of stimulating the development and uptake of trustworthy AI, in line with European values.",PUBLISHED,small,Julien,COM(2021)206,2021-08-06 21:19:10,withinfo,24212003,closed,64270747023-20,,
en,2665632,BEL,ngo,BETTER FINANCE ,Carlucci,Please see document attached,PUBLISHED,micro,Edoardo,COM(2021)206,2021-08-06 21:17:31,withinfo,24212003,closed,24633926420-79,,
en,2665629,NLD,ngo,ALLAI,Muller,See attached file,PUBLISHED,micro,Catelijne,COM(2021)206,2021-08-06 21:07:37,withinfo,24212003,closed,681704440014-85,,
en,2665628,BEL,other,European Federation of Psychologists’ Associations (EFPA),Evans,"EFPA, The European Federation of Psychologists (established 1981) has the mission to develop and apply psychology for a positive impact on European society and beyond. EFPA publications are regularly consulted to inform EU policy and process. Now consisting of 38 European country associations, EFPA represents almost half of the world’s Psychologists whose members are required to observe professional standards. Specifically, EFPA Board of Assessment, whose members have led on this response, convenes regularly to encourage and advance best practices in testing and assessment. 

EFPA thanks the EU Commission for their work thus far and agree the need for rules on Artificial Intelligence (AI) implementation to harness the full potential and benefits of this technology, while continuing to “put people first” in all instances. We applaud the proposal for requesting transparency and responsibility from organisations using AI.

However, we do have comments we hope are helpful as the regulations are debated:

(1)	EFPA suggests, as have several other organisations, that the risk-based approach in Title II be revised to contain more levels than the current three now proposed. This will provide better differentiation between the types of industries and organisations using AI. We agree with many of the comments that the current High-risk category is too general and needs to be more granular, so that the precautions mandated, and regulations implemented, are appropriate for the various industries engaged with AI technology. For example, psychological, occupational, health, and educational assessments have for decades utilised automated scoring processes (to minimise human error) which could be considered as “AI” technology within the current definition. However, the interpretation of the results of this scoring should only be carried out by a professional and never left to mere digital automation. 
Assessments in the field of psychology are created by scientific professionals for use by practitioners in psychology and related disciplines. We are committed to continued steps to bring attention to the importance of responsible and ethical applications. We promote this through guidelines, the distribution of publications, and presentation at conferences to encourage the highest levels of academic and practice review.

(2)	EFPA encourages care in imposing regulations for documentation on AI systems, particularly those related to assessment. Fairness and privacy are central to our work, and we regularly conduct research to ensure our assessments are fair for all individuals, particularly when used for high-stakes decisions (such as clinical diagnosis, employment, or university acceptances). We strongly adhere to the rule that assessments should always be built on the foundations of rigorous science; and would never advocate that decisions be based on one type of assessment tool (whether using AI or other process). 
Psychologists increasingly use AI technology, especially where it is shown to be beneficial to accuracy and fairness during test administration and interpretation. In these cases, many safeguards are given including: clear records of all applied methodologies and data models, employing human oversight, and monitoring against bias to foster inclusion. The regulations need to consider and reflect the current oversight and standards for fairness and privacy already utilised by psychologists.

(3)	EFPA urges regulators to hold public discussions with organisations and associations that have taken the time to provide specific feedback. These discussions can be part of your presentation to the European Parliament and Council, as the information we have provided, for example, would be critical in understanding the need for more differentiation in the guidelines. Any legislative debate would therefore be more informative for all concerned, and result in subsequent regulations that are transparent, reasonable, enforceable, and effective. ",PUBLISHED,micro,Nigel,COM(2021)206,2021-08-06 20:38:24,withinfo,24212003,closed,,,
en,2665627,SWE,eu_citizen,,Burden,"Please, see attached file since our response requires more than 4000 characters.
Best, Håkan",PUBLISHED,,Håkan,COM(2021)206,2021-08-06 20:13:49,withinfo,24212003,closed,,,
en,2665626,GBR,academic_research_instittution,University of Cambridge (Leverhulme Centre for the Future of Intelligence and Centre for the Study of Existential Risk),Clarke,"We are a group of academic researchers on AI with positions at the University of Cambridge’s Leverhulme Centre for the Future of Intelligence (LCFI) and Centre for the Study of Existential Risk (CSER), and the Universitat Politecnica de Valencia. We have published dozens of academic papers and reports on the ethics and governance of artificial intelligence. Our past work has included submissions on the HLEG’s Draft Ethics Guidelines and the White Paper. We have had many useful discussions with the Commission, including presenting at the 2020 AI Alliance Assembly at a session chaired by Eric Badique.

We are generally very supportive of the proposed regulation, and particularly the risk-based approach, which recognises the varying levels of risk posed by AI systems in different contexts. We are optimistic that, as the first legal framework for AI worldwide, the proposed regulation can help set standards internationally for enabling the benefits and mitigating the risks of this powerful set of technologies. However, the impact this regulation has will depend on many specific details of implementation, and on how it can interact with other evolving parts of the AI governance ecosystem. 

As researchers focused on the long-term societal impacts of AI, we are particularly concerned with how and to what extent the proposed regulation can:

1. Adapt to new capabilities and risks as they arise. 
Historically, regulation has often struggled to adapt as technologies evolve (Crootof, 2019; Picker, 2001). Ensuring adaptive regulation is particularly important - but also particularly challenging - for AI, where technological progress is rapid, interacts with other emerging technologies, and can have wide-ranging impacts. 

2. Manage the broader societal impacts of AI.
Like all technologies, AI may cause societal-level harms, even if their direct impact on individuals is minimal. For example, the use of AI to generate fake content online may reduce overall trust in scientific information in ways that could cause downstream harms, without harming individuals directly. 

In this submission we make some recommendations for how the proposed regulation could better address these two points, both in terms of (a) how the regulation classifies ‘high-risk’ systems, and (b) what requirements the regulation subjects those systems to. However, we also recognise that regulation is not always the most appropriate tool for addressing longer-term societal challenges. We therefore also make some suggestions for where the broader AI governance ecosystem may need to be strengthened to complement the proposed regulation, and how the European Commission can enable and respond to these changes.

In summary, our recommendations are to:
•	Empower the Commission to add high-risk AI systems to the list in Annex III that:
                 are under areas outside of those listed in points 1-8
                 pose a substantial risk of societal harm
•	Consider how to identify and regulate high-risk uses of general purpose systems
•	Consider how to regulate companies that may intentionally underemphasise the role that AI plays in their decision-making
•	Authorise the European Artificial Intelligence Board to propose changes to the regulation’s Annexes
•	Schedule specific timeframes on which to consider revisions to which AI systems are covered by the regulation
•	Broaden the current requirements to include evaluation of broader societal harms, beyond risks to health and safety or fundamental rights
•	Be aware of the risk of common specifications being ‘captured’ by industry, and maximise participation in the setting of these specifications to reduce this risk
•	Provide even further clarification and indicative examples in the Chapter 2
•	Connect the proposed regulation to a broader governance ecosystem",PUBLISHED,small,Samuel,COM(2021)206,2021-08-06 20:07:54,withinfo,24212003,closed,,,
en,2665625,USA,ngo,Avaaz Foundation,Andrew,"Avaaz is the world’s largest online civic movement. Our 69 million members, including 22 million in Europe, campaign for urgent action on the key issues of our time - the climate crisis, ecological collapse and the erosion of democracy. We have been at the forefront of research on online disinformation, and this has led us to a deep understanding of the role artificial intelligence (“AI”) can have in exacerbating risks of harm to fundamental human rights. Please see our feedback to the Proposal for Harmonised Rules On Artificial Intelligence attached below

Whilst we applaud the Commission's multi-sectoral approach and think that its system of assessment is a major step forward, we are concerned about its tiered approach to risk assessment that leaves mainstream AI development without any standardised ethical framework or structures. This approach limits the assessment of AI to the relatively small section of perceived ""high-risk"" categories in Annex III and misses the fundamental point that AI, without human oversight, has the potential to usher in unparalleled risks to our human rights through myriad apparently low-risk systems.  

We believe that every AI system provider whose AI system poses a risk to health and safety, or a risk of adverse impact on fundamental rights should conduct an assessment of their proposed AI against the criteria currently set out only for “high-risk” AI. Our feedback also addresses the need for transparency, accountability and enhanced workers rights and call for bans on some specific AI systems. 

Please see our full feedback attached for suggested amendments on the following topics:

Section 1: The right framework for AI risk assessment
Our key suggestions in this section include: 
Risk assessment: 
Article 7: compulsory assessment process for all new AI systems that could pose risks of harm to health and safety or a risk of adverse impact on fundamental rights;
Article 7: risk-based approach for providers of any AI system that distributes content online through any form of AI assisted content distribution algorithm and meets the test of posing a risk of harm to health and safety or a risk of adverse impact on fundamental rights.
Transparency:
Article 13: transparency of operation on all AI systems that pose risks of harm to health and safety or a risk of adverse impact on fundamental rights.
Article 52: transparency in relation to the detection, prevention and investigation of crime.
Article 45: extension of the right to appeal to civil society organisations and external stakeholders when they have a legitimate interest in the decisions taken by the notified authorities designated by each member state to carry out third-party conformity assessment for AI systems. 
Accountability:
Article 14: human oversight for all AI systems.
Scope of application:
Article 2: additional provisions for (i) the export of AI systems; and (ii) international cooperation with organisations or third-countries. 
Enhanced Workers’ rights:
additional rights for workers subject to workplace AI surveillance or monitoring systems. 

Section 2: Artificial Intelligence practices which should be prohibited
Our key suggestions in this section include: 
Extended protection for vulnerable groups:
Article 5,1(b): extend the scope of protection to also include specific groups of persons due to their gender, sexual orientation, ethnicity, race, origin, including migrants, refugees and asylum seekers, and religion.
A ban on certain AI systems including private actors as well as public authorities, namely:
Article 5,1(c): social scoring; 
Article 5,1(d): ‘real-time’ remote biometric identification systems; 
Article 5,1(e): the use of AI systems categorising individuals from biometrics into clusters according to certain identitary criteria; and 
Article 5,1(f): AI systems to infer emotions of a natural person",PUBLISHED,medium,Sarah,COM(2021)206,2021-08-06 20:06:56,withinfo,24212003,closed,475565317526-24,,
en,2665624,DEU,company,Johner Institut GmbH,Nickel,Please find attached our input to the proposed regulation of Artificial Intelligence. We thank the Commission for the opportunity to provide feedback and appreciate any further dialogue. If you have any questions please feel free to contact us.,PUBLISHED,medium,Angela,COM(2021)206,2021-08-06 20:04:55,withinfo,24212003,closed,,,
en,2665623,USA,ngo,Climate Change AI,Kaack,"Climate change is one of the most urgent challenges of our time, and addressing it will require rapid and concerted action across many sectors of the economy. As AI has increasingly transformational effects on society, it is therefore critical to holistically account for the effects — both positive and negative — that AI may have on climate change. In this light, we would like to suggest that the proposed regulation more explicitly account for the potential risks of AI systems to increase greenhouse gas emissions. In addition, we believe that the legislation provides an opportunity to collect much-needed information for assessing the greenhouse gas emissions impacts of AI. 

Climate Change AI proposes two additions that would be central to appropriately accounting for and shaping the relationship of AI and climate change:

1. More explicitly involving climate change mitigation and adaptation in the classification rules for high-risk AI systems. In particular, more explicitly acknowledging environmental protection —  including reduction of greenhouse gas emissions to mitigate climate change — as one of the fundamental rights that, if affected negatively by the AI system, trigger a high-risk classification.

2. Expanding reporting requirements for high-risk AI systems to collect data on greenhouse gas impacts, including impacts through both computational energy use and the applications for which these systems are used. This approach would leverage the opportunity of reporting requirements for high-risk AI systems to collect much-needed data for decision-making on decarbonization strategies.

Please see the attached document for details.
",PUBLISHED,micro,Lynn,COM(2021)206,2021-08-06 19:57:59,withinfo,24212003,closed,,,
en,2665618,DEU,business_association,5G Automotive Association,Springer,"The 5G Automotive Association (5GAA) welcomes the opportunity to provide feedback on the Artificial Intelligence Act and share our recommendations to help the EU adopt future-proof legislation accelerating the market entry of connected and automated vehicles and smart mobility services while ensuring innovation in the long run. The EU is likely to become one of the first, if not the first, regions of the world to define rules about the placing on the market of artificial intelligence solutions. This will contribute to defining world-class standards and improve the trust of consumers. We respectfully invite European policymakers to recognise the evolutionary path of emerging technologies, such as artificial intelligence, while stimulating an effective competition on the merits by providing the wider industrial ecosystem of connected and automated vehicles with clear, predictable and concise criteria for the application of the Artificial Intelligence Act.

5GAA is a global, cross-industry organisation of more than 125 automotive, technology, and telecommunications (ICT) companies, working together to develop end-to-end connectivity solutions for future mobility and transportation services. 5GAA bridges the automotive and telecommunication industries to address society’s connected mobility and road safety needs with applications such as automated driving, ubiquitous access to services, integration into intelligent transportation and traffic management.

To read the full 5GAA position, please see the attached file",PUBLISHED,small,Johannes,COM(2021)206,2021-08-06 19:39:13,withinfo,24212003,closed,802312827842-26,,
fr,2665617,FRA,business_association,Fédération Française de l'Assurance,Haffad,"The French Insurance Federation (FFA) supports the implementation of a framework for an ethical use of AI: human-centric, unbiased, transparent, and explainable. French insurers welcome the Commission's proposal for a risk-based regulatory framework with differentiated rules according to the risks presented by AI uses that makes it possible to encourage technological innovation, while guaranteeing European values. However, FFA believes that the introduction of a regulation establishing harmonized rules on AI requires  very clear and precise definitions of its concepts: the definition of an AI system, the notion of negative impact and its difference with the notion of fundamental rights infringement as well as the actors of AI. Indeed, on this last point, insurers insist on the need to clearly define the role of each actor in the AI value chain to better identify the responsibilities of each one of them. FFA is grateful for the opportunity to share their views and contribute to the European Commission consultation. 

On the classification of high-risk AI systems 
- A relevant classification of high-risk AIs is essential to ensure that the activities of private companies providing essential services are not inappropriately impaired
- Need to clearly define several concepts and criteria used for the assessment as the notion of impact on fundamental rights as well to distinguish it from other potential negative impact. 

On the obligations of AI operators 
- The criteria for transparency should be defined more precisely. The notion of explicability should be given priority in the regulation.
- It seems important that the user is informed that he interacts with an AI system (e.g. chatbot) and have the choice and the possibility to be redirected to a human.
- Human control must be adapted, i.e. carried out by technical profiles but also business profiles or new specialized professionals and done in a proportionate way (e.g. by sampling) to not lose the benefits of automation, obtained through AI.
- Error-free datasets requirement is disproportionate and unworkable in practice.
- Recording keeping obligation needs to be clarified (terms, duration, etc.) and to consider practical impossibilities (e.g. versioning data). 
- Access to the source code of AI systems requirement should be reviewed for cyber security reasons; traceability or explicability requirements seem more proportionate and relevant.

Other comments 
- It is necessary to articulate the proposal with European texts relating to data that already exist or are under discussion as well as with supervisory bodies competences in order to ensure the complementarity of these texts to avoid contradictions or overlapping obligations.
- FFA is in favor of labels for AI systems but is concerned about the articulation of these new certification requirements with the standardization tools already in place, the cost that this may have and the workload that it will generate for the labelling of each AI system. EU should focus on a clear, simple, and workable labelling system and propose the labelling of AI development processes.
- FFA welcomes self-assessment compliance and third-party compliance schemes for certain types of AI. Self-assessment should be based on common basic criteria.

In conclusion, insurers are convinced that a certain number of existing rules already allow the use of AI to be regulated, but that the Artificial Intelligence Act can give more clarification, provided that it does not slow down innovation and the adoption of AI by European companies as well as not add a disproportionate additional layer of regulation introducing irrelevant obligations. However, it is necessary to consider the extensive legislative framework already regulating the insurance sector. Finally, EU must pay attention to maintain proportionality in the regulation in order to make Europe a pole of excellence and trust in AI (e.g. risk of becoming less efficient than other countries outside EU.)",PUBLISHED,medium,Isma,COM(2021)206,2021-08-06 19:30:35,withinfo,24212003,closed,5149794935-37,,
en,2665616,FRA,trade_union,Federation of Craft Businesses in the automotive sector and in mobility services (FNA)      ,CASTELL,"Federation of Craft Businesses in the automotive sector and in mobility services (FNA) would like to thank the authors of the Proposal of European Regulation laying down harmonized rules on Artificial Intelligence (AI) to inform citizens and stakeholders about the Commission's work in order to allow them to provide feedback on the intended initiative and to participate effectively in future consultation activities. FNA representatives also express their thanks for being given the opportunity of once again making submissions on the Consultation. They make the following comments.

Summary
Undue hardship on advanced technologies

New European Regulation laying down rules on AI should be in line with technical progress on the Automotive aftermarket.
FNA representatives welcome option 3 in particular stating mandatory requirements, Articles 64, 71, 84 and Annexes, also Commission Staff Working Document on the Impact Assessment of April 21,2021, in particular Problem 2 - Use of AI poses an increased risk of citizens' fundamental rights and Union values violations as use of AI may, indeed, lead to discriminatory outcomes keeping out independent repairers of the maintenance and repair services.
Therefore FNA representatives call on the European Commission to make effective the following fundamental rights
 -Unrestricted access to vehicle repair and maintenance data  
 -Competition in the market for Automotive data services and AI information
 -Actual consumer’s choice being analysed regularly to make it effective on the Automotive services
 -More financial and human resources for national authorities

Detailed comments are attached",PUBLISHED,small,Brigitte,COM(2021)206,2021-08-06 19:26:07,withinfo,24212003,closed,705440625408-01,,
en,2665615,USA,company,IBM,LECLERC,"IBM welcomes the opportunity to provide comments to the European Commission's draft Artificial Intelligence Act. We welcome the Commission’s risk-based approach to regulating specific uses of AI systems, not the AI technology itself.  IBM believes certain aspects of the Regulation could be clarified, especially to better delineate AI tools and AI systems and outline the resulting obligations of the various actors involved in supplying, training, deploying and using AI systems.     

IBM's detailed comments are attached. ",PUBLISHED,large,jean-Marc,COM(2021)206,2021-08-06 19:26:01,withinfo,24212003,closed,    7721359944-96,,
en,2665614,FIN,business_association,Technology Industries of Finland,Mäkinen,"The Commission has come along with an ambitious proposal to regulate AI. Getting the balance right in ensuring that obligations drive policy outcomes, while allowing AI innovators sufficient flexibility in meeting those obligations, is going to be critical. Especially the ethical requirements will drive forward trust and sustainable use of AI solutions.

AI one of the key technologies for reforming the European industry as AI-driven analysis and optimization tools can easily be implemented to any process of which data is available. AI plays a major role in making energy-consuming processes green. Use cases for AI can be numerous and it is essential to concentrate on regulation on truly horizontal issues, that are good software development practices. AI is software but the proposed definition covers not only AI, but basically all software. 

The proposal sets horizontal AI regime on top of harmonised NLF EU law. This structure brings along risk to cohesion of EU requirements. At least, this is an issue that needs close coordination from the Commission and close cooperation with the industries. Scope is set so that societally important use cases (Annex III) and AI-driven safety components and stand-alone AI products meant in the NLF framework are regarded being high-risk use cases.

General requirements of the Chapter II are quite well oriented with the process of development of AI systems. However, the requirements on data – complete and free form errors - veer away from realism. As a rule, requirements should follow good AI development practice, e.g. MLOps. Set of general and role-based requirements are not essentially well-suited for in-house or specifically developed AI systems. As the Act is so detailed, it will place a heavy administrative burden, that is most heavily felt on SME companies.  

As a new field of technology, there are no existing standards or technical requirements for many use-cases of AI. Commission cannot fix this by setting common specifications on its own, without any interaction with the industries. Here, the proposed regulatory sandboxes should be put into use. 

Commission has extensive powers to adopt delegated acts to adjust the definition of AI, use cases and set common specifications for AI. These major powers not only to fine-tune but to essentially adjust major elements of the Act bring along concern of legal predictability. 

Our suggestions 

•	Put more emphasis on due process and lineate requirements with good industry practices, such as MLOps.
•	Remove excessive, casuistic, and highly detailed requirements.
•	Concentrate on ethical issues and take good care of cohesion of regulatory requirements. 
•	Put regulatory sandboxes into proper use when developing common specifications for AI.
•	Introduce proper limits for Commission powers to adopt delegated acts. 
•	Ensure there is a predictable lot for low-risk AI needed to optimise promises in many areas of society by limiting the scope, definition and introducing strict criteria for adjustment of high-risk use cases.
•	To fuel innovation, build clarity on how privacy-preserving technologies can be used to facilitate processing of personal data. Ensure full lineation with the GDPR to have high-quality learning data for AI systems. 
",PUBLISHED,medium,Jussi,COM(2021)206,2021-08-06 19:14:47,withinfo,24212003,closed,39705603497-38,,
en,2665611,BEL,ngo,The Future Society,Moës,"The Future Society (TFS) is a global nonprofit advancing the responsible adoption of Artificial Intelligence (AI) for the benefit of humanity. With a network of policy researchers and practitioners in the EU, the US and all over the world, we build understanding of AI and its impact, we build bridges between relevant constituents, and we build innovative solutions to help communities and people all over the world enjoy the benefits of AI and avoid its risks. We are glad and grateful that many of our past recommendations have been integrated in the AI Act or in the associated debate. We feel the European Commission is setting itself up for success by considering adaptive, forward-looking instruments like the European AI Board and flexible tools like the sandboxes. Building on the evolution of the debate surrounding the AI Act in the past year, we therefore make additional recommendations to further improve on these and other aspects. Our recommendations fall under 3 categories:
A. Investing in smart governance capabilities for AI and the Digital Single Market
B. Ensure governance is adaptive and responsive to macro-trends
C. Avoid a “lemons market” by fostering trustworthy market dynamics
All our recommendations are explained in the document attached below. ",PUBLISHED,micro,Nicolas,COM(2021)206,2021-08-06 18:42:47,withinfo,24212003,closed,473310732515-30,,
en,2665610,BEL,company,ABB,Conforti,"ABB welcomes the opportunity to comment on the European Commission proposal for a Regulation laying down harmonised rules on Artificial Intelligence (“AI Act”). The AI Act is an important step towards the ambitious goal to promote and facilitate the uptake of trustworthy AI in Europe.

We believe the debate should equally balance a discussion on potential risks with a focus on the significant opportunities linked to the uptake of AI in industrial sectors. Europe’s AI regulation can only aspire to become a global blueprint if it can equally address risks, facilitate innovation and promote AI's development and application across businesses in Europe. 

You will find attached our input on specific Articles of the Regulation and our proposals to improve the current text. We remain at your disposal for further information and feedback throughout the legislative process.
",PUBLISHED,large,Vincenzo,COM(2021)206,2021-08-06 18:40:18,withinfo,24212003,closed,7790041608-14,,
en,2665609,GBR,other,Governance in AI Research Group (GAIRG) ,Thornton,"We welcome the proportionate risk-based approach of this proposed Regulation and its attempts to support innovation while protecting safety. We also welcome the important observation that the legislation shall apply to users of AI systems originating from third countries outside the EU.

However, we have some concerns and suggestions for improvements, which are detailed in the attached document.
",PUBLISHED,micro,James,COM(2021)206,2021-08-06 18:38:09,withinfo,24212003,closed,,,
fr,2665608,FRA,company,LA POSTE,Kulig,"Selon Le Groupe La Poste, le Hub France IA et la Villa Numeris, l’intelligence artificielle (IA) ouvre des possibilités multiples pour l’innovation et la croissance et nous souhaitons saisir tout son potentiel.

Le Groupe La Poste, qui place le numérique et l’IA parmi ses axes prioritaires et stratégiques de développement dans le cadre notamment du plan stratégique de « La Poste 2030, engagée pour vous », le Hub France IA, association pour la promotion de l'Intelligence artificielle en France, moteur et fédératrice des initiatives en IA en France et la Villa Numeris, association promouvant un modèle européen du digital basé sur l'humain, ont décidé de mettre en commun leur expertise en vue de s’associer pleinement aux débats européens sur l’IA et l’éthique. 

Nous nous réjouissons par conséquent de pouvoir partager nos positions à l’occasion de la consultation publique de la Commission européenne sur la proposition de règlement du Parlement européen et du Conseil établissant des règles harmonisées en matière d’intelligence artificielle (Artificial Intelligence Act) et modifiant certains actes législatifs de l’Union.

Vous trouverez notre contribution commune et nos propositions dans le fichier joint.
",PUBLISHED,large,Gaelle,COM(2021)206,2021-08-06 18:24:13,withinfo,24212003,closed,01890906437-84,,
en,2665607,IRL,company,Facebook Ireland Limited,ELVELID,"Facebook Ireland (“Facebook”) welcomes the opportunity to provide comments on the European Commission’s proposed Artificial Intelligence Act. AI is a uniquely powerful technology that must be used responsibly, and we believe thoughtful regulation can help ensure this responsible use. Please see the attached document for Facebook's response to the consultation on the proposed AI Act. ",PUBLISHED,large,Janne,COM(2021)206,2021-08-06 18:13:40,withinfo,24212003,closed,28666427835-74,,
en,2665605,BEL,ngo,European Healthcare Fraud and Corruption Network-AI Working Group,," EHFCN AI Working Group welcomes the proposal of publishing a regulation in the field of  Artificial Intelligence (AI), which will facilitate the direct establishment of common regulatory rules for all Member States. Feedback will be focused on the dispositions related to Public Health. Creating a European Artificial Intelligence Board is a step in the right direction, but the relevant articles (56 & 57) should also include a more rigorous stakeholder representation and also specific procedure of stakeholder consultation, according to the principles of transparency and representativeness. The provision of adoption of Codes of Conduct  (article 69) should also include implementing acts and be linked to the relevant codes of conduct of GDPR legislation. Also codes of conduct, as soft law provisions, should be user friendly (in the field of public health the patient is considered to be the user of healthcare services), written in a clear, accessible and comprehensible language for every EU citizen (not only in english). Regarding AI Regulatory Sandboxes (articles 53 & 54) and given the fact that public health constitutes a main area of development of AI regulatory Sandboxes and also that health data are a special category of personal data (GDPR, art. 9), it would be very useful to determine the criteria of data collection with a link to GDPR legislation .  
Finally the suggested categorisation of risk activities in AI could also be related to the categories of data used  ( open source high-level aggregated national/regional data, or anonymised  individual (health) data), incorporating a clearer definition, distinction and methodology of  secondary use of data and a solid distinction between legal dispositions and ethical considerations. ",PUBLISHED,micro,,COM(2021)206,2021-08-06 18:01:06,anonymous,24212003,closed,314832942966-21,,
en,2665604,DEU,company,Siemens Healthineers,Gastaldi,"Siemens Healthineers welcomes the initiative of the European Commission to set a global benchmark for deployment of ethical and legal artificial intelligence (AI) applications. AI has already brought numerous advancements to the field of healthcare by providing assistance to medical professionals in tasks as diverse as diagnosis, treatment, therapy, monitoring patients’ health, and even the management of hospitals and care institutions. We are the global leader when it comes to AI patent applications in medical imaging and have been a pioneer in AI development for more than 20 years, providing significant means of support to healthcare professionals in complex diagnosis and optimised treatment, as well as further expanding precision medicine .  

Siemens Healthineers is proud to be contributing to solutions that support doctors in the fight against the Covid-19 pandemic, cancer, or in enabling early diagnosis of Alzheimer’s. Medical technology is instrumental for current healthcare systems that operate in synergy with doctors, nurses and other care providers. To ensure that only safe and performant devices are placed on the European market, all our technology is fully in line with the regulatory frameworks for medical devices already in place - the Medical Devices Regulation (MDR) and In-vitro Diagnostics Regulation  (IVDR). These are robust, comprehensive and up-to-date frameworks that ensures legal certainty to all market players and society at large. They cover the entire cycle of the new medical product from development to placement on the market and putting into service, and continuous monitoring throughout its life cycle. Beyond the legal frameworks, Siemens Healthineers takes very seriously the responsibility towards end-users, especially patients, for placing safe and highly performing products on the European market very seriously.  

We strongly support the existing frameworks for medical devices, which already lay down distinct requirements for medical devices software, and in particular AI. As such, we recommend keeping addressing the specificity required by the medical device sector through the dedicated medical devices frameworks. This would best serve the twin purpose of ensuring legal certainty for European manufacturers and fully protecting end-users through tailored regulation that specifically addresses AI in medical devices. The existing rules already provide stringent regulations for the use of devices – including AI - in healthcare. We therefore oppose a-size-fits-all approach in law-making, which lacks the specificity to guarantee the highest level of safety in medical devices. If any gaps in existing rules should emerge as AI develops, they can and should be addressed by supplementing sector-specific rules and not through horizontal regulation. 

 Moreover, it is noteworthy, that despite the enormous potential for healthcare innovation in Europe, it is evident that other jurisdictions, have become the preferred location to first place innovative medical software on the market. We are deeply concerned that the proposed Regulation on Artificial Intelligence (AI Act), would further accelerate this process, and result in (i) stifling the development of innovative solutions in Europe, (ii) increasing the costs for healthcare systems, and (iii) most seriously, potentially depriving European patients and citizens of access to state-of-the-art digital health technology.  

We therefore believe that the proposal requires further consideration, and improvements, which we briefly present in our attached analysis.",PUBLISHED,large,Elisa,COM(2021)206,2021-08-06 17:56:10,withinfo,24212003,closed,982823533509-58,,
en,2665603,DEU,company,European AI Forum,Dickmann,"Please find attached joint response to the European Commission's proposal on the regulation of Artificial Intelligence by KI Bundesverband (Germany), Hub France IA, AI Austria, AI Cluster Bulgaria, Fundacja Digital Poland and CroAI (Croatia). We want to express our sincere gratitude to the Commission for the opportunity to provide our feedback. We appreciate any further dialogue and joint discussions to foster an innovative environment for AI made in Europe.",PUBLISHED,large,Alex,COM(2021)206,2021-08-06 17:52:24,withinfo,24212003,closed,,,
en,2665602,DEU,business_association,KI Bundesverband e.V.,Dickmann,Please find attached KI Bundesverband's response to the European Commission's proposal on the regulation of Artificial Intelligence. We want to express our sincere gratitude to the Commission for the opportunity to provide our feedback. We appreciate any further dialogue and joint discussions to foster an innovative environment for AI made in Europe.,PUBLISHED,large,Alex,COM(2021)206,2021-08-06 17:48:40,withinfo,24212003,closed,,,
en,2665600,BGR,eu_citizen,,Fessenko,"This position paper focuses on analysis of the proposal for a regulation of the European Parliament and the Council laying down harmonized rules on artificial intelligence (Artificial Intelligence Act) and recommendations on the conceptual design of and certain fundamental rules under the proposal with view to the need of an introspective, smart and fair legal framework for design, development, deployment and use of AI in the EU. Please refer to the attached submission for further details.",PUBLISHED,,Dessislava,COM(2021)206,2021-08-06 17:46:40,withinfo,24212003,closed,,,
en,2665599,USA,other,Engine,Lindfors,"Engine is a non-profit technology policy, research, and advocacy organization based in Washington, D.C., that bridges the gap between policymakers and startups. Engine works with government and a community of thousands of high-technology, growth-oriented startups to support the development of technology entrepreneurship. We appreciate the Commission’s interest in mitigating potential risks posed by AI technology but believe that the proposal could better balance these interests with fostering innovation that can benefit EU citizens, companies, and governments. Please see the attachment for Engine’s perspective on the proposed law. ",PUBLISHED,micro,Nathan,COM(2021)206,2021-08-06 17:45:10,withinfo,24212003,closed,,,
fr,2665597,FRA,trade_union,CFE-CGC,Blanc,"La Confédération française de l’encadrement - Confédération générale des cadres (CFE-CGC) est un syndicat français de salariés fondé le 15 octobre 1944 sous le nom de Confédération générale des cadres (CGC), qui présente la caractéristique de défendre les intérêts d’une catégorie professionnelle spécifique, l’encadrement. 
La CFE-CGC salue la proposition de règlement de la Commission européenne établissant un cadre de régulation en matière d’intelligence artificielle.
C’est pour nous une bonne chose qu’un cadre réglementaire vienne encadrer les pratiques dans les entreprises avec l’arrivée du management dit « algorithmique ».
Les enjeux sont grands en termes de responsabilité et de liberté avec des outils d’aide à la décision qui peuvent se révéler opaques et générateurs de biais. 
Créer des obligations et des pénalités associées, dans la logique de ce qui avait été fait sur le RGPD, permet de responsabiliser les fournisseurs d’IA, tout en informant et en protégeant les utilisateurs d’IA.",PUBLISHED,large,Nicolas,COM(2021)206,2021-08-06 17:30:01,withinfo,24212003,closed,,,
de,2665596,DEU,eu_citizen,,Haimerl,"Sehr geehrte Damen und Herren,

vielen Dank für die Gelegenheit, Feedback zum Entwurf der Verordnung für Künstliche Intelligenz geben zu können. Die gesammelten Anmerkungen zu dem Entwurf entnehmen Sie bitte dem Dokument im Anhang. Bei Rückfragen können Sie sich gerne an mich wenden.

Mit freundlichen Grüßen
Martin Haimerl ",PUBLISHED,,Martin,COM(2021)206,2021-08-06 17:22:55,withinfo,24212003,closed,,,
en,2665595,LTU,business_association,INFOBALT,Dirma,"INFOBALT  welcomes the European Commission’s Commission for its leadership in developing a regulatory framework for the responsible development and use of artificial intelligence technologies (“AI”), and the opportunity to provide feedback via the public consultation phase. We are pleased to see the Commission continue this open engagement with stakeholders in the development of the right policy and investment framework for AI. Our members represents an incredible variety of organisations, all of them affected by AI-related technologies. In this contribution to the  consultation, we present and develop our approach towards a balanced yet effective regulatory framework. We provide recommendations on how and where policy-makers should prioritise. A more principles-and results-based approach that is calibrated for the AI ecosystem will help to drive clarity for regulated actors and spur innovation.
•	Ensure that AI competences, qualifications and skills can be easily identified, validated, recognised and understood. This are up-to-date with job market needs and reflect industry skills recognition, which make them immediately transferable across the EU.
•	Important to reflect the complexity of the AI ecosystem in the balance of obligations for different stakeholders. It will seldom be feasible or effective for providers of general-use AI systems to manage all of the risks associated with potential application in high-risk systems, as is currently envisaged. The provider will often not have access to the operational data necessary for post-market monitoring if the AI system has been put into operation by another entity. We recommend a new class of “deployers” be added to the AIA with responsibility for complying with regulatory requirements associated with deploying general-use AI systems in high-risk applications.
•	Clarify certain provisions to provide legal certainty around scope and protect privacy. The AIA includes a number of terms and provisions that would benefit from further clarification to ensure that providers of AI systems understand how the scope and requirements of the AIA apply to their products. For example, the definitions of “safety components” and “significant changes” could be further clarified to provide legal certainty around when systems come in scope of requirements for high-risk AI systems
•	Important to ensure that any conformity assessment clearly distinguishes risks to safety and value-based risks, and that existing rules on market surveillance or conformity assessments are not duplicated. It would also be recommended to integrate any requirements imposed on AI into existing conformity assessment processes, rather than building a new stand-alone horizontal process for AI.
•	Required to hold providers, deployers and users to feasible standards. As currently phrased, certain requirements of the regulation will be extremely difficult or impossible to meet in practice (the Art 10(3) requirement that datasets be “free of errors and complete” demands a level of perfection that is not technically feasible). Requirements should be composed in a fashion that reflects feasible, best practice standards.
•	Compliance assessment of an AI framework should factor in existing provisions in other pieces of legislation which require certain classifications and risk assessment. Provisions should be structured to refer to existing requirements in order to contribute to enhanced legislative clarity and to avoid any inconsistencies, through the development of sector specific guidance documents issued by the respective regulatory authorities. Encourage innovative, risk-based and collaborative regulatory oversight.
",PUBLISHED,small,Virgilijus,COM(2021)206,2021-08-06 17:21:34,withinfo,24212003,closed,,,
en,2665594,IRL,company,LinkedIn,O Sullivan,"LinkedIn joins Microsoft, our parent company, in welcoming the opportunity to comment on the European Commission’s proposed Regulation for harmonised rules on AI (the “AI Act” or the “Act”) and the New Coordinated Plan on AI. 

We are providing the following brief supplement to Microsoft’s submission to share comments specifically concerning the draft AI Act’s positioning with respect to AI systems used in recruiting and workforce management.",PUBLISHED,large,Kate,COM(2021)206,2021-08-06 17:14:52,withinfo,24212003,closed,,,
en,2665591,CHE,ngo,Global Legal Entity Identifier Foundation (GLEIF),Mentesoglu Tuncer,"The Global Legal Entity Identifier Foundation (GLEIF) is pleased to provide comments to the European Commission's Proposed EU Artificial Intelligence Regulation. GLEIF will focus its comments on how using the Legal Entity Identifier (LEI) in the AI framework can enhance traceability and transparency.

GLEIF agrees that the nature of AI often relies on large and varied datasets, which significantly impacts the quality and reliability of the AI systems. Algorithms rely on quality data, and most firms recognize the need for quality data inputs. In particular poor-quality identification data renders AI and machine learning approaches less reliable, and worse, may result in missing the signal in the data and or unexpected behavior of the algorithm. For example, in the financial industry, a GLEIF research demonstrates that financial firms use on average 4 identifiers for their legal entity clients which leads to low confidence in the reliability of the associated client reference data. Considering the interconnectedness of the financial sector and institutions, several institutions' use of low-quality data can negatively impact risk assessment and exposure analysis and create a challenge for financial stability. This is also applicable in the usage of AI for financial risk solutions. These new solutions use AI technology to aggregate and analyze client data across financial crime detection systems such as know your customer (KYC) and anti-money laundering (AML) to improve outcomes. The success of these solutions will be dependent on the data input that is leveraged. Given the Commission's proposal aims to strengthen the Union's role significantly to shape global norms and standards and promote trustworthy AI and provide the Union with a powerful basis to engage further with its external partners, an explicit reference to the global data standards, including the LEI for legal entity identification as part of the dataset, should be key in international fora on issues relating to the identification in AI. 

GLEIF would like to highlight that each AI solution is a product of multiple components. Each algorithm develops its patterns based on the input and the parameter set. The lack of labeled data is one of the most often encountered challenges when it comes to application of AI approaches. With the LEI code and the accompanying publicly available reference data, GLEIF offers a high quality labeled data set, that could be used for (i) different learning problems, (ii) as a benchmark data, or (iii) to be mapped to additional internal data in order to enhance the internal data and provide the data label. 

In the proposed Regulation, it is mentioned that for high-risk AI systems, the requirements of high quality data, documentation and traceability, transparency, accuracy and robustness, are strictly necessary to mitigate the risks to fundamental rights and safety posed by AI and that are not covered by other existing legal frameworks. Harmonized standards and supporting guidance and compliance tools will assist providers and users in complying with the requirements laid down by the proposal and minimize their costs. To satisfy these requirements, GLEIF would like to suggest that the Commission adds the LEI to the requirements under the ""ANNEX VIII INFORMATION TO BE SUBMITTED UPON THE REGISTRATION OF HIGH RISK AI SYSTEMS IN ACCORDANCE WITH ARTICLE 51"" and “ANNEX V EU DECLARATION OF CONFORMITY” to uniquely and unambiguously identify the AI system provider. 

Please see GLEIF's full response attached. ",PUBLISHED,medium,Burcu,COM(2021)206,2021-08-06 17:01:49,withinfo,24212003,closed,660337819709-62,,
en,2665590,HRV,ngo,CroAI - Croatian AI Association,Štedul,"Putting startups at the heart of AI innovation - CroAI’s opinion on the European Commission’s Artificial Intelligence Act

In April 2021, the European Commission (EC) published its much-awaited Artificial Intelligence Act (AIA), the first global attempt to establish a legal framework for a technology that, as the AIA states, carries both benefits and risks to humans and society. The Croatian AI Association (CroAI) welcomes the EC’s efforts to set its own approach to AI, as it previously did with privacy. However, our main concern is that the AIA does not adequately address the needs of start-ups, who are the main drivers of innovation. 

CroAI believes that the AIA must be an enabler of AI innovation and strongly stand behind startu-ps, especially during prototyping and testing while pursuing a product-market fit. We therefore advocate for the AIA to include unequivocal support for innovators by mandating the following measures:

1. Startups are allowed to create their own sandboxes on a case-by-case basis rather than a one-size-fits all approach, due to the unique nature of each test. 

2. Start-ups will follow a Code of conduct. The Code of conduct will help them mitigate risks while testing through tools such as limiting the number of test users, human oversight, purchasing insurance, transparency, and accountability.

3. While in their sandboxes, startups do not need to involve supervisory authorities. Still, they are accountable for complying with the Code of conduct.

4. When leaving their sandboxes, which means that they have found a product-market fit, startups need to invest in fully complying with AIA rules and regulations, which will make much more sense at that time in a product’s development cycle.

The principal concern founders and investors have when thinking about doing AI in Europe is the cost and unpredictability of complying with the AIA. They see it as an unnecessary risk and a burden that they can easily avoid by moving a start-up to some other innovation hub in the world. CroAI believes that by integrating these measures into the AIA, the EC will address most of those concerns and make the EU an excellent choice for AI innovation.

CroAI is at the European Commission’s disposal to elaborate more on the reasoning behind this proposal and how to get it to life.
",PUBLISHED,micro,Jan,COM(2021)206,2021-08-06 16:50:30,withinfo,24212003,closed,368152838541-76,,
en,2665589,FRA,ngo,Impact AI,Rugina,"Impact AI Position Statement on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act)
2
Dear Sir/Madam,
Impact AI welcomes the European initiative to establish first ever legal framework on Artificial Intelligence (AI) aiming to promote Europe’s innovation capacity in AI while supporting the development and uptake of ethical and trustworthy AI across the European Union (EU). It is a great achievement and a worldwide leading initiative that opens the path to an appropriate use of the leading-edge AI technologies in the world.
While AI has become a clear and undeniable force for business disruption and transformation, it also entails certain risks, such as potentially exposing people, including children, to significant mistakes that may undermine fundamental rights and safety, gender-based or other kinds of discrimination, opaque decision-making, or intrusion in our private lives.
Faced with the rapid technological development of AI and a global policy context where more and more countries are investing heavily in AI, the European Commission (EC) has acted and proposed a project for regulating AI usage.
At Impact AI we believe that designing and implementing frameworks to manage AI usage in an ethical, robust, controlled and secured manner is key for supporting adoption and holistic transformation of the organization. We have created a task force and communicated on the market on the best practices to deploy trustworthy AI across organizations.
We believe that this kind of legal regulation is necessary to guarantee the fundamental rights of EU citizens and residents. However, the Proposal of the EC is rather risk-oriented neglecting the wonderful benefits and opportunities the AI can bring to European citizens, to the society as well as to both the European economy and the economies of our trading partners.
As a group of companies representing several industries and of different sizes, we wanted to take the opportunity to share some comments on the core principles and themes that we consider important when promoting the uptake of trustworthy AI in Europe.
We thank you in advance for your consideration.
Best regard,
Impact AI collective",PUBLISHED,micro,Roxana,COM(2021)206,2021-08-06 16:46:35,withinfo,24212003,closed,RNA W923009945,,
en,2665587,ITA,company,Unipol Gruppo S.p.A.,Oliva,Please refer to the attached document.,PUBLISHED,large,Michele,COM(2021)206,2021-08-06 16:41:17,withinfo,24212003,closed,,,
en,2665586,NLD,company,Philips,Appelfeld,Philips welcomes the opportunity to comment on the proposed AI Act. Please see our feedback attached. ,PUBLISHED,large,Aleksandra,COM(2021)206,2021-08-06 16:39:36,withinfo,24212003,closed,035366013790-68,,
en,2665583,DEU,company,Infineon Technologies AG,SEBASTIAN,"Overall, Infineon Technologies AG welcomes the fact that the Commission has presented a risk-based approach aiming to regulate AI systems and their effects on economy and society. In our contribution to the upcoming discussion, Infineon focuses primarily on Article 3 (1) and Annex I – i.e. the definition of AI systems. Infineon proposes that the following aspects should be taken into account in the subsequent debate between the Commission, the European Parliament and the Council:

>>> The definition of AI systems should be critically revised <<<
- The question of which definition is “the best"" will never be answered conclusively. However, Europe will have to answer the question of whether the definition is “meaningful” in terms of the regulatory intention in a compelling and as precise manner as possible. The definition of AI systems will fundamentally determine whether the intended effect will be achieved with the ""AI Act"".
- On the one hand, the definition in the current draft is clearly too broad, because it also covers conventional software.
- On the other hand, the definition is too narrow, because it completely ignores hardware. This contradicts the OECD definition and the understanding of the Enquete Commission ""Artificial Intelligence - Social Responsibility and Economic, Social and Ecological Potentials"" of the German Bundestag. In case of hardware, one could e.g. think in particular of edge AI devices, which have a decisive influence, for example, on the energy consumption of digitalisation and security aspects, and which are important for strong European key industries.
- However, the chosen approach to the definition of AI systems may not even be helpful at all, because so far it mainly focuses on the technology itself, and only to a lesser extent on its effects, as will be explained below.

>>> The AI Act should regulate more the WHAT / effects and less the HOW / technology <<<
- AI regulation should not focus on individual implementations and technologies, i.e. exactly HOW an individual function was implemented (specific technology, hardware or software etc.), but on the potentially critical properties that can be associated with the use of AI and machine learning. In particular, the possibly limited predictability and validation (full validation and qualification) of the system response and effects resulting from the difference of data sets used for learning compared to the data in use (e.g. unintended bias, incompleteness of the learning data related to the actual use etc.). Source: AI definition of OECD and the German Bundestag
- A technology-specific regulation like the “AI Act” would in any case effectively have an innovation-inhibiting character - e.g. if it is aimed at embedded systems in which, for example, clearly defined and validatable functions are implemented as a neural network for the purpose of higher accuracy, energy efficiency or speed.
- In the present draft of the ""AI Act"", the definition of AI systems deals exclusively with the HOW, i.e. with technology. Due to this fact alone, the definition - regardless of its concrete final wording! - will always lead to a) some AI systems being unnecessarily covered by this regulation whose effects are predictable and desired, or b) some AI systems not being covered by this regulation because they are not (or not yet) covered by the scope as described in Article 3 (1) and Annex I.
- Therefore, it is not sufficient to discuss and adapt the definition of AI systems in Article 3 (1) and Annex I only, but the foundation of the AI Act must substantially be more focused on the effects than on the technology itself. This different approach could, for example, already be anchored more strongly in the preamble to the AI Act.

To summarise:
The proposal of Infineon is to start a debate on the above aspects as broad and open as possible between all relevant stakeholders.",PUBLISHED,large,Ina,COM(2021)206,2021-08-06 16:27:53,withinfo,24212003,closed,10751968675-85,,
en,2665582,GBR,other,SHERPA Project,Santiago,"SHERPA (Shaping the ethical dimensions of smart information systems (SIS) – a European perspective) is a EU-funded project that focuses on ethical and human rights aspects of smart information systems (artificial intelligence and big data analytics). SHERPA believes a robust, mandatory legal framework at the EU level is needed to ensure that ethical issues and human rights concerns related to AI are adequately addressed. Regulatory governance systems are a key part of the SHERPA Final Recommendations, which include a call for creating an EU regulatory framework and establishing an EU Agency for AI. 

There are many specific elements of the SHERPA recommendations that appear in the proposed text of the Regulation. However, the draft text does not do enough to protect fundamental rights, often lacks conceptual clarity, and leaves many questions unanswered. For example:

Risk-based approach: The risk-based approach adopted by the Commission may not be sufficient to guarantee safety and fundamental rights. There is no reference to the EU Agency for Fundamental Rights, nor are there provisions on complaint and redress mechanisms. Furthermore, the proposed regulation has a somewhat binary approach, failing to adequately take into account impacts across the spectrum of risk.

Definition of AI: SHERPA recommended that AI be clearly defined in each use context. While it is a challenge to precisely define AI, definitions used in the proposed Regulation are often overly broad and too open to interpretation. Additionally, despite attempts to be “technology neutral and as future proof as possible”, the proposed definition of AI is linked to ‘software’, leaving out potential future developments of AI. 

Red lines: While SHERPA welcomes the explicit inclusion of red lines in the regulatory framework, the short list is incomplete and has many loopholes.  For example, use of remote, real-time biometric identification in public spaces, including facial recognition, by law enforcement is technically banned, but can still be used for specific purposes subject to prior authorisation. Additionally, the draft Regulation does not apply to military applications, and therefore lethal autonomous weapons systems (LAWS) and military drones are not addressed.

Mandatory requirements for high-risk systems: SHERPA strongly welcomes a list of mandatory requirements including both ex-ante (before on market) and ex-post (after on market) enforcement mechanisms. However, more work is needed to ensure that the requirements have a high enough degree of detail and specificity so that developers, providers and users understand and can meet their legal obligations. Furthermore, the SHERPA recommendations on impact assessments and Ethics Officers could also be incorporated as part of the mandatory requirements to strengthen the impact of the Regulation. 

EU Agency for AI: Similar to the proposed European Artificial Intelligence Board, SHERPA called for the creation of a new centralised independent EU Agency for AI to help ensure cooperation, coordination and consistent application of EU law. While both proposals envisage a mandate for the new body focused on making recommendations and issuing guidance, SHERPA recommends more a independent structure that includes permanent representation from diverse stakeholder groups on standing committees including a scientific and technical committee and an advisory committee.
",PUBLISHED,small,Nicole,COM(2021)206,2021-08-06 16:22:31,withinfo,24212003,closed,,,
en,2665580,BEL,ngo,European Evangelical Alliance,DE PATER,"Summary
The European Evangelical Alliance welcomes the draft AI law the European Commission presented. It is a good start for further negotiations but more needs to be done to protect humanity, both individually and collectively.

We would like to see the following applications of AI added to the high-risk category:
•	Research and Development of certain AI applications: these are not explicitly excluded from the regulation, allowing entities to create AI which may be detrimental and harmful to humanity but that, if popularised and profitable, could become mainstream and normalised.
•	Any AI which extrapolates and uses in-depth insights into a person that the person may not themselves be aware of as this knowledge could be used to manipulate, to excessively nudge, or inappropriately interfere with a person’s freedom of thought, critical thinking processes, undermining their judgment.
•	Augmented AI and transhumanism – The current draft does not include any provision in relation to augmented humans and computer brain interfaces which advance or enhance human capacity and capability. There needs to be greater discussion as to the bioethics and AI ethics of such applications and clarity in law of the boundaries of what is considered safe and morally acceptable, particularly given that this kind of adaptation will have an (in some cases) irreversible impact on a person(s) and impacts on how society views and treats disability, amongst other physical and body traits.
•	Any AI which is either a standalone product or as a component part of a device or machine which has a detrimental impact on the environment.

Harm is more than just a specific violation of individual fundamental rights. The negative impact of Artificial Intelligence on moral agency, relationships, cognitive skills, dignity, life’s opportunities and the environment should be taken into account as well. Therefore, we suggest the adoption of a specific Declaration of Digital Human Rights. This document should include a ban of all biometric identification and categorisation systems.

To increase trust in AI, there should be a comprehensive assessment for all AI, not just AI believed to be high-risk today. Further, the discussion about the future of AI should not be limited to business and developers. Civil Society should be included in shaping the future of our societies.

Please, find a more detailed response attached.",PUBLISHED,small,Arie,COM(2021)206,2021-08-06 16:14:10,withinfo,24212003,closed,8314342726,,
en,2665579,GBR,business_association,techUK,Holden,Please see attached document. ,PUBLISHED,medium,Katherine,COM(2021)206,2021-08-06 16:11:42,withinfo,24212003,closed,,,
en,2665578,AUT,ngo,Women in AI Austria,Hafez,"Women in AI Austria welcomes the opportunity to comment on the proposed Artificial Intelligence Act. 

We commend the work of the European Commission in developing a framework for artificial intelligence and algorithmic systems. However, we believe there are several areas that require expansion which we seek to bring to your attention, and hope our feedback - submitted as a PDF - contributes to the further refinement of the proposed provisions.",PUBLISHED,small,Valerie,COM(2021)206,2021-08-06 16:01:07,withinfo,24212003,closed,815698241750-24,,
en,2665576,NLD,ngo,Open Future Foundation,Vogelezang,Please find our comments enclosed.,PUBLISHED,micro,Francesco,COM(2021)206,2021-08-06 15:52:52,withinfo,24212003,closed,,,
en,2665574,BEL,business_association,European DIGITAL SME Alliance,Linck,"The European DIGITAL SME Alliance thanks the European Commission for the opportunity to provide feedback on the proposal for a European Act on Artificial intelligence (the AI Act). 

DIGITAL SME experts who are part of the internal AI & Standards Task Force have prepared comments regarding the AI Act and its potential impact on SMEs, which you can find attached. We are  expecting further comments from our members by September in response to our wider members' consultation. Thus, we will submit our official position in the beginning of September and update the attached document if applicable.",PUBLISHED,small,Annika,COM(2021)206,2021-08-06 15:43:04,withinfo,24212003,closed,,,
en,2665573,IRL,company,Medtronic plc,Curran,Please find our feedback attached. ,PUBLISHED,large,Alma,COM(2021)206,2021-08-06 15:36:26,withinfo,24212003,closed,326347337793-89,,
en,2665568,DEU,company,BVI,Ertl,"BVI welcomes the opportunity to provide its views on the European Commission’s proposal laying down harmonised rules on artificial intelligence. 

The use of artificial intelligence (AI) and machine learning (ML) in asset management bears great potential. An increasing degree of automation of processes and interfaces has been common practice in the asset management industry for decades and is described with the keywords ‘business process automation’ (BPA) or ‘robotic process automation’ (RPA). RPA aims to automate even more complex process steps along the value chain in the asset management industry. An example of this would be a standardised online client check and initial advice in the securities business, possibly using language programmes. The use of AI is a major issue in the financial sector, as these technologies will bring about a profound change in society and the economy. AI goes beyond BPA and RPA by combining the use of large or increasingly available, but often unstructured and internal and/or external data sets with the improved possibilities for using these data. Through a combination of analytics and mass available data, new insights are to be gained that would not be possible with traditional research methods. The German supervisory authority BaFin was one of the first supervisors to analyse the challenges and implications for supervision and regulation of financial services in its report ‘Big Data meets artificial intelligence’. BaFin concludes that big data and AI bring about a profound change and enable innovation, successful implementations can spread rapidly, and supervision and regulation must address innovative developments early. Thus, we welcome the European Commission’s proposal on harmonised rules on artificial intelligence to establish a principal-based regulatory framework in the EU.

Asset management will be significantly influenced by improved availability of data, algorithms, digitalisation of assets, new processes in custody and settlement, and reporting. Quality data is a prerequisite for the provision of any service along the entire value chain in asset management, from research, portfolio and risk management, trading to clearing and settlement. Secure access to and availability of high-quality financial market data at all times is also indispensable in fund distribution or in regulatory and reporting. In the future, more and more non-traditional data sources (""big data"") will be integrated into the asset managers' business operations. 

Financial market data are often offered by natural monopolies and oligopolies such as stock exchanges and companies with a dominant market position. These have great market power and can set one-sided conditions, since the users on the asset manager side rely on such data and any disruption would jeopardise their business. The use of financial market data has therefore for years been associated with regular, sometimes massive price increases and the conclusion of increasingly complex data licences for the asset managers. With increasing cost pressure and the change of business models to more quantitative or passive investment, data costs are becoming more and more a success factor for many asset managers. The BVI advocates a revision of the existing EU regulations for the provision and use of financial market data on appropriate commercial terms, e.g. in MiFID/MiFIR, Credit Rating Agencies Regulation (CRAR), and an implementation of data user effectively protective regulations, e.g. in the Benchmark Regulation and the various EU regulations on regulatory reporting. Data charges should be determined on the basis of the marginal cost of producing and disseminating the data. 

Please see the attached position paper for more detail.",PUBLISHED,medium,Felix,COM(2021)206,2021-08-06 15:19:05,withinfo,24212003,closed,96816064173-47,,
pl,2665567,POL,company,Kancelaria Radców Prawnych Konieczny Wierzbicki,Syguła,"Dzień Dobry,

Przesyłamy stanowisko Konieczny Wierzbicki Kancelaria Radców Prawnych sp.p. 
w ramach konsultacji publicznych nad projektem rozporządzenia unijnego ARTIFICIAL INTELLIGENCE ACT.
",PUBLISHED,small,Paweł,COM(2021)206,2021-08-06 15:16:26,withinfo,24212003,closed,,,
en,2665565,BEL,company,Nokia,DAMAS,"Nokia congratulates the European Commission on the impressive accomplishment of proposing on 21 April 2021 what effectively constitutes the first regulation on Artificial Intelligence in the world. 

The proposal is clearly based on a significant amount of research, information rounds and consultations of numerous stakeholders. We regard it as a courageous first take on a topic that is highly debated by a wide variety of specialists. 

Nokia is honored to have been given the opportunity to contribute to the debate through its participation in the High-Level Expert Group. We are equally keen to continue to play a part in clarifying the legal and regulatory regime applicable to Artificial Intelligence systems.

The present position paper represents Nokia’s initial feedback to the Commission’s public consultation on the proposed package and touches upon a limited number of issues which we consider need urgent clarification.

Nokia thanks the European Commission for the opportunity to participate in this consultation. We are available for further discussions and would be honored to contribute our expertise for purposes of clarifying the issues highlighted above.",PUBLISHED,large,Florian,COM(2021)206,2021-08-06 15:14:12,withinfo,24212003,closed,35167875358-33,,
en,2665564,BEL,business_association,TIC Council,Nguyen,"TIC Council, representing independent Testing, Inspection, and Certification (TIC) companies, welcomes the Commission’s proposal for a Regulation establishing harmonized rules on Artificial Intelligence (AI), which moves toward greater safety and security for European consumers.  

AI is a fast-evolving technology with many benefits but also many challenges. As independent TIC companies, we are convinced that the safety and security of customers must prevail. We therefore support the provisions of the proposal that will help meet these objectives:  
- We support the risk-based approach that has been adopted for the proposal as it minimizes the potential impact on customer’s fundamental rights.  
- We also agree that safety and security components of products should, as proposed, be subject to third party ex-ante conformity assessment according to the relevant sectoral legislation. Following an already established system will certainly help to avoid legal uncertainty.  
- We welcome the possibility of regularly updating Annex III of the text. This will have to follow closely new technology developments to avoid legal gaps. 
- We also support the provision mandating new ex ante re-assessments of the conformity in case of substantial modifications to the AI systems.  
- We welcome the non-discriminatory access to data for validation of high-risk systems also for notified bodies through the European Common Data Spaces.  

However, TIC Council would like to express its concerns about certain provisions in the text that hinder the protection afforded to consumers, the safety of AI systems and contravene the objective of the text.  

TIC Council advocates a mandatory third-party conformity assessment procedure for stand-alone AI-systems, explicitly listed in Annex III and for new products with AI systems if the goal of the text is to be achieved. The self-declaration procedure by industry could either affect consumer’s fundamental rights or be impractical for some significant parts of the AI innovation ecosystem. On the contrary, TIC companies already have the capacity, resources, infrastructure, and expertise to perform the monitoring and assessment of AI-systems. TIC experts are able to perform the required assessments with a high degree of certainty, highly mature processes and organizational structures within the assessment organization to ensure reliability and replicability of the assessments. Thus, these AI-systems will be preventively assessed by TIC companies and will access the internal market in a safe and compliant manner.  

Then, one of the pillars of the text is to make AI trustworthy. TIC companies, characterized by their independence and expertise, are the actors that can effectively participate in the improvement of the AI environment and make it trustworthy, especially for European consumers. TIC companies conducting third-party conformity assessments guarantee impartial and comprehensive testing, inspection and certification of AI systems entering the European market. Ultimately, third-party conformity assessment procedures will boost consumers’ confidence and encourage them to choose AI-systems that meet the highest standards of protection.   

Finally, we call on the Commission to also grant access to the European Common Data Spaces for lower-risk systems, to allow notified bodies to provide their services even outside the mandatory framework set by the Commission. As a result, TIC companies conducting third-party conformity assessments will contribute to a high level of consumer protection and better conditions for fair competition and innovation.  ",PUBLISHED,small,Mann,COM(2021)206,2021-08-06 14:59:17,withinfo,24212003,closed,840667012559,,
en,2665563,BEL,business_association,Beltug ,Jacobs,"Beltug - VOICE - Cigref - CIO Platform NL - We are the Belgian, Dutch, French and German CIO-associations; the communities of Chief Information Officers (CIO’s) and other senior leaders that are responsible for digital technologies and digital transformations within private or public organisations. We do not represent ICT providers and consultants. 

Business users of digital technology are a key link in the digital transformation of society and economy: including the use of AI systems. To reap the benefits of Artificial Intelligence in Europe, the position of business users - companies and government organisations - of digital technology must be taken into account. Business users also play an influential role in the development of AI systems that comply with European standards and values through their joint purchasing power in the market for AI systems. Therefore, we are a main stakeholder in the establishment of the AI Act.

We have identified five points that we believe require additional attention or should be re-evaluated. Taking into account these aspects would help create an AI-Act that enables business users to effectively contribute to European economy and society by using AI systems.
General overview of AI and the proposed AI Act
Companies and governments use AI systems to an increasing extent in essential business processes. With that, the impact of AI on the daily life, health, mobility, and safety of EU citizens grows. AI systems enrich and expand the possibilities of the digital domain through their ability to process (large amounts of) data in order to gain insights or execute actions (with or without human intervention). By these means, AI systems can improve business processes for both companies and government organisations and create value for organisations, the economy and society. 

For example, in healthcare AI systems make diagnoses by comparing images of a patient with a gigantic database of images. Another example, AI systems are used to prevent the failure of factory lines by timely signalling the wear off of parts outside their “regular” patterns. Furthermore, AI technology is and will increasingly be a necessary component in cyber security solutions because of its great strength in for instance pattern recognition. 

Consequently, we support:
1.	The appropriate framework that the AI Act introduces for the safe, responsible, and sustainable use of AI systems in Europe, essential for the full exploitation of AI systems in the years to come.
2.	The risk-based approach of the AI Act by establishing classes of AI systems for which the intensity of requirements increase with the risk that an AI system poses to the health and safety of EU citizens and their fundamental rights. The risk-based approach provides clarity and oversight, without creating unnecessary market entry barriers for low-risk applications of AI systems.
3.	The application of the AI Act to all AI systems that are placed on the market or have their output in the European Union. 

We would like to express our serious concerns about five aspects of the proposed AI Act:
1.	Scope and risk classifications should keep pace with evolution of AI technologies and market situations
2.	The need for guidance for developers of AI and users
3.	The fairness of the balance in the responsibilities of providers and users
4.	The assurance of compliance and harmonised approach to oversight
5.	The scope and application of the sandbox environments to stimulate innovations

The aspects mainly concern high-risk AI systems. We elaborate on the five aspects in the file attached to our answer. 
",PUBLISHED,micro,Danielle,COM(2021)206,2021-08-06 14:55:43,withinfo,24212003,closed,488493238396-32,,
en,2665562,BEL,other,OpenForum Europe,Grzegorzewska,"We welcome European Commission's draft for the EU Regulation on AI and support the regulatory approach taken, yet point to some issues that need to clarified. Those include: the definition of AI needs further clarification; the scope of the Regulation should be clear for open source developers; harmonised standards should be a basis for demonstrating compliance and should be preferred over Codes of Conduct; common specifications should be pursued in exceptional cases only; and market surveillance authorities should be equipped with appropriate access to technology they need. We expand on these topics in the document attached.",PUBLISHED,micro,Paula,COM(2021)206,2021-08-06 14:51:56,withinfo,24212003,closed,2702114689,,
en,2665561,USA,business_association,Digital Therapeutics Alliance,Coder,Please see attached document. ,PUBLISHED,micro,Megan,COM(2021)206,2021-08-06 14:50:31,withinfo,24212003,closed,,,
en,2665559,DEU,company,Infineon Technologies AG,,"Overall, Infineon Technologies AG welcomes the fact that the Commission has presented a risk-based approach aiming to regulate AI systems and their effects on economy and society. In our contribution to the upcoming discussion, Infineon focuses primarily on Article 3 (1) and Annex I – i.e. the definition of AI systems. Infineon proposes that the following aspects should be taken into account in the subsequent debate between the Commission, the European Parliament and the Council:

>>> The definition of AI systems should be critically revised <<<
- The question of which definition is “the best"" will never be answered conclusively. However, Europe will have to answer the question of whether the definition is “meaningful” in terms of the regulatory intention in a compelling and as precise manner as possible. The definition of AI systems will fundamentally determine whether the intended effect will be achieved with the ""AI Act"".
- On the one hand, the definition in the current draft is clearly too broad, because it also covers conventional software.
- On the other hand, the definition is too narrow, because it completely ignores hardware. This contradicts the OECD definition and the understanding of the Enquete Commission ""Artificial Intelligence - Social Responsibility and Economic, Social and Ecological Potentials"" of the German Bundestag. In case of hardware, one could e.g. think in particular of edge AI devices, which have a decisive influence, for example, on the energy consumption of digitalisation and security aspects, and which are important for strong European key industries.
- However, the chosen approach to the definition of AI systems may not even be helpful at all, because so far it mainly focuses on the technology itself, and only to a lesser extent on its effects, as will be explained below.

>>> The AI Act should regulate more the WHAT / effects and less the HOW / technology <<<
- AI regulation should not focus on individual implementations and technologies, i.e. exactly HOW an individual function was implemented (specific technology, hardware or software etc.), but on the potentially critical properties that can be associated with the use of AI and machine learning. In particular, the possibly limited predictability and validation (full validation and qualification) of the system response and effects resulting from the difference of data sets used for learning compared to the data in use (e.g. unintended bias, incompleteness of the learning data related to the actual use etc.). Source: AI definition of OECD and the German Bundestag
- A technology-specific regulation like the “AI Act” would in any case effectively have an innovation-inhibiting character - e.g. if it is aimed at embedded systems in which, for example, clearly defined and validatable functions are implemented as a neural network for the purpose of higher accuracy, energy efficiency or speed.
- In the present draft of the ""AI Act"", the definition of AI systems deals exclusively with the HOW, i.e. with technology. Due to this fact alone, the definition - regardless of its concrete final wording! - will always lead to a) some AI systems being unnecessarily covered by this regulation whose effects are predictable and desired, or b) some AI systems not being covered by this regulation because they are not (or not yet) covered by the scope as described in Article 3 (1) and Annex I.
- Therefore, it is not sufficient to discuss and adapt the definition of AI systems in Article 3 (1) and Annex I only, but the foundation of the AI Act must substantially be more focused on the effects than on the technology itself. This different approach could, for example, already be anchored more strongly in the preamble to the AI Act.

To summarise:
The proposal of Infineon is to start a debate on the above aspects as broad and open as possible between all relevant stakeholders.",PUBLISHED,large,,COM(2021)206,2021-08-06 14:42:26,anonymous,24212003,closed,10751968675-85,,
en,2665558,GBR,academic_research_instittution,"Compliant and Accountable Systems group, University of Cambridge",Cobbe,"We are academics in the Compliant & Accountable Systems research group, University of Cambridge. We seek to highlight issues with the Regulation regarding AI services; where a company places on the market an AI system as-a-service (AIaaS). 

AIaaS systems are typically generic and broadly defined (e.g. ‘object recognition’). Customers use the provider’s API to implement the AIaaS system’s functionality in their own applications and processes. An AIaaS system often serves many customers, across many different contexts and purposes. AIaaS is often ‘turn-key’, with few checks on customers’ identities or context or purpose of use. 

To avoid the Regulation’s obligations for ‘high-risk’ AI systems, AIaaS providers will likely specify in their terms that their services should not be used as safety critical components or for purposes listed in Annex III. This raises two problems in particular:

(1) The nature of AIaaS systems brings serious, systemic risks to fundamental rights, freedoms, and interests and of harm and injury that are unaccounted for by the Regulation.

Even if not intended or used for ‘high risk’ purposes, substantial risks are enabled by AIaaS systems as components in customers’ applications or processes. The combination of generic, turn-key AIaaS systems with hardware or software components or other services can result in serious harms; including where the AIaaS system has biases, errors, or malfunctions, or has not been tested properly for a customer’s context or purpose of use. Such issues in an AIaaS system can propagate across the provider’s customer base – potentially affecting many applications and processes across many sectors. AIaaS systems may also interact with components that produce physical outputs and thus risk physical, real-world harm and injury (e.g. in the IoT). Yet, because AIaaS systems are generic and turn-key, AIaaS providers cannot foresee or mitigate many such risks. 

Importantly, providers of such non-‘high risk’ AIaaS systems would not need to institute risk management processes (Art 8), ensure data quality (Art 9), keep records of use (Art 12), or provide information (Art 13) that assists customers in assessing when it is appropriate (or not) to use a particular AIaaS system, what other hardware or software components it is appropriate to use in conjunction with, etc. Customers of AIaaS will thus have little idea whether is it safe or appropriate to use a given AIaaS system as a component in their application or process.
(2) Customers using AIaaS for unintended ‘high risk’ purposes will be the AIaaS system’s ‘provider’ (Art 28). These unintended purposes may be reasonably foreseeable by the AIaaS provider (e.g. those detailed in terms of service), even if they breach the providers’ terms. 

Yet, under the Regulation AIaaS providers would not need to consider the risks of these unintended yet foreseeable purposes, despite them being best-placed to handle them. Under the Regulation, these services would not be ‘high risk’, given the AIaaS provider’s intended purposes, and the AIaaS provider would not themselves be the ‘provider’ for the customer’s ‘high risk’ unintended purpose (Art 28). As such, the AIaaS provider would not need to mitigate any risks of unintended purposes, to keep any records of use, or to provide information about the AIaaS system that may assist the customer (as a ‘provider’) in themselves reducing risk, despite the AIaaS system’s potential to enable harm. This contrasts with the regime for AI systems intended for ‘high risk’ purposes, where providers must consider the risks of reasonably foreseeable misuse (e.g. Art 9). 

For these and other reasons, we urge more attention be paid to how the Regulation accounts for AI services – particularly given that AIaaS will likely become the main way AI is used. To assist, we attach our published research providing a legal and policy analysis of AIaaS. We would be pleased to discuss any related issues. 

Dr J Cobbe, Dr J Singh",PUBLISHED,micro,Jennifer,COM(2021)206,2021-08-06 14:32:08,withinfo,24212003,closed,,,
en,2665556,USA,company,Microsoft Corporation,Kutterer,"Microsoft applauds the Commission for its leadership in developing a regulatory framework for the responsible development and use of artificial intelligence technologies (“AI”). The Commission’s proposed Regulation for harmonized rules on AI (the “AI Act”), and the New Coordinated Plan on AI, are ambitious and important steps toward making trustworthy AI the norm in Europe and around the world. We share the Commission’s goal to ensure that the vast potential of AI can be realized by all in ways that are safe, respectful of fundamental rights, and align with European values. We are committed to supporting this effort as part of our ‘Tech Fit 4 Europe’ initiative to help Europe realize its AI and other tech-related aspirations, and we appreciate the opportunities made available by the Commission to participate in the preparatory work for the AI Act.

We support the AI Act’s vision and direction, in particular that it adopts a risk-based framework focused on a clear list of high-risk uses as well as certain prohibited uses, recognizes the benefits of transparency in promoting trustworthy AI, provides for self-assessment in appropriate cases, and incentivizes the broad adoption of responsible AI governance through codes of conduct. We urge policymakers to preserve these elements as the AI Act moves through the legislative process.

Microsoft also believes there are ways to strengthen the proposal. We offer some suggestions based on our experience of developing our own internal responsible AI program and the emerging trends that we see in the commercial and research landscape. These suggestions are informed by the many innovative ways in which our European partners and customers are deploying AI.

While we firmly agree that the AI Act should articulate what regulated actors should seek to achieve, we believe it will provide a more effective framework for responsible innovation – now and into the future – if it is less prescriptive on how they achieve them. A more principles-and results-based approach that is calibrated specifically for the AI ecosystem will help to drive clarity for regulated actors and spur innovation within necessary guardrails. 

Our detailed comments are attached.",PUBLISHED,large,Cornelia,COM(2021)206,2021-08-06 14:21:29,withinfo,24212003,closed,0801162959-21,,
en,2665554,IRL,company,Trilateral Research,Santiago,"Trilateral Research is a UK and IE-based ethical technology development and research company.  Our experienced interdisciplinary teams apply rigorous, cutting-edge research when developing and assessing new technologies to ensure they achieve sustainable innovation and measurable impact. We believe that regulation should be a part of the governance framework for AI, as it will help ensure AI systems are safer, more reliable, and better suited for inclusion as part of the fundamental infrastructure of society.  

Trilateral Research is aware of the potential harms that can come from unregulated use of AI and the pressures to rapidly make use of this new technology.  We are also deeply familiar with the philosophy behind the draft regulation and have already adopted practices in line with many elements of the proposed Regulation.  Additionally, Trilateral Research has an established history of research and work related to privacy and data protection under GDPR, and we understand that enforcement strategies, and support and guidance, particularly for SMEs, will be very important to the impact of the Act.

- We support establishing red lines for AI systems that pose unacceptable levels of risk. We caution against too narrowly defining those prohibited AI systems and recommend that the list of prohibited AI systems be non-exhaustive and a process established for other AI systems to be added in the future.  

- We support mandatory requirements that are clear, implementable and support the development of secure, trustworthy, and ethical AI for high- risk AI systems. However, mandatory requirements should not place unnecessary or onerous burdens on developers, especially SMEs, because these burdens could discourage proactive measures to support compliance.   

- We recommend that impact assessment becomes an explicit and mandatory requirement of the risk management system for high-risk AI systems.  Such assessment should include, at a minimum, assessment of bias, discrimination, invasion of privacy, misuse of personal data and damaging trust, and could be modeled on privacy and data protection impact assessments required under the GDPR.

- We welcome regulatory sandboxes and other support measures for SMEs and small-scale providers. As conditions are set out in implementing acts, we recommend that the selection criteria are strictly, carefully, and transparently defined to avoid the potential for misuse by participants to gain unfair competitive advantages both in regulatory advice and in being first to the market.

- We support the creation of an independent, centralised EU body to ensure cooperation, coordination and consistent application of EU law related to AI.  Like the European Data Protection Board, the independent, centralised body should be tasked with developing and promulgating guidance on legal concepts and regulatory issues associated with AI, including specific guidance for SMEs. 

- We recommend strengthening and expanding enforcement mechanisms to better protect fundamental rights and whistleblowers.  Stronger measures are needed to ensure that individuals and citizens can adequately and effectively raise concerns about harmful AI systems and be protected from retaliation if such concerns are voiced.  These mechanisms could complement other complaint and redress tools, including those under GDPR and within national human rights institutions. 

- We support the instrumental role that standards will play in regulatory compliance. It is important that any of the European Standard Organisations mirror the ongoing activities taking place in the international arena and strive to reduce duplication of work and effort. This would reduce the onerous burden for SMEs, who may have to navigate through all the forthcoming standards to determine which ones are functional. Furthermore, we recommend the use of standards to meet regulatory requirements as opposed to the use of common specifications. 
 ",PUBLISHED,medium,Nicole,COM(2021)206,2021-08-06 14:16:25,withinfo,24212003,closed,841780842634-63,,
en,2665553,FRA,business_association,Paris EUROPLACE,vigna,"Dear Sir/Madam,
Please find attached Paris EUROPLACE’s response to the European Commission’s feedback on Artificial intelligence – ethical and legal requirements.
Best regards,
Olivier Vigna
olivier.vigna@paris-europlace.com
ID NUMBER 523145616037-10
",PUBLISHED,small,olivier,COM(2021)206,2021-08-06 14:14:52,withinfo,24212003,closed,523145616037-10,,
pl,2665551,POL,other,Koalicja AI w Zdrowiu & Grupa Robocza ds. Sztucznej Inteligencji - sekcja ds. zdrowia,Lorent,"I submit the position towards the European Commission's proposal for the Artificial Intelligence Act on behalf of the Koalicja AI w Zdrowiu (the ‘AI in Health Coalition’) and Grupa ds. Sztucznej Inteligencji - sekcja ds. zdrowia (the ‘Artificial Intelligence Group - Health Section’).

The AI in Health Coalition includes technological, pharmaceutical and medical care companies with local and global reach. We share an interest in developing the potential of artificial intelligence. We use and conduct research and development works on technologies enabling the use of artificial intelligence in the field of, among others: medical care, including telemedicine and advanced diagnostics, management in health care, clinical trials, distribution of medicinal products. We create medicinal products and medical devices that use these technologies.

The Artificial Intelligence Group - Health Section is an expert group that was created at the Chancellery of the Prime Minister of Poland. The group brings together dozens of experts from the private and public sectors who deal with issues related to artificial intelligence in health care. The Group's goal is to work out legislative solutions enabling the effective and safe development and implementation of AI in health, as well as education and popularization of solutions based on AI.",PUBLISHED,medium,Rafał,COM(2021)206,2021-08-06 14:07:53,withinfo,24212003,closed,,,
en,2665550,DNK,ngo,European Edtech Alliance,Trier,"Dear Sir/Madam,

On behalf of the European Edtech Alliance please see the enclosed letter.

Best Regards, 
Esben Trier

CEO, EdTech Denmark
esben@edtechdenmark.dk
+45 4032 3010

",PUBLISHED,large,Esben,COM(2021)206,2021-08-06 14:07:51,withinfo,24212003,closed,890631109,,
en,2665548,GBR,company,Hogan Lovells International LLP,Whitehead,"Dear Sir/Madam,

Attached is a copy of our feedback to the Commission's consultation on the proposed AI regulation.

Yours faithfully, 

Hogan Lovells International LLP",PUBLISHED,large,Dan,COM(2021)206,2021-08-06 14:06:08,withinfo,24212003,closed,,,
en,2665547,BEL,business_association,Insurance Europe,HILLIARD,"Insurance Europe welcomes the overall objective of the Commission to create a proportionate and principles-based horizontal framework of requirements that AI systems must comply with in the EU, without unduly constraining or hindering technological development and innovation. Moreover, insurers welcome the focus on the development of mandatory requirements for high-risk AI systems that pose significant risks to the health and safety or fundamental rights of persons.

The introduction of harmonised rules on AI, however, requires a very clear and precise definition of an AI system. We understand that the Commission has based its definition on the OECD’s definition of an AI system, which it defines as “a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments”. While there is currently no universally agreed upon definition of an AI system, the OECD definition is an appropriate basis to use for any European approach, particularly given the inherently global nature of AI systems and the need to ensure consistency at the international level.

The definition of an AI system as currently proposed in Article 3 of the draft Regulation, however, significantly widens the OECD definition by also including software within its scope. This will result in the inclusion in its scope of systems, techniques and approaches that should not be considered as AI and will generally create confusion and a lack of legal certainty. For example, the use of statistical output from a linear regression model in the actuarial function would be covered by this proposed definition, as would statistical approaches such as exploratory data analysis that mostly involves using graphical techniques to analyse datasets, or task allocation systems that form part of the back-office functions of companies.

For this reason, not only should the general area of a system’s application be considered but also, on an individual level, its specific purpose. Furthermore, it should be stated clearly that the AI applications already referred to in Annex III also need to fulfil the conditions set out in Article 7(1) to be classified as high risk.

In the context of its Framework for Classifying AI Systems, the OECD notes that “certain systems that use compute technologies and analyse data are not AI systems. If the system does not fit the definition of an AI system used in this framework, it is not considered an AI system. For example, Microsoft Excel is a system for data storage and analysis. The software allows users to store, sort, and run basic analysis on inputted data. However, it is not an AI system.” This is also true of a wide variety of other software types that may potentially fall under the proposed definition.

The broad definition of an AI system that has been proposed in the draft Regulation should be narrowed to be fully aligned with the OECD and avoid running the risk of inconsistent and divergent classifications of AI systems. Insurance Europe also wishes to stress the importance of ensuring consistency not only with the OECD definition, but also with the definitions used in any existing or upcoming European texts that address AI, including in particular the forthcoming liability framework for AI.

Article 33(8) proposes a requirement for notified bodies to take out appropriate liability insurance for their conformity assessment activities. It is insurers’ understanding that entities acting as notified bodies under the proposed Regulation and in a corresponding capacity foreseen by other EU legislation, for instance the Medical Devices Regulation, will continue to be able to cover all of their conformity assessment activities under a single contract of liability insurance. Further, it is insurers’ understanding that such insurance will be written on the basis of standard market terms and conditions in accordance with applicable insurance contract law.",PUBLISHED,small,Arthur,COM(2021)206,2021-08-06 14:05:13,withinfo,24212003,closed,33213703459-54,,
en,2665546,NLD,ngo,Future of Life Institute (FLI),Brakel,"The Future of Life Institute (FLI) welcomes the opportunity to provide feedback on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act).

FLI recommends that a final Act i) accounts for the full and future risks of AI, ii) enhances protections of fundamental rights, and iii) boosts AI innovation in Europe.  

Please refer to the attachment for FLI's full submission.",PUBLISHED,small,Mark,COM(2021)206,2021-08-06 14:00:00,withinfo,24212003,closed,787064543128-10,,
en,2665543,DEU,company,Siemens Energy,Herges,Siemens Energy contribution in the attached file.,PUBLISHED,large,Benedikt,COM(2021)206,2021-08-06 13:36:57,withinfo,24212003,closed,,,
en,2665542,DEU,company,Onfido,Nikolaidis,On behalf of Onfido please find our feedback attached.,PUBLISHED,large,Alexandros,COM(2021)206,2021-08-06 13:29:24,withinfo,24212003,closed,,,
en,2665541,CHE,ngo,European Society for Medical Oncology (ESMO),Scholte,"The European Society for Medical Oncology (ESMO) welcomes the proposal for a Regulation laying down harmonised rules on Artificial Intelligence. While the intention of the Regulation is welcomed, ESMO is concerned regarding the potential impact of this Regulation on the usage of artificial intelligence technologies for the purposes of health research and healthcare practice. Currently in oncology, artificial intelligence is being used in the radiology setting (in terms of imaging) and we are in the nascent stages of AI applications in pathology, radiomics, genomics, proteomics and drug development, in addition to developing clinical decision-support tools that integrate clinical practice guidelines with new clinical data in the area of medical oncology, among others. It is thus crucial to ensure that if any regulations will be put in place to govern the safety of such systems they will aim to foster health research and optimal healthcare practice, rather than obstruct it.

Additionally, EU rules on AI should be developed to serve the needs of healthcare professionals and patients, enable advancements in the AI field, and they should be made applicable in a consistent manner in all EU countries. Thus, any measures supporting the trustable development, deployment and use of AI in healthcare should address the following issues:

1. Enabling regulatory environment for AI: EU rules and legislation governing the use of AI as well as AI-related measures should be tailored to the needs of healthcare professionals and patients, and they should be developed and implemented in a manner that does not stifle the use of AI in medical research and clinical practice. The current proposal for a Regulation laying down harmonised rules on AI follows a risk-based approach that sets stricter rules for AI systems that are considered as ‘high-risk’ under the Regulation. ESMO believes that, even though robust requirements are needed for systems that pose potential threats to the safety of EU citizens, the risk-based approach should not unintentionally prevent the use of AI systems that could advance medical research and improve the treatment and care provided to patients with cancer. Moreover, legislation on AI should not impose additional administrative burdens to medical researchers that use AI-driven systems as part of their research.

2. GDPR: ESMO would like to underline the need for harmonising the implementation of EU data protection rules in all EU countries. In order to seize the full benefits of applying AI systems in oncology, we believe that it is crucial to ensure that the GDPR is interpreted in a consistent manner in all EU Member States. ESMO’s work on the GDPR: https://www.esmo.org/policy/eu-general-data-protection-regulation

3. Impact on healthcare professionals and regulating the ‘Black Box’ doctrine: Applying AI systems in medical research and healthcare may require specific knowledge and competences. It is important that measures are taken to ensure that healthcare professionals are involved in the development of these tools from their inception and they gain the skills needed to use AI systems, including by integrating this in educational and university curricula. Moreover, while respecting intellectual property rights, the principles of AI algorithms and logic should be understandable and open for review by experts and healthcare professionals.

4. Integration of future-proof scenarios: Ensure that new regulations governing the use of AI are created in a forward-looking manner that does not impede advances in scientific research, and allows for the use of health data for AI-applications supporting the medical field, e.g clinical decision-support systems et al., without making it cumbersome for healthcare professionals.

ESMO welcomes the draft Artificial Intelligence Act, and would be glad to work with the European Commission to ensure that we create an AI Regulation that fosters research, especially in the healthcare setting.",PUBLISHED,medium,Marijn,COM(2021)206,2021-08-06 13:27:19,withinfo,24212003,closed,947929324977-41,,
en,2665540,DEU,business_association,Federation of German Industries / Bundesverband der Deutschen Industrie (BDI),Klein,"Artificial Intelligence (AI) is one of the most important key technologies in industry. Therefore, unbureaucratic and innovation-friendly framework conditions for the use of AI are a central prerequisite for securing the innovation capacity and competitiveness of the German and European industry. 

However, the rules on AI proposed by the European Commission need to be amended significantly. Against this background, the Federation of German Industries (BDI) is preparing a detailed statement on the regulation proposal, which will be available at https://english.bdi.eu/publication/news/proposal-for-a-regulation-on-artificial-intelligence/ after the consultation of BDI’s committees has been finished. 
",PUBLISHED,medium,Oliver,COM(2021)206,2021-08-06 13:26:41,withinfo,24212003,closed,1771817758-48,,
en,2665539,BEL,business_association,Japan Business Council in Europe (JBCE),Kurihara,"JBCE welcomes the European Commission’s attempt to set a horizontal Regulation for trustworthy AI with the AI Act. 

In this context, JBCE asks for a clear definition of AI to ensure an effective and harmonized interpretation and enforcement of the Act. We encourage continued work with International Organisations such as the OECD to provide legal certainty and a shared understanding worldwide on what constitutes an AI system. 

The same applies to the definition of prohibited and high-risk AI systems. We ask for more clarity on the classification of AI systems and related wording, such as ‘intended to be used’ for a specific purpose. Similarly, while we welcome that the Commission correctly identified much of the legislation in Section B of Annex II that addresses the most common safety concerns, we would like further clarity on the scope for high-risk AI listed in Section A of Annex II and Annex III - notably for “safety components” (beyond the definition in art.3(14)). We strongly encourage the European Commission to consult industry and deliver guidelines to facilitate harmonized interpretation.

JBCE appreciates the effort to provide a list of requirements for high-risk AI systems in Chapter 2. However, we believe it is crucial to design tools for companies to provide the proper documentation to meet requirements. We also believe that the definition of responsibilities between provider and user could be improved in art. 29. In fact, it is not always clear who the provider and the user are. We agree that these obligations should be imposed on providers, but we would argue that the user of the high-risk AI system should be responsible if they misuse, negligently or intentionally, the AI system. Further, if AI-origin risks are sufficiently eliminated or reduced with built-in ‘fail safe’ measures in high-risk AI systems, we believe the mandatory requirements on them should be exempted.

We all agree that data is key for the correct application of AI systems.
Art. 10 should be better designed in order to make it possible for companies to test their AI systems without too many ex-ante obligations and restrictions in terms of “free of error” data. The focus should not be on imposing these principles in the testing or training phases, but rather on the final AI system, in order to support innovation. 

We are also concerned by the adoption of Sandboxes as defined in the Act.  We would rather encourage the adoption of ‘auditable data’ processes. In addition, to get high quality data, it will be essential that the EU sponsors data programs to encourage citizens to share their data altruistically in a secure, effective, and eventually rewarded way.

We would like the European Commission to: 1) deliver the necessary standards on time; and 2) work more intensively on standards with International Organisations, to avoid ‘European-only’ standards which could pose severe challenges to innovation and international cooperation. In addition, when defining standards and common specification to ensure the appropriate level of human oversight, we encourage practical consideration of the pros and cons of human oversight and machine control respectively - given that, for example, AI-controlled machines register lower accident rates than human oversight.

We believe that the AI Board should also include experts from industry amongst its members. This would be particularly important in terms of ensuring that the Board can effectively help the Commission and national supervisory authorities to provide guidance on emerging technology issues, as provided in art.57.

Art.64(2) stipulates that market surveillance authorities can access to the source code of the AI system. However, we suggest that this provision should be deleted, as it would endanger the confidentiality of trade secrets, and contradict existing trade agreements (e.g. the EU-Japan EPA) which ban disclosure requests for source code.
",PUBLISHED,micro,Takenobu,COM(2021)206,2021-08-06 13:26:27,withinfo,24212003,closed,68368571120-55,,
en,2665537,FRA,trade_union,Federation of Craft Businesses in the automotive sector and in mobility services (FNA)  ,CASTELL,"Federation of Craft Businesses in the automotive sector and in mobility services (FNA) would like to thank the authors of the Proposal of European Regulation laying down harmonized rules on Artificial Intelligence (AI) to inform citizens and stakeholders about the Commission's work in order to allow them to provide feedback on the intended initiative and to participate effectively in future consultation activities. FNA representatives also express their thanks for being given the opportunity of once again making submissions on the Consultation. They make the following comments.

Summary
Undue hardship on advanced technologies

New European Regulation laying down rules on AI should be in line with technical progress on the Automotive aftermarket.
FNA representatives welcome option 3 in particular stating mandatory requirements, Articles 64, 71, 84 and Annexes, also Commission Staff Working Document on the Impact Assessment of April 21,2021, in particular Problem 2 - Use of AI poses an increased risk of citizens' fundamental rights and Union values violations as use of AI may, indeed, lead to discriminatory outcomes keeping out independent repairers of the maintenance and repair services.
Therefore FNA representatives call on the European Commission to make effective the following fundamental rights
 -Unrestricted access to vehicle repair and maintenance data  
 -Competition in the market for Automotive data services and AI information
 -Actual consumer’s choice being analysed regularly to make it effective on the Automotive services
 -More financial and human resources for national authorities

Detailed comments are attached
",PUBLISHED,small,Brigitte,COM(2021)206,2021-08-06 13:19:08,withinfo,24212003,closed,705440625408-01,,
en,2665536,BEL,business_association,"ITI, Information Technology Industry Council",Leto Barone,Please refer to the attached paper for ITI's detailed feedback to the proposal.,PUBLISHED,medium,Marco,COM(2021)206,2021-08-06 13:13:47,withinfo,24212003,closed,061601915428-87,,
en,2665535,BEL,business_association,Association of Financial Markets in Europe,Gbadebo,Please see attached the feedback from the Association for Financial Markets in Europe (AFME).,PUBLISHED,medium,Tola,COM(2021)206,2021-08-06 13:06:26,withinfo,24212003,closed,,,
en,2665534,BEL,other,radiomics,Lauwers,"As a company that focuses on AI powered healthcare, working in research and commercial projects that involve partners from all over Europe, we applaud the ambitious initiative of creating a EU-wide regulatory framework. After consultation of the feedback given by different actors in this field, we do not see anything that we could add. We would like to however point out the importance of closely involving companies 'on the ground' in the development of this framework, to assure its feasibility, and will be happy to provide our input in such consultations. ",PUBLISHED,small,Steven,COM(2021)206,2021-08-06 13:03:17,withinfo,24212003,closed,,,
en,2665533,BEL,business_association,BSA | The Software Alliance,Quattrocchi,Please find attached our position paper commenting on the AI Act Regulation proposal,PUBLISHED,medium,Matteo,COM(2021)206,2021-08-06 12:40:49,withinfo,24212003,closed,75039383277-48,,
en,2665532,BEL,ngo,MedTech Europe,Kalina,"MedTech Europe, the European trade association representing the medical technology industry including diagnostics, medical devices and digital health, would like to provide its response to the European Commission’s adoption consultation on the proposed Artificial Intelligence Act (AIA). Artificial Intelligence (AI) technology is increasingly used in healthcare and in recent years has been greatly enhancing the workflows and decision-making processes of healthcare providers.

The medical technology industry would like to stress the importance of a robust regulatory framework, which provides legal coherence, certainty, and clarity to all actors. In particular, interpretation issues of the new rules for AI that comprises, or is incorporated in, a medical technology, should be addressed. We call for particular attention to be paid to misalignment between provisions in the AIA and the Medical Device Regulation (MDR) and In-Vitro Diagnostics Regulation (IVDR) as well as the General Data Protection Regulation (GDPR). Addressing this misalignment is essential to ensure the legal coherence, certainty and clarity needed to foster innovation, citizen access to quality care and competitiveness of the industry.

DEFINITION/TECHNICAL SCOPE
MedTech Europe would like to point out that the proposed broad definition of AI and risk classification will result in any medical device software (placed on the market or put into service as a stand-alone product or component of hardware medical device) falling in the scope of the AI Act and being considered a high-risk AI system, since most medical device software needs a conformity assessment by a Notified Body.

MISALIGNMENT BETWEEN AIA AND MDR/IVDR 
Duplication and potential conflicts arising from misalignment between the AIA and existing obligations under MDR/IVDR must be avoided in order to ensure legal coherence, certainty and clarity. The sectoral regulations MDR/IVDR lay down some of the most stringent rules in the world on the safety and performance of medical technologies, including those medical technologies that comprise, or incorporate AI. These include, for instance, dedicated rules on risk management, quality management, technical documentation, and conformity assessment with Notified Bodies. Obligations in the AIA are thematically similar to the requirements in MDR/IVDR but differ in terms of details, which may lead to complex interpretation issues. Although we acknowledge that duplication is not the Commission’s intended vision, there are concerns that the AIA would in effect create the need for manufacturers to undertake duplicative certification / conformity assessment, via two Notified Bodies, and maintain two sets of technical documentation, should misalignments between AIA and MDR/IVDR not be resolved. Duplication of this kind would lead to unnecessary overlaps in the regulatory approval of AI as/in medical technology, which could have a negative effect on the timely access of citizens and patients to highly innovative and fairly priced AI medical technology in the EU.

MISALIGNMENT BETWEEN AIA AND GDPR
The AIA appears to provide a legal basis for processing certain categories of personal data. For instance, Article 10(5) states that providers of high-risk AI systems “may process special categories of personal data referred to in Article 9(1)” of the GDPR, where doing so is “strictly necessary for the purposes of ensuring bias monitoring, detection, and correction,” subject to additional safeguards set out in that paragraph. While MedTech Europe supports this positive development the AIA is nevertheless not sufficiently providing legal ground for processing personal data in general under the GDPR (as stated in Recital 41). As such, providers of medical technology are likely to face numerous challenges in ensuring that the steps they take to comply with the AIA do not conflict with their obligations under the GDPR. ",PUBLISHED,small,Judith,COM(2021)206,2021-08-06 12:37:56,withinfo,24212003,closed,,,
en,2665530,BEL,ngo,European Cancer Organisation,de Martini,"Because AI systems used in healthcare must be trustworthy and because lives and patients’ outcomes depend on it, the European Cancer Organisation recognises the necessity of a strong pan European framework to ensure the trustworthiness of AI in healthcare. The European Cancer Organisation therefore welcomes the aim of the European Commission to address a number of legal and ethical issues, raised by AI, to foster the development and uptake of AI.  

- As the applications of AI systems in healthcare are still at a comparatively early stage, with many innovations still occurring, the European Cancer Organisation agrees with the European Commission’s intended approach of maintaining evolving definitions in the field of AI regulation. 

- Fundamental to the success of AI, and its regulation, including ensuring its safety, reliability and trustworthiness are the crucial requirements of data quality, transparency and human oversight. This should be embedded in the EU’s approach to AI regulation. 
 
- The European Cancer Organisation welcomes efforts made to ensure that regulation is proportionate to risk, including for AI. Most applications of AI in clinical settings will be classified as high risk under this new regulation, meaning that a conformity assessment process will be required to permit the use of the AI function. It is of great importance that the conformity assessment process includes the possibility for healthcare professional views and perspectives to be taken into consideration during this decision-making. Healthcare professionals are not only a key end user, but also have high consciousness of the practical safety risks and ethical considerations.
 
- Specific consideration is required on the integration of this new regulation with existing EU regulations in the field of health. An example in this regard is the Medical Devices Regulation which already regulates “AI with an intended medical purpose”. As the healthcare environment is already highly regulated, it is important that the AI Regulation does not overlap with other conformity assessment schemes and surveillance systems. Conflicting regulations should be avoided not to overcomplicate decisions in clinical settings.
",PUBLISHED,small,Amélie,COM(2021)206,2021-08-06 12:22:21,withinfo,24212003,closed,51022176260-12,,
en,2665528,BEL,ngo,European Patients' Forum (EPF),Calabro,"EPF welcomes the EC’s endeavours to develop a European framework for artificial intelligence (AI) based on excellence, trust, human rights, and fundamental values. In healthcare, it is essential to adopt an approach aimed at ensuring the creation of an ethical, transparent, and trustworthy AI within an environment that fosters innovation bringing concrete added value to patients. EPF calls for particular attention on ensuring that AI in healthcare enhances society and is an enabler of – and not a threat to – patients’ rights and wellbeing, guaranteeing that the value of real human contact is not minimised or entirely replaced by technological alternatives.

AI has the potential to transform care delivery methods and can provide great benefits at several levels of the healthcare value chain. However, there may also be unrealistic expectations. AI has risks, limitations and concerns including ethical, technical, and legal, which are often closely connected and should be considered when adopting a new framework on AI.

There are risks of limiting human autonomy if AI were to make a calculation on risk or restrict a patient’s right to free, fully informed choice of treatment. Maintaining human oversight of AI-based decisions and the decisions flowing from them is thus particularly important in healthcare. AI must be seen as a support tool to improve care delivered by healthcare professionals (HCPs), not as a replacement. 

AI also depends on the availability of very large amounts of data. If the data are not enough, not good quality, this limits the potential of AI to be useful. Furthermore, biases in data also introduce issues in terms of the potential for AI-enabled decisions themselves to be biased or discriminatory.

AI also has the potential to make wrong decisions, lead to overdiagnosis or overtreatment: reliability and safety are particularly critical in healthcare, where errors can have serious consequences. The EU framework on AI should take this into full consideration: it should clarify accountabilities and liabilities ensuring ample protection and safety. The exercise of rights should be made simple and not overly burdensome. In terms of risks related to AI, EPF calls for particular attention on the definition of high-risk AI in healthcare and a dedicated discussion on this topic inclusive of the views of patients.

Furthermore, lack of skills and digital health literacy, for both patients and HCPs can also limit the potential of AI in health and create potential dangers. Empowering people by enhancing skills and literacy is a precondition to make the most out of AI-based innovation. Furthermore, patients must be fully informed about the functionality, consequences, and possible consequences of AI incorporation in their healthcare pathways.

The points mentioned above should also apply beyond clinical practice, when AI is used to inform broader delivery of services, public health interventions, and policy making in the field of healthcare.

Considering this context and given the ongoing development of the EHDS, which is also covering AI in healthcare, it will be essential to ensure effective alignment and synergies between both these new frameworks and existing Regulations, to avoid unnecessary confusion, complexity and duplication that could potentially impede rather than facilitate progress and positive application of AI for the benefits for patients. Finally, the EU framework on AI should be ‘future-proof’ and able to adapt rapidly in response to technological evolution.

EPF’s views on AI, detailing the points shared above, can be found in our AI White Paper response (https://www.eu-patient.eu/globalassets/documents/1.-ai-white-paper_consultation-response_epf_statement-final.pdf) and in our response to the 2021 EHDS Consultation, where we specifically addressed AI (https://www.eu-patient.eu/globalassets/0.-ehds_epf-accompanying-paper---final_06-aug.pdf). 
",PUBLISHED,small,Michele,COM(2021)206,2021-08-06 12:13:10,withinfo,24212003,closed,61911227368-75,,
en,2665527,POL,ngo,Moje Państwo Foundation,Siwanowicz-Suska,"Please find attached the feedback of Moje Państwo Foundation to the European Commission’s proposed Artificial Intelligence Act.

",PUBLISHED,micro,Magdalena,COM(2021)206,2021-08-06 11:55:40,withinfo,24212003,closed,020222326905-75,,
en,2665526,GBR,company,Sky,MITTESTAINER,"We are analysing the proposal and will share a more detailed position at a later stage; however, we welcome this opportunity to share our initial reaction to the text. The proposed EU-wide, harmonised, value-based, legislative framework represents a positive step for the future of AI regulation. This framework should avoid a patchwork of conflicting rules developing across Europe and should stimulate the creation of common standards to provide businesses with clarity to invest in innovation. Likewise, the risk-based approach set out represents a proportionate approach to regulatory requirements. Yet, more precise definitions and obligations, better tailored to the nature of different types of AI and different industries, would provide enhanced safety measures whilst still encouraging growth and innovation. Further details can be found in the attached document.",PUBLISHED,large,Alberto,COM(2021)206,2021-08-06 11:51:42,withinfo,24212003,closed,62536168216-12,,
en,2665525,BEL,company,Mastercard,CESAR,"Mastercard welcomes the opportunity to provide feedback to the European Commission on the Proposal for a Regulation laying down harmonised rules on artificial intelligence.  

This consultation comes at a difficult moment for many individuals, communities and businesses. Nearly a year and a half into the COVID-19 pandemic, despite breakthrough vaccines, we are still fighting the tragic health implications of the virus and grappling with the new reality of physically distanced and increasingly digital lives. 

At a moment of increased vulnerability across the economy, AI-based tools, built on a robust governance framework, have helped Mastercard facilitate access to the digital economy, thus increasing trust in our payment network and solutions. 

Mastercard understands the importance of trust and decency. Clearly the events of the past 18 months have only underscored the need for these values in our lives and business relationships. Mastercard therefore fully supports the creation of a proportionate regulatory framework that will facilitate the uptake of trustworthy AI in the European Union and wishes to respectfully submit its comments on the following main topics: 
1.	Definitions and scope 
2.	Remote biometric identification  
3.	Requirements for high-risk AI systems 
4.	Enforcement and competent authorities

Please find attached our detailed position on those main topics.",PUBLISHED,large,Jasmien,COM(2021)206,2021-08-06 11:50:28,withinfo,24212003,closed,58204758673-16,,
fr,2665524,FRA,other,Fondation pour le droit continental,GROS,"Avec la proposition de règlement sur l’intelligence artificielle (IA) présentée le 21 avril 2021, la Commission européenne réaffirme l’importance stratégique de l’IA pour l'Europe et la nécessité d’encadrer son usage dans les différents secteurs d’application.  Nous tenons à saluer cette initiative unique dans le monde, qui place l’intelligence artificielle au centre des réflexions juridiques, politiques et sociétales, desquelles elle est désormais indissociable. En investissant le terrain règlementaire, l’Union Européenne entend se positionner en chef de file, comme elle l’a déjà fait avec le RGPD et c’est une excellente nouvelle.  

Comme avec le RGPD, l’objectif est d’instaurer un climat de confiance tout en permettant l’innovation. L’approche par les risques est équilibrée (I) pour permettre un juste déploiement des systèmes d’IA en Europe. Néanmoins, en raison de ses enjeux spécifiques et délicats, nous appelons à traiter le secteur de la Justice comme un secteur hautement sensible pour nos concitoyens (II). Nous regrettons que le texte soit trop restrictif et ne comprenne pas suffisamment les enjeux de la justice prédictive avec les garde-fous nécessaires au bon respect des droits fondamentaux des individus.

(Suite dans le document joint ""Note Règlement IA et Justice en Europe"" ",PUBLISHED,medium,Anne-Charlotte,COM(2021)206,2021-08-06 11:49:51,withinfo,24212003,closed,621032542798-37,,
en,2665521,BEL,ngo,Center for Data Innovation,Mueller,The Center for Data Innovation is pleased to respond to the European Commission’s request for feedback on the Artificial Intelligence Act. Please see the enclosed document for our comments. ,PUBLISHED,micro,Benjamin,COM(2021)206,2021-08-06 11:37:34,withinfo,24212003,closed,367682319221-26,,
en,2665520,ESP,company,Equifax,Holland,Please find Equifax's feedback in the attached file. ,PUBLISHED,large,Stuart,COM(2021)206,2021-08-06 11:32:47,withinfo,24212003,closed,232484027628-74,,
en,2665519,BEL,ngo,Standing Committee of European Doctors (CPME),SEEBOHM,"Dear Madam/Sir,

Please find attached in pdf the CPME Feedback on Commission Proposal for a Regulation on Artificial Intelligence.
Kind regards,

Annabel Seebohm

Annabel Seebohm, LL.M. (Auckland)
Secretary General
 
Comité Permanent des Médecins Européens
Standing Committee of European Doctors
15 Rue Guimard - 1040 Brussels
tel:    +32 2 732 72 02 
mob: +32 477 8132 44
annabel.seebohm@cpme.eu
www.cpme.eu

The Standing Committee of European Doctors (CPME) represents national medical associations across Europe. We are committed to contributing the medical profession’s point of view to EU institutions and European policy-making through pro-active cooperation on a wide range of health and healthcare related issues.




",PUBLISHED,micro,Annabel,COM(2021)206,2021-08-06 11:26:50,withinfo,24212003,closed, 9276943405-41,,
en,2665518,NOR,trade_union,Negotia,Bang,"Negotia would like to commend the European Commission for proposing legislation that represents an important step towards protecting fundamental rights when it comes to the rapidly evolving technology of artificial intelligence.  

It is our belief that new technology creates jobs and increased value for companies. As a trade union, we are nevertheless concerned that the proposed legislation does not provide employees with sufficient protection against the negative consequences of using artificial intelligence in the workplace. 

We fully support categorizing the use of artificial intelligence in the workplace as high-risk, based on the possible threat to workers' fundamental rights. The European Commission's proposal mentions, in particular, the use of artificial intelligence in recruitment processes, in decisions regarding promotions or termination of employment, in the distribution of work tasks or the monitoring and evaluation of employees. Negotia agrees that all of these areas have a major impact on the career opportunities and lives of individual workers. The use of artificial intelligence in these areas can easily be perceived as intrusive. Flaws in the technology used can, in worst case scenarios, lead to systematic discrimination. The introduction of artificial intelligence in these areas must therefore be treated with great caution.

Any introduction of artificial intelligence into an organization will affect the employees. It is our belief that the Commission's proposed legislation on the use of artificial intelligence in the workplace, does not provide adequate protection in the face of these risks. Negotia therefore believes that all artificial intelligence used in the workplace should be defined as high-risk.

Furthermore, the proposed regulation does not take into consideration the unique power dynamics in the workplace. The proposed system suggests that the users of artificial intelligence systems are mostly consumers. In the workplace, however, adopting new technology is a leadership decision. Even though it is the employees who risk the greatest negative consequences when introducing such systems, they may not be involved in the decision-making at all. If a new system is adopted, it is unlikely that any employee can choose to opt-out. Negotia believes this should be clearly reflected in the regulation of artificial intelligence in the workplace.

The proposed regulation also does not directly address the ethical implications of using artificial intelligence in the workplace. For instance: If the employer's purpose in using artificial intelligence is to monitor the employees, this may have a negative effect on them, even if the technological solution performs as intended. The undesirable effects of this kind of intrusive technology must be taken into account in the final regulation.

The proposed regulation does not address the role of shop stewards in ensuring that worker's rights are safeguarded when artificial intelligence is introduced in the workplace. The best solutions are found through local dialogue between management and worker's representatives. To ensure co-determination from employees, Negotia believes that the social dialogue should have a clearly defined role in the regulation of artificial intelligence in the workplace. We also believe that the conformity assessment of artificial intelligence solutions that are intended to be used in the workplace should be performed by an independent third party.

In conclusion, Negotia believes the European Commission's proposal to regulate artificial intelligence, is a great step in the right direction, but that it has clear weaknesses when it comes to regulating technology in the workplace. Further measures should be put in place to ensure workers' rights, in the face of technological developments.

We are grateful for the opportunity to provide feedback on the proposal,and look forward to following the legislative process going forward.",PUBLISHED,large,Heidi,COM(2021)206,2021-08-06 11:17:53,withinfo,24212003,closed,,,
en,2665517,DEU,company,ZVEI e.V.,WIRTHS,"ZVEI welcomes the ambitious goal of the EU-Commission to promote and facilitate the uptake of Trustworthy AI in Europe. The EU-Commission puts forward a proposal for a harmonised European approach addressing potential risks and uses of AI applications. 

We plead ​to also focus on the immense opportunities related to the uptake of AI. Every market access regulation such as the proposed AI Act, carries the risk of hindering innovation as it sets down requirements for the placing on the market of products including certain technologies. Therefore, each provision in the proposal needs to be carefully examined and justified in particular for its potential impact on innovation and technological development. AI research and development should be strengthened on a European level, experimental space (i.e. regulatory sandboxes) should be provided for, as well as fostering both the continuous evolution of the technology and the understanding of the associated risks. Next to promoting a trustworthy, human-centric approach that aims at tackling risks linked to the usage of AI-technology, a future-proof AI regulation should also facilitate innovation and promote the general goal of boosting AI development and deployment across businesses in Europe. 

Industrial artificial intelligence is already a key driver for securing the future of European and German industry in a competitive global environment. AI is used in many applications and domains, as well as in public and private sectors. The deployment of industrial AI can optimise production processes and enable new levels of efficiency. In this way, AI can also make a significant contribution to resource efficiency and sustainability. As an enabling industry at the interface between IT and production world, the electrical industry has a key role to play in facilitating and implementing artificial intelligence in different application contexts. 

ZVEI welcomes the risk-based approach whereby the applicability of stricter requirements for placing on the market and putting into service of AI-based products is limited only to high-risk AI systems. However, the legislator should carefully assess regulatory gaps, inconsistencies and redundancies when addressing AI as a technology in its entirety. Particularly, in the case of AI embedded in products, potential risks regarding product safety and security are already covered by existing harmonisation legislation,or by functional constraints, where products can only act within their intended purpose. 

ZVEI in principle supports the NLF (New Legislative Framework) approach taken by the EU-Commission referring to established methods of conformity assessment in existing Union harmonisation legislation (Annex II). However, the Commission proposal contains a large number of additional requirements and obligations as compared to the NLF and the EU legislation based on it. Specific justification therefore is necessary for those additional elements, since established Union harmonisation legislation has proved to be effective in addressing public interest protection issues. This horizontal AI regulation should be discussed together with the relevant harmonised and sector-specific legislation and hence, needs to be in line with, and should ideally refer to, the conformity assessment procedures such as defined by the NLF (cf. Decision 768/2008/EC). In this respect the NLF provides the important basis to reach the necessary consistency over the relevant pieces of legislation.

The legislator should carefully review and narrow down the scope of the proposed AI regulation, as the currently proposed AI definition is much too broad. It would not only cover AI products, but also applications that use pure statistical and knowledge-based approaches used for conventional data analysis and software that has been well-established for a long time. 

Our detailed comments can be found in the attached paper. ",PUBLISHED,medium,Franziska,COM(2021)206,2021-08-06 11:14:54,withinfo,24212003,closed,94770746469-09,,
en,2665516,ITA,company,Enel SpA,Díaz,"Dear Members of DG CNECT A.2,

Enel SpA, a multinational company in the energy sector, highly appreciates the EC proposal for a regulation aimed to create the conditions for an ecosystem of trust for Artificial Intelligence products and services, on the EU market. 

To provide feedback  on the EC proposal, Enel wrote its views in the enclosed document for your kind consideration.

Please, let us know if you require additional information or further clarification regarding Enel´s response.

With best regards,

Almudena Diaz on behalf of Enel SpA
",PUBLISHED,large,Almudena,COM(2021)206,2021-08-06 11:08:58,withinfo,24212003,closed,,,
en,2665515,DEU,company,Volkswagen AG,Brecht,"Volkswagen AG very much welcomes the AI Act and its risk-based approach, and understands and supports the unique opportunity that the European Commission has in advancing the safe and trustworthy use of AI.  We believe that this approach will give Europe a competitive advantage on the long-term development and deployment of AI systems.
Besides our attached feedback we clearly indicate our support of the feedback to the AI Act given by the European Automobile Manufacturers Association (ACEA).",PUBLISHED,large,Benedikt,COM(2021)206,2021-08-06 11:08:45,withinfo,24212003,closed,6504541970-40,,
en,2665514,LTU,ngo,Artificial Intelligence Association of Lithuania (AIAL),Petkevičius,The feedback of the Artificial intelligence association of Lithuania on AIA provided is attached file.,PUBLISHED,medium,Linas,COM(2021)206,2021-08-06 11:07:12,withinfo,24212003,closed,677615842341-26,,
en,2665510,BEL,ngo,Council of European Dentists (CED),Heilpern,"The Council of European Dentists (CED) welcomes the European Commission’s Proposal for a Regulation on AI to establish a legal framework on AI Systems and believes it to be a step in the right direction in the EU’s effort to categorize AI applications according to risk and acceptability.
As in other sectors, AI is also increasingly applied in healthcare, and dentistry is no exception. The human factor is particularly relevant for professions such as dentists that are relying on patient trust. Algorithms can help dentists in making diagnoses, guided by the principles of patient-centered health care, but the CED believes that they should never replace them in this task. In healthcare, trust and empathy are of crucial importance and cannot be replaced. Thus, suitable regulatory frameworks are required to assure their safe usage and to define the boundaries of AI applications in healthcare.

Apart from diagnosis, treatment and patient management, the CED recognises the wider potential for the application of AI in dentistry. An example of recent innovation could be AI-driven orthodontics where AI could offer automation of the procedure i.e., going from the 3D scan to the appliance without any user intervention. This would mean that AI would take over several steps that are now done by humans and allow for fully customised and personalised appliances. AI application in implantology and endodontics creates even more opportunities for more precise treatment and preventive actions. Robotic dental procedures would require AI for vision, instrument manipulation and real-time decision making.

We would like to share the following key points to ensure that the application of AI in healthcare benefits patients:
-	The draft Regulation needs to establish clarity as to the conditions under which AI processes can be used, especially in view of the MDR Regulation (EU) 2017/745. There is concern that the existence of different risk assessments under the AI and MDR might create confusion. We believe that this is needed for legal certainty and consistency reasons and to avoid interpretation issues. 
-	The draft Regulation would need to clarify how human oversight and the provision of information to users is defined and applicable, especially, examples from medical device software would be appreciated. Algorithmic transparency is also crucial to ensure patient rights to information and explanation of how a decision might have been reached. Clear standards and legally binding assessment criteria to ensure transparency of AI systems in healthcare are needed.
-	The draft Regulation should ensure full respect of EU data protection rules (notably the GDPR), while observing the balance between the interests of advancements in medical research and citizen protection.
-	The list of high-risk AI systems detailed in Annex III of the draft Regulation currently lacks some use cases which involve significant risks, such as the use of AI for assessing medical treatments or for health research purposes. The Annex will need to be regularly updated to ensure that its scope is appropriate.
-	To digitally revolutionize the healthcare system, an educated, well-trained workforce is paramount. The EU and Member States should put in place mechanisms to ensure professional and educational assistance to both patients and the healthcare professionals to better understand and assess AI decision-making. Priority should be given to the implementation of digital skills education into the healthcare studies curriculum.  Professionals should have the opportunity to undertake courses on algorithm functioning, as well as receiving adequate training on AI tools management. The workforce needs to be appropriately trained and financially supported during the introduction and application of AI systems in healthcare settings. Understanding AI processes and their application is a first step in supporting confidence in AI technologies among healthcare professionals, including dentists.",PUBLISHED,micro,Caroline,COM(2021)206,2021-08-06 10:38:01,withinfo,24212003,closed,4885579968-84,,
en,2665508,DEU,business_association,German Banking Industry Committee,Lüke,"
Dear Sir or Madam,

Please find enclosed the statement of the German Banking Industry Committee on the European Commission's proposed regulation on the regulation of artificial intelligence (AI act).

Sincerely,
Dr. Wiebke Lüke
",PUBLISHED,large,Wiebke,COM(2021)206,2021-08-06 10:36:00,withinfo,24212003,closed,52646912360-95,,
en,2665507,GBR,academic_research_instittution,Global Digital Foundation,Higgins,"We propose the addition of an article mandating the requirement contained in Whereas clause 60  (repeated below) for third parties to cooperate with providers and users. This would provide legal certainty for all actors in the value chain. It would ease the compliance burden for providers and AI value chain actors. It would increase public trust in AI.
 
“In the light of the complexity of the artificial intelligence value chain, relevant third parties, notably the ones involved in the sale and the supply of software, software tools and components, pre-trained models and data, or providers of network services, should cooperate, as appropriate, with providers and users to enable their compliance with the obligations under this Regulation and with competent authorities established under this Regulation.”
 
The additional article should place an obligation on these relevant third parties to provide such technical information as is required by providers and users to meet their obligations under this regulation.",PUBLISHED,micro,John,COM(2021)206,2021-08-06 10:26:31,withinfo,24212003,closed,686462040358-12,,
en,2665505,USA,company,Twilio,Michalowitz,"Twilio welcomes the opportunity to engage further with the European Commission on its proposal regarding appropriate rules for the use of Artificial Intelligence (AI). Twilio has already submitted a response to the European Commission’s White Paper on Artificial Intelligence – A European Approach and to the Inception Impact Assessment on developing requirements for Artificial Intelligence.

Twilio appreciates the risk-based approach the Commission is taking to address AI, and the Commission’s efforts to ensure that the huge potential that this technology has is fulfilled. Although still in its early stages, AI is already enabling businesses to deliver high quality engagement with an ever larger number of customers. Today, AI supports businesses in interacting with customers, routing calls and sending emails among many other possible use cases which continue to emerge.  However, Twilio is aware that the benefits of this developing technology come with certain risks that need to be managed. 

Twilio acknowledges the Commission’s efforts in developing a balanced proposal. Further engagement with industry stakeholders as part of developing its legislative approach and ensuring that any legislation is as  ‘future-proof’ as possible is important. While Twilio supports the Commission’s overall approach, a number of aspects of the proposal should be reviewed in order to ensure its overall objectives can be achieved.

In particular, Twilio believes modifications in the following areas would ensure that the legislation is more proportionate and effective: 
- Clarifying the responsibilities of general purpose software providers
- Defining AI systems appropriately to ensure legal certainty and targeted regulation
- Narrowing the list of AI systems determined to be high-risk
- Turning assessment procedures into guidelines based on standards, not static check-the-box exercises 

More details on these points are outlined in the attached document, in order of their appearance in the proposal.",PUBLISHED,large,Irina,COM(2021)206,2021-08-06 10:16:52,withinfo,24212003,closed,067223231522-58,,
en,2665504,BEL,eu_citizen,,HELLEPUTTE,See attached document.,PUBLISHED,,Thibault,COM(2021)206,2021-08-06 10:00:00,withinfo,24212003,closed,,,
de,2665503,DEU,other,Deutscher Anwaltverein ,Adzakpa,"Siehe vollständige SN in der PDF-Datei.

1. Grundsätzlicher Ansatz der Regulierung
Der grundsätzliche Ansatz der Regulierung in Form einer horizontalen Regulierung sowie die Grundpfeiler für hochriskante KI-Systeme sind zu begrüßen, namentlich:
-	eine verstärkte Verpflichtung zum Qualitäts- und Risikomanagement (Art. 9 und 17) einschl. Post Market Monitoring (Art. 61),
-	eine stärkere Regulierung von Trainings- und Testdaten (Art. 10 und 9 Abs. 5 – 7),
-	eine Forderung eines Event Logging (Art. 12, 16 lit. a) und d) bzw. Art. 29 Abs. 5) sowie schließlich
-	die Beobachtungspflichten nicht nur des Anbieters, sondern auch des Nutzers (Art. 29 Abs. 4).

2. Gegenstand und Reichweite der Regulatorik
Annex I des Entwurfs fasst die regulierten Systeme sehr weit und schließt normale Expertensysteme sowie Such- und Optimierungsmethoden ein. Zu prüfen ist, ob als KI mittels des Annex I nur solche Systeme definiert werden sollten, deren Entscheidungen bzw. deren Verhalten (als mit zumutbaren Mitteln) praktisch unvorhersehbar anzusehen sind.  
Soweit dann nach dem KI-VO-E ein Fall künstlicher Intelligenz vorliegt, unterscheidet diese zwar zu Recht verschiedene Stufen der KI; die vorgesehene Dreistufigkeit erscheint jedoch teils etwas willkürlich. 

3. Umfang und Aufbau des Verordnungsentwurfs / Regelungstechnik
Die Gesamtverordnung ist sehr lang und in ihrer Struktur durchaus komplex. Dieses dürfte zu gewissen Anwendungsschwierigkeiten und u.U. auch zu Rechtsunsicherheiten führen. Die Regelungen sind von unterschiedlicher Regelungstiefe. Teils werden echte Details, teils nur grobe Linien vorgegeben. Hier sollten die allgemeinen Prinzipien deutlicher herausgearbeitet und mit anderen Prinzipien des europäischen Produktsicherheits-, aber auch des Produkthaftungsrechts abgeglichen werden. Damit würde auch die Rechtssicherheit erhöht.

4. Einzelregelungen und Aspekte
-	Inhaltlich zu hinterfragen ist, warum Nutzer im Sinne der geplanten Verordnung nur professionelle Nutzer sind (Art. 3 Abs. 4).
-	Kritisch zu hinterfragen ist, dass biometrische Identifikationssysteme weitgehend nur verboten sind, wenn sie in Realtime funktionieren (Art. 5 Nr. 1 lit. d))
-	Auffällig ist, dass in Art. 9 für Sicherheitsmaßnahmen nur der anerkannte Stand der Technik gefordert wird.
-	Nicht realistisch erscheint die Anforderung in Art. 10 Abs. 3 S. 1, nach der Trainings-, Validierungs- und Testdatensätze relevant, repräsentativ, fehlerfrei und vollständig sein müssen
-	Widersprüchlich ist es, wenn in Art. 28 Abs. 2 der Anbieter teils per se aus der Verantwortlichkeit entlassen wird, wenn die KI-Systeme signifikant verändert bzw. entgegen ihrem Verwendungszweck genutzt werden. Dieses widerspricht dem grundsätzlichen Ansatz sowohl des Produkthaftungsrechts als auch des KI-VO-E (vgl. etwa Art. 9 Abs. 2 lit. b), 4 lit. c), 13 Abs. 3 lit. b) (iii) und 14 Abs. 2), nachdem auch immer der zu erwartende Missbrauch eines Systems vom Anbieter mitberücksichtigt werden sollte.
-	Innovationschädlich könnten (je nach Gebrauchsanweisung) über die Beobachtungspflichten hinausgehende Überwachungspflichten des Nutzers im Sinne eines permanenten Monitoring sein (Art. 29 Abs. 4).

5. Daten- und Geheimnisschutz
Die aus Sicht des öffentlichen Sicherheits- und des Zivilrechts zu begrüßende, vom KU-VO-E geforderte umfassende Datenaufzeichnung (insb. von Eventlogs) und Dokumentation kollidiert mit dem Daten- und Geheimnisschutz. 

6. Regulatorik und KMUs 
Zu fragen ist, ob die KI-Reallabore (Sandboxes), wie sie in Art. 53 f. geregelt sind, im Rahmen des verfassungsrechtliche Zulässigen (Gleichberechtigung) ausgeweitet werden sollten, um KMUs die Entwicklung innovativer KI-Systeme und auch deren Erprobung in der Praxis zu ermöglichen

7. Sonstige Konsequenzen
Die KI-VO wird, wenn sie in Kraft tritt, direkte Auswirkungen auf das nationale Recht haben. Insofern fragt sich, ob nicht einige der Regelungen der KI-VO-E den Akteuren zu weit reichende Pflichten und damit Haftungsrisiken auferlegten.",PUBLISHED,medium,Hannah,COM(2021)206,2021-08-06 09:57:10,withinfo,24212003,closed,87980341522-66,,
en,2665502,BEL,other,AI4Belgium,Ackerman,"Key feedback points
1)	Overall, the proposed regulatory framework is much appreciated and welcomed.
2)	The proposal’s “list-based” approach risks being incomplete, and it requires periodic assessments.
3)	The scope of the proposal requires further refinement, and overlap with sectorial legislation that already covers AI needs to be more closely considered.
4)	Further clarifications are needed for certain terms that are used throughout the proposal. 
5)	The requirement of ex ante third party conformity assessments could potentially be extended in certain cases.
6)	Some open questions remain around the value of the proposed CE label.
7)	Concerns about the regulation’s enforcement mechanism should be addressed.
8)	There is a need for guidance as to how the requirements should be implemented.
9)	Additional efforts are needed to stimulate innovation. 
10)	More attention should be given to the impact of AI on societal interests and the environment.
",PUBLISHED,large,Nathanael,COM(2021)206,2021-08-06 09:55:40,withinfo,24212003,closed,,,
en,2665501,BEL,other,AI4Belgium,,"Key feedback points
1)	Overall, the proposed regulatory framework is much appreciated and welcomed.
2)	The proposal’s “list-based” approach risks being incomplete, and it requires periodic assessments.
3)	The scope of the proposal requires further refinement, and overlap with sectorial legislation that already covers AI needs to be more closely considered.
4)	Further clarifications are needed for certain terms that are used throughout the proposal. 
5)	The requirement of ex ante third party conformity assessments could potentially be extended in certain cases.
6)	Some open questions remain around the value of the proposed CE label.
7)	Concerns about the regulation’s enforcement mechanism should be addressed.
8)	There is a need for guidance as to how the requirements should be implemented.
9)	Additional efforts are needed to stimulate innovation. 
10)	More attention should be given to the impact of AI on societal interests and the environment.
",PUBLISHED,large,,COM(2021)206,2021-08-06 09:54:36,anonymous,24212003,closed,,,
en,2665499,BEL,business_association,AESGP - Association of the European Self-Care Industry ,Schaeffer,"AESGP welcomes the European Commission’s effort to develop a European approach for artificial intelligence (AI) that promotes Europe’s innovation capacity while ensuring the creation of an ethical and trustworthy AI. If properly addressed, AI could bring tremendous benefits to the healthcare sector as a whole and some of its potential applications are already described in numerous publications.

Particularly in the self-care sector, AESGP believes that AI can help fostering a personalised self-care approach and empowering people to better manage their health and well-being, especially regarding the prevention, early detection and treatment of potential health concerns. AI, by analysing data shared by consenting individuals, could support them in practicing correct self-diagnosis and responsible self-care.

AESGP welcomes the risk-based approach but calls for internationally aligned criteria 
Supervision of AI should be indeed proportionate to the intended use and led by defined risk categories with clear internationally defined criteria. 

Data governance is a cornerstone for AI policy development 
Discretionary application of GDPR and market fragmentation are major issues that need to be properly addressed in the coming years, concomitantly to AI policy development. 

Clearly defined accountabilities and liabilities that allow progress and utility of AI
The development and use of AI-powered tools is subject to different contributors. As the development of the various applications of AI is progressing, it is becoming absolutely necessary that accountabilities and liabilities are clearly defined, in respect of user safety, developers’ guidance and business certainty.

A regulatory framework with flexible mechanisms for quick adaptation to technology evolution 
As recognised in the regulation proposal, the current fast-paced development of new technologies requires a change in the way these new services/products are regulated.  

Fair and horizontal regulation 
In the healthcare sector, the pharmaceutical industry is used to working in a highly regulated environment. The various types of products must follow a strict procedure of safety testing and extensive evaluations of their benefits before any marketing authorisation is issued by the authorities. Industry actors such as digital developers may be new to this highly regulated environment of traditional healthcare. Without overly limiting access to innovative technologies, regulators must ensure a fair level playing field for all actors by drafting and enforcing horizontal regulations across sectors while taking into account the particularities of the healthcare sector, respecting the benefit/risk profiles of the products and the data privacy of the patients.

Empower people’s health and digital literacies for better understanding and use of AI
If general population is more informed on the added value of healthcare data for individuals and society, it could be more prone to register and use data while, consequently, increasing data quantity and quality. Information on rights, duties and possibilities granted by the use of healthcare data will create more competent and confident citizens in dealing with digitalization. The prerequisites must, therefore, be met for individuals to be the responsible owners of their data and to enact this sovereignty.
Having everyone digitally and data literate will not only ensure responsible and adequate use of the digital tools, but will build trust and confidence in the system as well. 

Europe must ensure a fit for purpose regulatory framework so that people can safely enjoy the benefits brought by digital transformation and new technologies. AESGP believes that industry, regulators, legislators and society at large have to work together to accelerate the effort, if Europe is to become a leader in the digital health space.
",PUBLISHED,small,Paul-Etienne,COM(2021)206,2021-08-06 09:50:38,withinfo,24212003,closed,99565011637-64,,
en,2665498,JPN,business_association,Keidanren,Digital Economy,Please see the attached.,PUBLISHED,large,Committee,COM(2021)206,2021-08-06 09:47:28,withinfo,24212003,closed,,,
en,2665497,BEL,business_association,Small Business Standards (SBS),EDVARDSSON,"SBS thanks the European Commission for the opportunity to provide feedback on the proposal for a European Act on Artificial intelligence (the AI ACT). 

SBS has prepared feedback regarding the impact of the AI ACT on SMEs, especially on technical issues, quality assurance and certification, and impact on innovation. 

The feedback is currently pending approval from SBS members and we will submit it in the coming days.
",PUBLISHED,micro,Maria,COM(2021)206,2021-08-06 09:46:39,withinfo,24212003,closed,653009713663-08,,
en,2665494,USA,company,Intel Corporation,,Intel's comments are in the attached file.,PUBLISHED,large,,COM(2021)206,2021-08-06 09:40:27,anonymous,24212003,closed,7459401905-60,,
en,2665491,BEL,academic_research_instittution,KU Leuven Centre for IT and IP Law,,"Dear members of the European Commission, 


Please find enclosed the feedback of several researchers (including myself) at the KU Leuven Centre for IT and IP Law. 

Should you have any questions and/or remarks, please do not hesitate to contact us. 


Yours sincerely, 

Koen Vranckaert",PUBLISHED,medium,,COM(2021)206,2021-08-06 09:30:20,anonymous,24212003,closed,,,
en,2665490,BEL,company,Johnson & Johnson,Martin,"Johnson & Johnson welcomes the opportunity to build a trustworthy and innovative ecosystem for artificial intelligence (AI), where the AI Act (AIA) plays a critical role in combination with the existing regulatory framework. The AIA should have a clear scope and framework, facilitating its implementation and avoiding complexity, in order to build trust between citizens, developers, deployers and users and create a favourable environment which fosters innovation. The international implications of this regulation and cross-border regulatory cooperation are important to ensure EU citizens and businesses have access to beneficial AI solutions.

The AIA includes in its objectives addressing health-related AI systems, and aims to support parties in effectively managing risks and unlocking the potential benefits of using AI in healthcare. We support the current approach of focusing on healthcare applications that are covered under the Medical Devices Regulation (MDR) and In-Vitro Diagnostics Regulation (IVDR), such as Software as Medical Device (SaMD).  

This proposal is joining a very robust regulatory framework which already addresses potential risks emerging from the development and use of AI in medical technologies. The interplay of the new regulation with regulations such as the General Data Protection Regulation (GDPR), MDR and IVDR needs to be further clarified. 

Medical devices, including many aspects related to medical software, are comprehensively and clearly regulated under the EU MDR/IVDR, which pursue similar objectives as the AIA, i.e., ensuring a high level of protection of human health and avoiding a harmful impact on the health and safety of persons, and they already take into account the use of AI systems operating as “components of products”. In order to achieve legal certainty, we believe that those requirements (e.g., conformity assessment, database registration, Notified Bodies, etc.) already specified under the sectoral legislation, i.e., MDR/IVDR, should supersede AIA provisions. Nevertheless, the AIA may help fill any potential gaps by giving a framework upon which the Medical Devices Coordination Group (MDCG) could develop further guidance for AI in medical technologies.

The scope of the regulation needs a clear definition of AI. As written, it could cover any software or statistical approach, leading to  a lack of certainty regarding which products are within the scope of which obligations.

Taking into account the GDPR experience will be fundamental for a successful development of this regulation, as we observed how important objectives and well-intended architecture led to fragmented interpretation and inconsistent implementation across Member States. The role and flexibility given to Member States should not lead to divergence but rather to ensuring a more cohesive and harmonized interpretation, implementation and enforcement of the legislation. ",PUBLISHED,large,Angel,COM(2021)206,2021-08-06 09:18:40,withinfo,24212003,closed,75617941310-89,,
de,2665488,DEU,company,Verband kommunaler Unternehmen e.V.,Kessel,"Der VKU bedankt sich für die Möglichkeit, zu dem Verordnungsvorschlag der Kommission über harmonisierte Regeln für künstliche Intelligenz Stellung zu nehmen. 
Wir begrüßen, dass die Kommission in ihrem Vorschlag einen risikobasierten Ansatz verfolgt. Damit bleibt die Mehrzahl der KI-Anwendungen unberührt, da von ihnen kein Risiko ausgeht. 

Wichtig ist nun, dass unter den Anwendungsbereich des Verordnungsvorschlags nur entsprechend risikoreiche Systeme fallen. Dies bedeutet erstens, dass nur Systeme den strengen Anforderungen entsprechen müssen, die unmittelbar im Hochrisikobereich zum Einsatz kommen. Zweitens dürfen unter die Definition der künstlichen Intelligenz keine herkömmlichen Algorithmen fallen.

Kommunale Unternehmen verwenden bereits heute in vielen Bereichen künstliche Intelligenz. Neben der Momentaufnahme ist die mittel- bis langfristige Perspektive von großer Relevanz, da KI zukünftig noch breiter und in allen Geschäftsbereichen zum Einsatz kommen wird. Darüber hinaus wird die Menge an Daten aus intelligenter Sensorik stark anwachsen und so auch die Nutzung von KI in vielen Geschäftsbereichen ermöglichen und voranbringen. Schon heute wird die Verwendung künstlicher Intelligenz in verschiedensten Betriebs- und Geschäftsbereichen, beispielsweise dem Vertrieb, bei Kundenschnittstellen, der Instandhaltung von Maschinen und Geräten oder auch in der Netz- und Verkehrssteuerung erwogen und teilweise bereits von den Unternehmen selbst oder in Kooperation mit Partnern entwickelt. Darüber hinaus werden auch Open Source Lösungen genutzt oder KI-Lösungen eingekauft, die wiederrum teilweise an die spezifischen Unternehmenssituationen angepasst werden. Letztlich ist für die Unternehmen auch die eigene Entwicklung oder die gemeinsame Entwicklung mit Partnern von KI-Systemen denkbar, die dann auch auf dem Markt angeboten werden könnten.

",PUBLISHED,medium,Simon,COM(2021)206,2021-08-06 09:00:20,withinfo,24212003,closed,1420587986-32,,
de,2665487,DEU,business_association,Bundesverband Digitale Wirtschaft (BVDW) e.V.,Rieke,"Der Bundesverband Digitale Wirtschaft (BVDW) e.V. ist die Interessenvertretung für Unternehmen, die digitale Geschäftsmodelle betreiben oder deren Wertschöpfung auf dem Einsatz digitaler Technologien beruht. Als Impulsgeber, Wegweiser und Beschleuniger digitaler Geschäftsmodelle vertritt der BVDW die Interessen der digitalen Wirtschaft gegenüber Politik und Gesellschaft und setzt sich für die Schaffung von Markttransparenz und innovationsfreundlichen Rahmenbedingungen ein. Sein Netzwerk von Experten liefert mit Zahlen, Daten und Fakten Orientierung zu einem zentralen Zukunftsfeld.

Der BVDW bedankt sich für die Möglichkeit der Stellungnahme und fügt seine ausführliche Positionierung als separates Dokument bei. 

Besonders problematisch ist aus Sicht des BVDW:
o	die zu weit gefasste Definition von KI;
o	Rechtsunsicherheiten allgemein und insbesondere in Bezug auf Hochrisiko-KI;
o	die Vielzahl an Widersprüchen im Vorschlag insbesondere bei den Transparenzpflichten;
o	Die teils unverhältnismäßigen Anforderungen und die Bürokratie, die im Vorschlag mit einem hohen Bußgeld einhergehen.

Eine innovationsfreundliche und vertrauensvolle „KI made in Europe“ braucht aus Sicht des BVDW:
1.	eindeutige Begriffsdefinitionen, klare Anwendungsgebiete und Rechtssicherheit;
2.	Auswirkungsregulierung statt Technikregulierung; 
3.	Weniger bürokratischen Aufwand;
4.	Eile bei den technischen Spezifikationen;
",PUBLISHED,small,Katharina,COM(2021)206,2021-08-06 08:46:52,withinfo,24212003,closed,479540331468-69 ,,
en,2665486,DEU,company,Deutsche Börse Group,Doser,"Dear Sir or Madame, please see the response of the Deutsche Börse Group attached.

Best regards,
Jan Doser ",PUBLISHED,large,Jan Wolfgang,COM(2021)206,2021-08-06 08:01:23,withinfo,24212003,closed,20884001341-42,,
en,2665484,USA,academic_research_instittution,The Electronic Privacy Information Center (EPIC),Schroeder,"The Electronic Privacy Information Center (EPIC) is a privacy-focused public interest research center based in Washington, D.C.  EPIC welcomes the European Commission's significant work towards putting in place a regulatory framework addressing the development and use of artificial intelligence (AI) systems. However, EPIC believes that the current Artificial Intelligence Act (AIA) draft does not meaningfully address several privacy and human rights concerns related to use of AI systems. In the attached paper, we describe how these unaddressed concerns may allow AI systems to be used in ways that cause serious harm to individuals interacting, knowingly or unknowingly, with those systems. We also identify specific portions of the AIA draft that require changes or additions in order to create more robust and meaningful protections and align with current privacy and human rights guidance. 

Specifically, EPIC recommends that the following actions be taken:

1)	Close loopholes and remove exemptions on regulatory requirements for AI systems and expand prohibitions where necessary

Current exemptions within the AIA are far too broad, functionally allowing for extensive use of AI systems that are high-risk, unnecessary, and manipulative while the existence of the AIA would give individuals a false sense of protection. In addition, the prohibitions contained in the AIA are extraordinarily narrow in scope, likely requiring highly subjective evaluations to determine which systems fall under the prohibitions. EPIC recommends that the Commission expand prohibitions and remove exemptions to existing prohibitions in order to truly protect individual privacy rights affected by AI systems.

2)	Mandate that individuals subject to AI system decision-making be notified prior to use of the system

The AIA requires notification to individuals only in extremely limited circumstances (which are themselves subject to numerous exceptions). Further, the AIA does not require that individuals subject to AI system decision-making be provided a method to opt out of or challenge that decision-making - either its use or results. EPIC urges the Commission to mandate that notification be given to affected individuals prior to system use, along with an option to reject or challenge the use of AI decision-making.

3)	Fully ban emotion recognition and biometric categorization systems

While nominal limitations on use of AI systems related to emotion recognition and biometric categorization are present in the AIA draft, use of these systems is still permitted in certain contexts. EPIC recommends that the Commission fully ban emotion recognition and biometric categorization systems, as the proposed notice requirements cannot mitigate the severe problems of inaccuracy and bias that are inherent within these technologies.

4)	Require that conformity assessments be reviewed and approved by data protection authorities prior to use

Currently, the AIA draft requires that all high-risk AI systems listed in Annex III undergo conformity assessments to ensure compliance with the AIA. However, many of these audits will not require review by a third-party auditor and may be kept entirely confidential and internal. EPIC recommends that the Commission mandate that all conformity assessments be reviewable by trained and qualified enforcement authorities, as well as available to the public, rather than remaining purely internal. 

The attached paper provides detailed explanations of why EPIC believes that each listed action is necessary, potential harms if these actions are not taken, and specific recommendations for how to carry out each action within the AIA draft. We welcome the opportunity to work with the Commission on these matters.
",PUBLISHED,small,Calli,COM(2021)206,2021-08-06 06:15:37,withinfo,24212003,closed,,,
en,2665483,BEL,company,Fujitsu,CANTON,"Fujitsu understands the need for European regulation of AI and encourages the European Commission to make all efforts to finalize a framework legislation that can be effective both at European and global level with a strong focus on supporting technology exchange and innovation. In order to do that, this regulation must be as simple as possible with clear definitions and limit the obligations for all stakeholders in the value chain by focusing on essential rules and requirements. 

We appreciate the efforts of the European Commission in providing the first horizontal-framework proposal for a regulation of such a complex topic with impact on many technologies and ecosystems-value chains with clear synergies with other regulations at European level. 

Artificial Intelligence (AI) must be seen primarily as a great opportunity to improve our society and the life of all citizens rather than a threat. It is important to make sure that this technology does not cause harm to anyone and in particular to the most disadvantaged categories of citizens, ensuring a responsible rollout of the technology with particular focus on privacy, cybersecurity and transparency.  

Fujitsu is promoting a Human-Centered development of AI solutions in full respect of human rights, ethics values and in line with the UN SDGs. In 2018 Fujitsu formulated the ""Fujitsu Group AI Commitment,"" demonstrating dedication to Safe and Secure Use of AI. In this statement, we highlighted five main principles: 

1 - Provide value to customers and society with AI 

2 - Strive for Human Centric AI 

3 - Strive for a sustainable society with AI 

4 - Strive for AI that respects and supports people's decision making 

5 - As corporate social responsibility, emphasize transparency and accountability for AI 

Strong emphasis must be placed not only on respecting fundamental human rights but also on sustainability, climate change and green transition in line with the UN SDGs, the recent conclusions of the G20 and the European Commission strategies.  

Fujitsu strongly believes in the importance of having a global approach toward AI. An open dialogue about this hugely important technology, allocating strong efforts on Global Standards, R&D investments, shared definitions, rules and principles, will encourage strong cooperation and trade among countries sharing the same principles and fast adoption of AI systems that goes across borders. 

Attached a more detailed response to this consultation that incorporates feedbacks and suggestions from Fujitsu experts coming from Europe, Japan, US, Canada and other Countries.  ",PUBLISHED,large,Marco,COM(2021)206,2021-08-06 01:19:01,withinfo,24212003,closed,441732425040-02,,
en,2665481,USA,business_association,Consumer Technology Association,Johnson,Please see attached comments from the Consumer Technology Association (CTA).,PUBLISHED,medium,Douglas,COM(2021)206,2021-08-05 23:02:54,withinfo,24212003,closed,,,
en,2665480,GBR,academic_research_instittution,"Legal, Ethical & Accountable Digital Society (LEADS) Lab, University of Birmingham",Smuha,"This document contains the response to the Commission’s Proposal for an Artificial Intelligence Act from members of the Legal, Ethical & Accountable Digital Society (LEADS) Lab at the University of Birmingham, authored by Nathalie Smuha, Emma Ahmed-Rengers, Adam Harkens, Wenlong Li, James MacLaren, Riccardo Piselli and Karen Yeung.",PUBLISHED,micro,Nathalie,COM(2021)206,2021-08-05 21:35:43,withinfo,24212003,closed,,,
nl,2665479,NLD,company,CIO Platform Nederland,van der Pol,"CIO Platform Nederland welcomes the opportunity to provide feedback on the proposal for the Artificial Intelligence Regulation. We consider the AI proposal a starting point to assess how to honour fundamental rights, health and security, while allowing AI technology to flourish. 

In brief, an appropriate framework for the safe, responsible, and sustainable use of AI systems in Europe is essential for the full exploitation of AI technology in the years to come. AI encompasses a very broad range of technologies which develops at a very high rate. For business users, each category of AI has specific opportunities, operational contexts and use cases as well as risks.

In particular, we have identified five aspects that we believe require additional attention or should be re-evaluated. Taking into account these aspects would help create an AI-Act that enables business users to effectively contribute to European economy and society by using AI systems.

For our full view, please find our position paper below. 
",PUBLISHED,micro,Idsard,COM(2021)206,2021-08-05 20:49:32,withinfo,24212003,closed,002126439066-24,,
en,2665478,GBR,company,Standard Chartered,Webber,Please see attached feedback from Standard Chartered. ,PUBLISHED,large,Marianne,COM(2021)206,2021-08-05 20:41:56,withinfo,24212003,closed,16595501800-80,,
en,2665477,BEL,ngo,Move EU,FAZLIU,"Move EU welcomes the European Commission’s ambitious proposal for an AI Regulation and the opportunity to provide feedback on behalf of its member companies in the on-demand mobility sector. 

Move EU brings together the leading actors in the field of ride-hailing services. Speaking with one voice, our members aim to foster the rapid and sustainable deployment of on-demand mobility in the European Union. 

The Commission's proposal is a fundamental step towards building a solid framework that creates legal certainty, ensures the respect of fundamental rights, and unlocks innovation. However, there are some points, as listed in the attached contribution, which from our perspective should be further clarified to secure the most balanced framework.
",PUBLISHED,micro,Getoar,COM(2021)206,2021-08-05 20:20:01,withinfo,24212003,closed,315076940253-42,,
en,2665476,DEU,business_association,German Medical Technology Association (BVMed),Marx,"With the publication of the draft of an ""Artificial Intelligence Act"" (AIA), the European Com-mission has issued a proposal for a regulation which outlines how artificial intelligence (AI) and its use shall be regulated in the EU. The Act intends to create a uniform legal framework across all sectors for the development, use and marketing of AI, i.e. regardless of whether it is used in a computer game, a self-driving car or digital medical devices, in particular. The German Medical Technology Association (BVMed) welcomes in principle the draft on the harmonization of AI regulations.
AI technologies have the potential to revolutionize an area of medicine in the way that the discovery of penicillin did.  AI medical devices in particular can make a highly relevant con-tribution to the emergence of innovations in this regard. Such disruptive innovations are urgently needed to meet the challenges of an ageing society. Moreover, AI systems are in-creasingly enabling better outcomes in the treatment of patients. There are numerous ex-amples of studies with AI medical devices in the areas of prevention, diagnosis and therapy. Although many examples are currently still at the experimental stage, the disruptive po-tential of this technology for medicine is already notable. 
Against the background of the EU Commission's cross-sectoral, horizontal approach, howev-er, it is important to BVMed that overregulation of AI developments and applications in the field of medicine and healthcare is avoided at all costs, in order to guarantee access for all patients to highly innovative, affordable AI medical products in Germany and the EU in the future. Additional costs and uncertainties in the approval of AI medical devices must not lead to barriers to innovation. In addition, the draft regulation should take greater account of the fact that with the implementation of the Medial Device Regulation (MDR), medical devices that are already CE-certified today, e.g. algorithm-based solutions, have a very high level of safety and quality for patients.

Specifically, BVMed identifies a need for change in the Artificial Intelligence Act (AIA) regarding the following issues:
1. The classification as ""high-risk AI systems"" is too general. The context of the application must be given greater consideration.
2. The definition of artificial intelligence in Art. 3 No. 1 AIA is too broad. In order to prevent market access barriers for medical device manufacturers, a tightening of the definition is urgently required. 
3. Sufficient access to medical data can prevent bias. This requires a uniform legal basis for data processing. 
4. Only basic safety and performance requirements should be regulated in the AIA. Specific requirements adapted to the field of AI medical devices should be defined in the harmonized standards mentioned in Art. 40 of the AIA. 
5. In the context of the geographical scope of application of the AIA, clarification is needed to ensure that the secondary use of AI results obtained in a third country does not lead ret-roactively to the applicability of the AIA.
6. Due to the provision in Art. 67 of the AIA, there is a risk of ""double"" post-market control of AI medical devices that is not objectively justified. Contradictions with MDR requirements and duplication of efforts for medical device manufacturers must be avoided. Existing regu-latory requirements can already address the risks associated with AI.
7. The scope of distributor obligations must be clearly defined in order to avoid an ambiguity regarding the obligations of economic operators down the supply chain. 
8. The requirement for common technical documentation under Art. 11 (2) of the Artificial Intelligence Act can make cooperation between companies more difficult. A Clarification is needed. 
9. An additional overload of the Notified Bodies due to increased requirements for medical devices must be avoided. Extensive retesting of products must be avoided.
",PUBLISHED,small,Katja,COM(2021)206,2021-08-05 19:52:52,withinfo,24212003,closed,,,
en,2665475,GBR,company,RELX,CROSSICK,"RELX welcomes the opportunity to provide feedback on the European Commission’s proposed regulation on Artificial Intelligence (AI). The regulation sets out how the Commission wishes to develop harmonised rules on AI and will have a significant impact on the future development and use of AI systems in Europe. It marks the world’s first specific piece of legislation targeted at regulating the use of AI. The regulation is an opportunity to strengthen the EU’s position as a first mover on responsible technology oversight. 

AI is a central issue to the global digital economy and will likely be at the heart of future economic transformation across the globe. Global competitiveness in AI is high and the development of effective AI is a priority for countries around the world. The US, UK and China are all direct competitors to the EU in both the academic and commercial sectors and are looking to lead in AI development. This regulation is therefore an important milestone in the EU’s bid to be a leader in responsible AI. 

RELX supports the approach the Commission has taken, not trying to regulate the technology itself but rather applying a risk-based approach and limiting its application to high-risk AI use cases. That said, it is important that where there are overlaps with other EU regulations, such as financial regulation, sector-specific regulation, and GDPR, this regulation should be without prejudice. We support the encouragement of industry-led standards and innovation friendly measures including sandboxes. 

RELX is a global provider of information-based analytics and decision tools for professional and business customers across a range of sectors, including financial services, legal, agriculture, science, technology, medical, healthcare and energy. We employ 33,000 people worldwide and support customers in 180 countries. We utilise technology and data to help our customers improve their decision making across the sectors we serve. We help scientists make new discoveries, doctors and nurses improve the lives of patients, lawyers win cases, prevent online fraud and money laundering and insurance companies evaluate and predict risk. 

AI is a key interest for RELX’s businesses. RELX is both a significant consumer and developer of the latest technologies, investing annually approximately €1.2 billion in technology and employing over 9,000 technologists. Some of these techniques fall within the broad definition of Artificial Intelligence. Our technology is developed across the world, with a significant number of our data scientists and developers in Europe, with tech hubs in London and Amsterdam. As a matter of best practice and corporate mission, RELX focuses on the responsible application of AI and aims for fair and transparent application of these approaches.

The position paper attached, outlines RELX’s initial views on the Commission’s proposed regulation, ahead of further discussion with the EU institutions. Our goal is to support innovative technologies that benefit society while ensuring that those technologies respect an individual’s fundamental rights. 

RELX looks forward to working with the EU institutions to finalise this important Act.",PUBLISHED,large,elizabeth,COM(2021)206,2021-08-05 19:25:11,withinfo,24212003,closed,338398611148-62,,
en,2665474,DEU,business_association,SPECTARIS e.V.,Mutter,"SPECTARIS - the German Industry Association for Optics, Photonics, Analytical and Medical Technology welcomes the Commission´s initiative to lay down harmonised rules on Artificial Intelligence. SPECTARIS agrees with the objectives outlined in the European Commission´s proposal (Artificial Intelligence Act; AIA): to further research and innovation whilst maintaining a high level of safety for EU citizens. 

However, the planned legislation includes additional regulatory requirements for medical devices by rigidly classifying most medical devices as high-risk AI devices - regardless of their respectively used AI elements and risk class as defined in MDR (Regulation (EU) 2017/745 on medical devices) and IVDR (Regulation (EU) 2017/746 on in vitro diagnostic medical devices). Since applicable law has already set extensive AI requirements for medical devices and software, SPECTARIS would like to highlight a general concern: Parallel and unaligned regulatory frameworks regarding AI components in medical devices and in vitro diagnostics should by all means be avoided. Otherwise it would lead to regulatory overlapping, discrepancies and additional legal uncertainty for medical technology manufacturers. 

Hence, SPECTARIS recommends further alignment of the proposed AIA with the applicable law for medical devices and in vitro diagnostic as well as necessary additional conceptual clarifications and changes in the AIA. Detailed feedback on key issues are included in the attached position paper. 
",PUBLISHED,small,Corinna,COM(2021)206,2021-08-05 18:48:37,withinfo,24212003,closed,55587639351-53,,
en,2665473,GBR,company,DeepMind,Belias,Please find attached DeepMind's views on the European Commission's proposal on the Artificial Intelligence Act. We thank the Commission for the opportunity to provide feedback and look forward to engaging in further discussions to foster scientific excellence and trustworthy AI in the EU.  ,PUBLISHED,large,Alexandra,COM(2021)206,2021-08-05 18:48:30,withinfo,24212003,closed,086621031693-51,,
en,2665472,POL,business_association,ZPP,BIŃKOWSKI,"ZPP od dawna podkreślał, że przygotowanie adekwatnej regulacji w zakresie sztucznej inteligencji będzie wyjątkowo wymagającym zadaniem. Z jednej strony bowiem oczywistym celem regulatora jest zabezpieczenie obywateli i podmiotów gospodarczych przed nieetycznym stosowaniem technologii AI, z drugiej jednak – wprowadzenie zbyt daleko idących restrykcji skutkowałoby zahamowaniem innowacji, a przez to również pogorszeniem pozycji konkurencyjnej europejskich podmiotów. Jakkolwiek w przedstawionym projekcie dostrzegamy przestrzeń do doprecyzowań i zmian, uważamy że podejście Komisji jest w powyższym kontekście co do zasady proporcjonalne i w odpowiedni zabezpiecza oba ze wspomnianych dóbr. 
W przedstawionym projekcie dostrzegamy cztery obszary, które powinny być przedmiotem dalszych prac bądź rewizji. 
W pierwszej kolejności zwracamy uwagę na brak adekwatnego rozróżnienia odpowiedzialności między użytkownikami AI, realnie wykorzystującymi określone rozwiązania w prowadzonej działalności, a podmiotami dostarczającymi te rozwiązania na rynek. Wydaje się, że najbardziej logiczne byłoby, gdyby odpowiedzialność za zgodność rozwiązania i monitorowanie go po wprowadzeniu na rynek ponosiły podmioty wdrażające (wspomniani użytkownicy) – tylko oni mogą bowiem weryfikować ostateczne zastosowanie konkretnego rozwiązania opartego na AI. Problematyczny w tym kontekście wydaje się brak definicji podmiotu wdrażającego – wydaje się, że należałoby uzupełnić regulację w tym zakresie.
Dalej, zwracamy uwagę na fakt, iż niektóre sformułowania wykorzystane w przedstawionym projekcie wyznaczają standardy de facto niemożliwe do spełnienia. Dla przykładu, art. 10 ust. 3 stanowi, że „zbiory danych szkoleniowych, walidacyjnych i testowych muszą być adekwatne, reprezentatywne, wolne od błędów i kompletne”.  Trudno jest zagwarantować „brak błędów”, a więc doskonałość – dlatego zobowiązanie powinno dotyczyć raczej należytych starań w zakresie zapewnienia, że zbiory danych są wolne od błędów i kompletne. Co więcej, art. 14 (4a) stanowi, że osoby którym powierzono nadzór ludzki muszą umożliwiać „zrozumienie w pełni możliwości i ograniczeń systemu sztucznej inteligencji wysokiego ryzyka”. Zrozumienie „w pełni” tak skomplikowanej i szybko ewoluującej materii będzie prawdopodobnie wyznaczało nieracjonalnie wysoki standard, stąd też należałoby zrewidować regulację w taki sposób, by odnosiła się do „odpowiedniego” lub „wystarczającego” zrozumienia.
Część przewidzianych w regulacji wymogów wydaje się ponadto nieproporcjonalna, choćby art. 64 (2) zobowiązujący do zapewnienia (na uzasadniony wniosek) organom nadzoru rynku dostępu do kodu źródłowego systemu sztucznej inteligencji. Wobec faktu, że istnieją alternatywne metody weryfikacji działania systemu AI, a sam kod źródłowy jest chroniony jako tajemnica handlowa, obowiązek ten wydaje się być nadmierny.
Ostatecznie, zwracamy uwagę na konieczność doprecyzowania części elementów regulacji, m.in. poprzez skonkretyzowanie oczekiwań dot. należytej staranności w kontekście art. 10 poświęconego danym i zarządzaniu nimi, czy choćby sposobu pogodzenia wynikającego z art. 12 obowiązków dotyczących rejestrowania, z wynikającymi z RODO zasadami w zakresie minimalizacji pobieranych danych.

",PUBLISHED,small,Jakub,COM(2021)206,2021-08-05 18:15:38,withinfo,24212003,closed,868073924175-77,,
en,2665471,ITA,company,Sella Group,Daly,"Sella Group thanks for the opportunity to provide contributions about a notable regulatory project, which responds to ethical needs and respect for values and fundamental rights in the EU. The technological evolution should not affect health, safety and protection of fundamental rights, nor lead to discrimination. On the other hand, Artificial Intelligence is one of the key factors that could foster social inclusion. It is essential to set the conditions to open the innovation process to the entire entrepreneurial system by fostering the research and experimentation by start-ups and SMEs. In such context, the Regulation should be inspired by proportionality and its compliance costs should be sustainable even for the smallest innovators.
Moreover, the Regulation should not penalize AI systems themselves, but it should mitigate the AI-related risks which occur in some specific contexts/uses and that would not subsist - or would have a minimum impact - without the use of AI. Regulating AI if it is the mere executive mean of human choices would break the principle of technological neutrality and, at the same time, divert the attention from the effective causes of potential harmful and non-ethical behaviors.
For these reasons, Sella Group believes that a risk-based approach is a proper solution. The provision of minimum requirements for the medium-risk systems (those that determine a direct interaction of the final user with the output, without human intermediation – article 52) and the choice not to regulate low-risk systems, would limit the regulatory pressure to those uses of technology that could bring negative externalities to society.
At the same time, the residual character of the low-risk systems – opposite to the mandatory list of high-risk systems – theoretically allows to regulate only risky use cases.
However, the adoption of an approach actually proportioned to the risk is hindered; indeed:
•	The definition of AI appears too wide;
•	Annex III provides a mandatory list of high-risk systems contradicted by an open definition approach, leading to a wide application of most restrictive requirements without a specific and precise identification of risky use cases.
Therefore, we suggest some amendments and integrations to the Regulation (please see file attached).
Similarly, in order to foster the competition of small and medium-sized enterprises in this market development phase, we suggest to provide a transitory framework in which limiting requirements to a voluntary and experimental context, also encouraging the adherence to the proposed regulatory scheme. The definitive legislation could be adopted with some amendments/different identification of the high-risk uses, based on the acquired experience. Moreover, small-sized enterprises could invest in research and programming, entering the market and avoiding concentration risks which benefit larger corporations and can affect level playing field and the whole ecosystem.",PUBLISHED,large,Andrea,COM(2021)206,2021-08-05 18:07:31,withinfo,24212003,closed,,,
en,2665469,DNK,company,KMD A/S,Damm,"KMD welcomes the proposal for an AI Act by the European Commission.

For half a century, KMD’s contributions to Danish society has helped shape one of the most modern and progressive public sectors in the world. The high degree of trust that Danes have in digital and data driven solutions today stems from decades of developing solutions that are reliable, secure, transparent, unbiased, and explainable. KMD aims to continue building on this foundation in the future, and AI is a central component of this vision. AI regulation will affect a great part of the solutions that KMD – as a leading supplier of govtech and business-critical solutions – has already placed and plan to place on the market. 

KMD finds that regulation of AI will strengthen trust in the solutions and providers of the technology, and as such it will benefit the tech sector, authorities, and society at large. As AI has achieved a certain level of maturity and is being implemented in a wide array of software solutions, the time is ripe for establishing common standards and regulation of the use of AI across the EU. Clear and common requirements for AI solutions in EU makes the competitive situation equal across the Union. KMD also applauds the idea of creating an EU alternative to the implementation of AI in third countries. 

KMD, however, finds that there is room for improving the proposal especially in the following areas (described in detail in the attached document):
•	AI is weakly defined
•	Standardization
•	Quality measures
•	Unclear definition of roles
•	Human oversight
•	Sandboxes and data spaces
•	High-risk classification
•	Placing on the market and putting into service

An overall concern from KMD is finding the right balance of the regulation, so that innovation and development will not be impaired, and the requirements for testing, documentation etc. will not stand in the way of using and inventing AI solutions that benefit society and citizens.",PUBLISHED,large,Peter,COM(2021)206,2021-08-05 18:02:01,withinfo,24212003,closed,,,
en,2665468,DEU,company,E.ON SE,Abel,"E.ON welcomes the opportunity to actively participate in the consultation process on the Artificial Intelligence (AI) Act. E.ON generally supports the plans of the European Commission to create a uniform framework for AI with the primary goal to strike a risk-based balance between the fundamental rights of European citizens and enabling competitive AI systems to develop.

In order to ensure effective and fair regulation in this field, E.ON proposes inter alia the following:
- The proposed definition of AI is too broad. The definition should therefore include the ability of software to self-learn.
- Regulate the application, not the technology. Regarding the existence of an abstract high risk of AI, it is decisive in particular whether a decision affecting the environment it is interacting with is autonomously left to the AI itself or not.
- There should be synergies with other measures already established in other areas such as data protection management systems or cyber security standards established for critical infrastructure.
- Call for a priority focus on AI use cases with a benefit for climate and the environment.

Please find our recommendations attached.",PUBLISHED,large,Frederik,COM(2021)206,2021-08-05 17:56:16,withinfo,24212003,closed,72760517350-57,,
en,2665467,NLD,company,"Arthur's Legal, Strategies & Systems",VAN DER WEES,"Arthur's Legal, Strategies & Systems strongly supports and endorses the initiative by the European Commission for the current proposed Artificial Intelligence Act. The Commission's initiative will help ensure that AI is safe, lawful and in line with EU fundamental rights, and related responsibility, accountability (and liability) attribution. The overall goal is to stimulate the uptake of trustworthy AI in EU society and economy. We underline the importance of establishing such framework to foster safe, trustworthy and accountable AI-supported innovation. 

We also strongly support the proportionate, risk-based approach of the regulation. Adding AI to a process, technology or (eco)system could strengthen the capacity to do good and address societal challenges – of which also the EU and its member states, communities and residents have plenty, both short term, mid term, long term and extreme long term –, but it could also increases risk and augment the potential material detrimental impact. It is important to ensure that these risks are mitigated, and organisations (whether providers, users or otherwise) being ethically, socially and legally responsible, accountable and liable.

We believe that the current proposed AI Act is generally well-developed; however, some parts are not yet sufficiently developed or otherwise deserve further considerations, conditions, qualifications, detailing and other improvements.

We believe it will be helpful to take in and consider the observations, recommendations and other feedback as attached in the seperate file.
",PUBLISHED,small,Arthur,COM(2021)206,2021-08-05 17:56:16,withinfo,24212003,closed,510448420031-72,,
en,2665465,BEL,business_association,Assuralia,Dias,Please find enclosed Assuralia's contribution.,PUBLISHED,medium,Patricia,COM(2021)206,2021-08-05 17:30:45,withinfo,24212003,closed,0026376672-48,,
en,2665464,CHE,company,Novartis International AG,Grooten,"Novartis welcomes the European Commission's first-of-its-kind legislative proposal on the use of Artificial intelligence (AI), which positions the EU as leader in trustworthy AI and digital. As healthcare company, we use this innovative science to discover and develop medicines. AI technologies are transforming how we innovate and operate and they offer high potential for improving healthcare quality. Novartis is using AI extensively in drug discovery and, more recently, applies AI to drug development. In development, we view the applicability of AI in two ways. One includes the horizontal capabilities and tools that increase our understanding of unstructured data from various sources, like images and texts for applications across our pipeline. The other way is the application of AI algorithms and methodologies for product-specific uses across the life-cycle: early discovery, development, manufacturing and deployment of products and services. We look forward to further engage to shape a system that is fit for future and fosters innovation.

Novartis has published its commitment to ethical and responsible use of AI, provided in attachment. This includes a commitment to deploy AI systems in a transparent and responsible way and to ensure that the use of AI systems has a clear purpose that is respectful of human rights, is accurate, truthful, not misleading, and appropriate for their intended context. We are in agreement that a well-designed AI regulation is supportive of the protection of fundamental rights, ensuring safety and attributing liability. 

We want to comment on 3 key areas:
1) Definition of scope: the regulation would apply to the placing on the market, putting into service and use of AI systems: (i) software developed in line with the first Annex – covering AI based on machine-learning approaches, logic and knowledge-based approaches and statistical approaches and (ii) software that can, for a given set of human-defined objectives, generate outputs influencing the environments they interact with. Novartis believes that the scope should be more precise and narrow; focused on AI created through machine-learning approaches. Application of more general terms such as “rules-based” and “statistical” AI could cover applications which are not new, their underlying logic being transparent due to their explicit programming or curation. Our concern is that this regulation could place undue burden on long-utilized techniques/approaches, already agreed among regulators and industry today.
2) Risk assessment: the proposal introduces new oversight for “high-risk” AI systems which will require a case-by-case assessment from AI providers, based on the Annexes and a series of criteria. Among the identified high risks are harm to health and safety that could result from application to human beings, risk of negative impact on fundamental rights and potential for discrimination. We agree to cover these risks, but note that this proposal creates additional challenges for development of AI-driven software, if a different risk-level is applied vs the Medical Device and In-vitro Diagnostic Regulations. Clarification is needed on proposed criteria to specify “malfunctioning” of an AI algorithm.
3) Harmonization of approaches: the proposed AI Act will set forth obligations in addition those set forth in the Medical Device and In-vitro Diagnostic Regulation. We need harmonized regulatory definitions for AI & related terms, aligned risk classification and clarification on the responsibilities related to distributors who incorporate an algorithm vs. manufacturers of the algorithm. 

Clarification on the range of non-medical device uses of AI in development and their risk classification is needed; e.g. would AI used to categorize patients based on radiology scans or genetic profiles fall into this category? These data would be captured from medical devices, GDPR compliant, in clinical trials and would optimize data analytics. 
",PUBLISHED,large,Elke,COM(2021)206,2021-08-05 17:30:41,withinfo,24212003,closed,91269481588-28,,
en,2665463,DEU,business_association,Association of German Chambers of Industry and Commerce (DIHK),von Eicke,"The Association of German Chambers of Industry and Commerce (DIHK) welcomes the opportunity to provide feedback to the consultation on the proposal for the Artificial Intelligence Act. Artificial intelligence (AI) is considered to be one of the key technologies of digitalisation and a driver of economic growth. In order to ensure that the course is set for the successful development and application of AI, the DIHK advocates the improvement of the AI framework conditions at both state and federal level, as well as at EU level. For SMEs in particular, it is important that security and con-fidence are strengthened with respect to the use of AI technologies. A European legal framework can make an important contribution to this. The key here is to find the right balance between safe AI systems and innovation-friendly framework conditions. The legal provisions must not impose unnec-essary barriers to the further development of AI, but instead should have the effect of promoting in-novation. The key demands: the creation of legal certainty through a definition of the term ""AI sys-tem"" that is not only differentiated, but also as clear as possible. Here, the actual specific risk posed by these systems should be taken into consideration for the risk qualification. Obligations are to be designed in a pragmatic manner so that the bureaucratic burden for companies is kept as small as possible. Furthermore, coherence with requirements from existing European regulations is to be en-sured, thereby making sustainable use of the advantages of a single European market and strengthening global competitiveness.

Attached please find our detailed position regarding the AI Act proposal.",PUBLISHED,large,Steffen,COM(2021)206,2021-08-05 17:28:39,withinfo,24212003,closed,22400601191-42,,
en,2665462,BEL,ngo,Access Now Europe,Leufer,"Access Now welcomes the European Commission’s pioneering proposal for a regulatory framework for artificial intelligence. We have consistently pointed to the insufficiency of ethics guidelines and self-regulatory approaches, and have long called for regulatory intervention in the field of AI. The current Proposal provides a workable framework to ensure the protection of fundamental rights, but requires significant modifications in a number of areas or it risks failing to achieve the objective of protecting fundamental rights.

Broadly speaking, the main aim of the Proposal is to regulate so-called high-risk AI systems, namely those which pose a risk of harm to health and safety or a risk of adverse impact on fundamental rights. Title III of the Proposal “contains specific rules for AI systems that create a high risk to the health and safety or fundamental rights of natural persons,” and a list of high risk AI systems is provided in Annex III. The Proposal also contains provisions to prohibit certain AI practices in Article 5, which the Explanatory Memorandum says “comprises all those AI systems whose use is considered unacceptable as contravening Union values.” Additionally, the Proposal contains provisions for transparency obligations for certain AI systems such as chatbots and emotion recognition systems. As we will argue in the attached pdf document, a number of shortcomings of the Proposal prevent it from adequately achieving the aim of protecting fundamental rights.

As we explain in the attached pdf document, the definitions of emotion recognition and biometric categorisation contain certain technical flaws which must be addressed. Without defining these applications of AI correctly, we cannot hope to adequately address them in regulation. Section II therefore proposes alternative definitions of both of these terms, as well as a discussion of the severe risks they pose to fundamental rights. Having defined these two terms correctly, we then argue in Section III that they should both be prohibited.

Section III begins with a discussion of the four existing prohibitions in Article 5 of the Proposal. We point out flaws in the formulations of the prohibitions in Article 5, paragraphs 1(a), 1(b), and 1(c), and propose alternative formulations. We then provide justifications and proposed formulations for a number of additional prohibitions. Finally, we argue that Article 5 must be supplemented with a list of criteria to define ‘unacceptable risk’ so that other applications of AI can be added to the list of prohibited practices if evidence emerges that they pose unacceptable risks, and that a mechanism must be added to Article 7 to allow for additional practices to be added to Article 5.

In Section IV we address the relative lack of obligations placed on ‘users’ of AI systems as compared to those placed upon ‘providers.’ We suggest that additional obligations be placed upon users, including mandating that some form of impact assessment is carried out for all high-risk AI systems. In Section V, we discuss the need to extend the scope of the publicly viewable database of high risk AI systems proposed in Article 60.

Section VI raises a number of concerns about the current proposal for AI regulatory sandboxes in Articles 53 and 54, and makes recommendations for a number of modifications, including that the use cases under paragraph (i), namely those related to law enforcement applications of AI, be removed from Article 54. Section VIII addresses a number of gaps in the enforcement and redress mechanisms provided by the proposal and, finally, in Section VIII we note a number of additional concerns which need to be addressed in the Proposal.

While the current Proposal provides a workable framework for regulating harmful applications of AI, it requires serious modifications in a number of areas. We look forward to working with the co-legislators and other stakeholders in the coming months to ensure that these issues are addressed.
",PUBLISHED,micro,Daniel,COM(2021)206,2021-08-05 17:26:15,withinfo,24212003,closed,241832823598-19,,
en,2665458,NLD,ngo,Bits of Freedom,Benaissa,"Bits of Freedom welcomes the European Commission’s objective to ensure that AI systems are used safely and with respect to fundamental rights and Union values. We agree that the future of AI depends on public trust in the safety and compliance of AI with fundamental rights and that a human-centric approach is essential. Therefore we welcome the opportunity to share our concerns relating to the protection of fundamental rights in the Artificial Intelligence Act. In particular, we are worried that the scope of prohibited AI practices and high-risk classifications, and the support offered to citizens effected by AI to exercise their fundamental rights, fall short.  In order to provide a regulatory framework that truly respects fundamental rights, we encourage the Commission to revise the proposal and provide additional safeguards.

1. Prohibited practices
Bits of Freedom welcomes the prohibition of AI-practices that create unacceptable risks to fundamental rights. However, one could argue that the proposal’s broad exceptions, limited scope and vague wording render these prohibitions meaningless. Therefore, Bits of Freedom advises the Commission to prohibit the practices in article 5 without exceptions. 

2. High risk classification
In article 6 the Commission suggests a higher level of protection is needed for AI systems that are considered high risk. We agree with the Commission. However, by only providing effective rules for AI systems that are classified as high-risk, people might fail to be protected from AI systems that are not subject to this definition. To provide a sufficient level of protection, regulatory measures must also address these AI-systems.

3. Transparency
The proposal offers people no possibility to verify AI-output that effects their lives. Bits of Freedom urges the Commission to raise the level of protection offered to natural persons and users to guarantee that they can review and verify AI-output and -decisions. 

See Bits of Freedom's attached paper for further analysis and recommendations",PUBLISHED,micro,Nadia,COM(2021)206,2021-08-05 16:54:14,withinfo,24212003,closed,,,
en,2665455,USA,business_association,Association of Test Publishers,Thiemann,"Attached please find a link to the comments on the European Commission’s proposal on the Artificial Intelligence Act from the Association of Test Publishers. We thank the Commission for the opportunity to share feedback, and look forward to engaging in further discussions on how to best achieve proportionate, risk-based AI regulation in Europe -- and globally.    We wish to note that in our preparation, we coordinated with the European Test Publishers Group, the European Federation of Psychologists' Associations, and several members of the European EdTech Alliance.

Link to comments:  https://www.testpublishers.org/assets/Final%20Comments%20on%20EC%20AI%20Regs%2008042021.pdf",PUBLISHED,micro,Alan,COM(2021)206,2021-08-05 16:47:34,withinfo,24212003,closed,,,
en,2665452,DEU,company,BMW,Pape,"The BMW Group welcomes the European Commission's approach of creating uniform guidelines and rules for the use of artificial intelligence. Due to the complexity and the almost limitless possibilities, this can both strengthen confidence in the respective technology and maintain the innovative power.
With this point of view, we would like to draw particular attention to the following aspects:

AI Applications within Supply chains and supplier: BMW, like many other companies, frequently purchases AI applications and integrate them into new products or features. Consequently, AI systems are provided by a third party. It can be difficult for the manufacturer, which is considered as the ‘new’ provider, to prove that the AI system is compliant with the legislation: in the current Proposal it is unclear how that will practically work and whether the third party would need to provide the ‘new’ provider with all necessary testing/training data, source code or documentation in order to ensure compliance. This may interfere with legitimate interests of suppliers to protect their know-how and IP. Thus, the transparency requirements must be balanced with the IP requirements. 

Timeline for the application of the requirements to motor vehicles: AI Application will play a fundamental role for upcoming products and innovations. For the application of the new AI requirements the development cycle length of these products should be duly considered. Products with artificial intelligence applications that will be launched in the next years are already developed and trained.
Reference to and Coordination with other Regulation: As mentioned in Article 40, existing Regulation should be considered first, before setting up new regulations, processes, and institutions. In terms of data protection the GDPR set the guidelines for high security and data subjects rights standards. GDPR principles and protection standards are not to be exceeded. For technical functions the ISO26262 and SOTIF Guidelines already describe how safety and security within these functions can be reached. BMW is offering expertise to adapt existing standards and regulations for AI applications.

Guidelines for High Risk AI Systems: Overall, in Chapter 2, Title III, we note a vague and unspecified use of adjectives such as ‘appropriate levels’, ‘adequate’ or ‘relevant’ which are not legally defined and are a major factor of uncertainty. Clear references to standards for each of these parameters should be made. 
Furthermore, the requirements for risk management and data governance (“Training, validation and testing data sets shall be relevant, representative, free of errors and complete”) are not feasible in practice, because it is unrealistic to have for example a “complete data set”. This could negatively influence innovation and could lead to competitive disadvantage.

The BMW Group has been working on the topic of artificial intelligence for several years and has founded “Project AI” for this purpose. In October 2020, the ""Codes of Ethics"" was published by the BMW Group, which sets out the basic principles for the development and use of AI applications for the BMW Group.
See link https://www.bmwgroup.com/de/innovation/technologie-und-mobilitaet/artificial-intelligence.html
",PUBLISHED,large,Ruediger,COM(2021)206,2021-08-05 16:20:05,withinfo,24212003,closed,,,
en,2665448,USA,ngo,Europe Technology Policy Committee of the Association for Computing Machinery,Eisgrau,"HIGHLIGHTS OF COMMENTS BY THE EUROPE TECHNOLOGY POLICY COMMITTEE OF THE ASSOCIATION FOR COMPUTING MACHINERY * (PLEASE SEE ATTACHED DOCUMENT FOR EUROPE TPC'S FULL ANALYSIS OF THE PROPOSAL)
 
The Association for Computing Machinery (ACM) is the world’s largest and longest established professional society of individuals involved in all aspects of computing. It annually bestows the ACM A.M. Turing Award, often popularly referred to as the “Nobel Prize of computing.” ACM’s Europe Technology Policy Committee (“Europe TPC”) is charged with and committed to providing objective technical information to policy makers and the general public in the service of sound public policy formation. ACM and Europe TPC are non-profit, non-political, and non-lobbying organizations.
 Europe TPC is pleased to provide the following “feedback,” as invited by the European Commission, on the pending Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Proposal).

GENERAL OBSERVATIONS

Europe TPC supports the intent of the proposed AI Regulation to set harmonised rules for the development, placement on the market, and use of AI systems in the European Union following a proportionate risk-based approach that would most regulate the use of AI systems which pose significant risks to the health, safety, or fundamental rights of individuals. The Proposal, however, raises several significant technical concerns detailed below as “Specific Comments.” Europe TPC first wishes to highlight in broad terms that:

1. The Proposal’s definition of high-risk systems would benefit from a more precise definition of the risk hierarchy (see discussion of Articles 6 and 7);

2. The proposed regulation incorporates technical computer science and mathematical terms without formally defining them. Such imprecision may lead to counterproductive ambiguity in their interpretation and inconsistency in their application. In addition, the Proposal’s treatment of “health and safety” issues is generic rather than based, as would be advisable, on the highly evolved technical discipline of Safety Assurance (see discussion of Article 15);

3. Final regulations to be adopted by the co-legislators must in multiple ways fully respect personal privacy, affording at minimum the full protections mandated by all other applicable current law or regulation (including the General Data Protection Regulation). Protections afforded by the Proposal also must, in Europe TPC’s view, expressly include constraints on continuous data collection in public contexts (see discussion of Article 52); and

4. While constructively calling on all Member States to have a sufficiently large workforce expert in AI and many other related matters, the Proposal does not concretely address or enable the forms of education necessary to create such a highly skilled and appropriately diverse population (see discussion of Article 59).

CONTACT: ACMPO@ACM.ORG
_____________
* Principal authors of this document for ACM’s Europe Technology Policy Committee are: Alejandro Saucedo, Engineering Director at Seldon Technologies and Chief Scientist at Institute for Ethical AI & Machine Learning; and Chris Hankin, Fellow of the Institute for Security Science and Technology and Professor of Computing Science at Imperial College, London. Also contributing were: Oliver Grau, Principal Investigator at Intel Corporation; Dame Wendy Hall, Regius Professor of Computer Science, University of Southampton; Andrew McGettrick, Professor Emeritus of Computer and Information Sciences, University of Strathclyde; Enrico Nardelli, Professor in Informatics at Università di Roma; Gerhard Schimpf, Senior Manager at SMF Management Consulting - Pforzheim; Gurkan Solmaz, Senior Researcher at NEC Laboratories Europe; and Julie Williamson, Lecturer in Human Computer Inter¬action, University of Glasgow. (NOTE: All affiliations listed for identification purposes only.)
",PUBLISHED,large,Adam,COM(2021)206,2021-08-05 15:30:35,withinfo,24212003,closed,133002517679-87,,
en,2665447,FRA,ngo,Renaissance Numérique,Galissaire,"Please, find attached the contribution of think tank Renaissance Numérique and of the Chair on the Legal and Regulatory Implications of AI of Grenoble Alpes University related to the proposed Artificial Intelligence Act.

This contribution focuses on specific dispositions of the text which raise questions with regard to the implementation of the future regulation. Other aspects of the regulation, such as those related to “real time” remote biometric identification systems, are the subject of parallel works by the co-signatories . 

The two co-signatories hope the reflections presented in this paper will feed the ongoing debates at the European level in a useful way. ",PUBLISHED,micro,Jessica,COM(2021)206,2021-08-05 15:25:01,withinfo,24212003,closed,388341731904-25,,
fr,2665445,FRA,business_association,Fédération nationale des travaux publics (FNTP),GAUBERT,"L’approche par les risques de la Commission européenne semble être la meilleure approche dans la mesure où elle favorise la confiance dans l’intelligence artificielle sans entraver son développement. 
En revanche, il est essentiel de garder une marge d’innovation. Une attention particulière doit donc être portée aux définitions, notamment celles des systèmes IA et des systèmes à haut risque, car les obligations et exigences afférentes sont très lourdes et difficiles à mettre en place. Des définitions claires et suffisamment précises sont d'autant plus importantes qu'elles seront amenées à servir de références dans d'autres textes. Elles ne doivent donc pas conduire à créer de l'insécurité juridique.

L’article 7 prévoit la possibilité pour la Commission européenne d’adopter des actes délégués pour faire évoluer l’Annexe III établissant une liste de systèmes IA considérés à haut risque. S’il est effectivement nécessaire de tenir compte des évolutions dans la définition des IA à haut risque, le fait que la Commission puisse le faire par actes délégués est source d’insécurité juridique. Or, cela pourrait décourager les entreprises à développer des solutions d'IA innovantes en raison de l'évolution imprévisible du champ d'application de la réglementation au cours des prochaines années.
De plus, les critères énumérés à l'article 7, qui habilite la Commission à mettre à jour la liste de l'annexe III, sont parfois vagues et mériteraient d’être précisés afin de soutenir la sécurité juridique et la prévisibilité du marché. Il conviendrait par exemple d'introduire des dispositions explicites pour la participation des entreprises à tout processus futur de mise à jour de la liste.

Par ailleurs, il semble que la proposition de règlement ait été élaborée en méconnaissance du fait que de nombreux produits réglementés intégrant de l'IA en tant que fonction de sécurité, sont aujourd'hui utilisés en particulier des machines soumises à la Directive 2006/42/CE. Or, la méthodologie retenue par la directive Machines est d'effectuer une analyse de risques afin de prendre les mesures de protection adaptées. La proposition de règlement, quant à elle, considère d'office certains systèmes IA comme étant à haut risque, sans considération pour l'analyse de risques effectuée par l'entreprise aux termes de la directive Machines, ce qui est contradictoire.

En outre, dans la mesure où les données sont très importantes pour le développement de l’IA, il est nécessaire d’articuler la proposition de règlement sur l’IA avec les textes européens relatifs aux données préexistants ou en cours de discussion (RGPD, Data Governance Act, Data Act) afin d’assurer la complémentarité de ces textes et de veiller à l’absence de contradictions ou à la superposition d’obligations similaires (accès et partage des données, obligations en termes de fiabilité des données ou de profilage, sanctions…).

Au niveau national, il existe aujourd’hui de nombreuses instances et autorités françaises agissant dans le domaine du numérique et, avant de désigner une nouvelle autorité en charge de l’IA, il serait souhaitable, d’une part, de mieux définir les missions de cette nouvelle autorité à l’aune des enjeux de compétitivité internationale, d’innovation et de traitement des données (personnelles ou industrielles) et, d’autre part, de clarifier le rôle de chacune de ces autorités déjà en place et éventuellement de repenser le périmètre de ces instances.

Il est, de surcroît, essentiel que les décisions et doctrines des autorités de contrôle nationales soient harmonisées afin d’éviter les risques de fragmentations dans l’application du règlement.
",PUBLISHED,medium,Nicolas,COM(2021)206,2021-08-05 14:51:18,withinfo,24212003,closed,30032231266-82,,
en,2665444,ITA,company,Mediaset Italia S.p.A.,,see attached file,PUBLISHED,large,,COM(2021)206,2021-08-05 14:43:54,anonymous,24212003,closed,91471238809-21,,
en,2665443,NLD,company,Liberty Global,Kolthek,"Liberty Global welcomes the opportunity to comment on the European Commission (EC)’s consultation of the proposal for a Regulation laying down harmonized rules on artificial intelligence (the proposal) and its annexes.

In previous submissions, Liberty Global supported the EC’s comprehensive evaluation of the regulatory environment applicable to nascent digital technologies such as robotics, Internet of things and particularly AI. Liberty Global agrees with the staged approach taken by the EC towards AI, as well as the EC’s ultimate objective of ‘fostering the development and uptake of safe and lawful AI that respects fundamental rights across the Single Market by both private and public actors while ensuring inclusive societal outcomes’. We also support the twin objective of promoting the uptake of AI and of addressing the risks associated with certain uses of this new technology, as laid down by the EC in its AI White Paper. Striking the right balance is key to achieving an optimal outcome. 

In Liberty Global’s view, the proposal succeeds in striking the right balance, meeting the principles of appropriateness and proportionality. The success of the proposal depends – to a large extent – on whether the scope is left intact throughout the further steps in the legislative process. The current, broad scope, can ensure a uniform regulatory approach in the EU towards AI and avoid barriers to the single market, arising out of potential regulatory fragmentation. 

In Liberty Global’s view, the risk-based approach foreseen by the proposal is one of its strengths, and will be a key contributor to the achievement of its objectives. Manipulative and other harmful applications of AI should indeed be subject to a blanket ban. Low-risk AI can benefit from predominantly supporting and voluntary measures. In respect of high-risk AI, Liberty Global notes that the proper identification of high-risk AI and its harmonized regulation are critical to the EC’s twin objectives. The proposal ensures maximum regulatory clarity in this respect. All relevant high-risk AI applications have been identified. Further expansion of the scope is not necessary. Maintaining a clear separation between these different regulatory regimes ensures regulation in line with the principles of appropriateness and proportionality.  

Liberty Global also agrees with the clear identification of the roles and responsibilities of the different actors in the value chain, as well as how to identify which party has what role and what actions may trigger a role shift. It is clear that, if a party further on in the value chain substantively alters an AI system, or changes its purpose, this party will be subject to the same responsibilities as the original AI provider. This adequately addresses the risks that compliant AI systems are altered, or their purpose is modified, after they have been placed on the market, without subjecting the entire value chain to significant regulatory or administrative burdens where this would not be appropriate, nor proportionate. 
Liberty Global supports the proposal’s leveraging of the existing principles of EU product conformity law, as well as references made to Union harmonisation legislation based on the New Legislative Framework in order to come to a clear and consistent regulation of products and services.   

The proposal ensures that the EU can be leading in the development and uptake of ethical AI. As a provider of next-generation connectivity across Europe, Liberty Global will be one of the key contributors to the uptake and accessibility of a broad spectrum of AI applications. Therefore, we would like to emphasize the importance of the completion of the Digital Single Market, also for the democratization and wide-spread deployment of AI in the EU. Finally, as set out in our paper on the role of AI in telecommunications (attached), the Code provides the appropriate framework for the use of AI in our sector.",PUBLISHED,large,Robert,COM(2021)206,2021-08-05 14:27:54,withinfo,24212003,closed,82853397708-89,,
en,2665442,BEL,company,Huawei Technologies ,Dedopoulou,"Huawei welcomes the opportunity to provide feedback on the European Commission’s proposal for an Artificial Intelligence Act (hereinafter referred to as the “AI Act”), a crucial piece of legislation which will support the EU in fulfilling its digital ambitions, foster the single market, and consolidate Europe’s position as a leader in the digital sphere.

We strongly support the Commission’s objective to lay down harmonised rules for AI systems, in line with the guidelines presented by European Commission President Ursula von der Leyen to support the EU in being a global leader in the development of secure, trustworthy and ethical artificial intelligence. We are happy to see that this proposal is putting forward the much-needed legal framework addressing the risks of artificial intelligence. At Huawei, we believe AI developers, deployers and users would greatly benefit from such legal clarity, which in turn would ensure the wider take-up of artificial intelligence across Europe.

We recognise the Commission’s success in producing the first ever draft legal framework on artificial intelligence, and we believe the proposal would benefit from further deliberations. We are pleased to participate in the discussion around this innovative proposal by submitting our feedback to the public consultation, and we very much look forward to working with the Commission and co-legislators to attain a fair, clear and practical outcome that works for all.

Our comments are structured along the following points:

1. Definitions concerning AI system
1.1 Definition of AI system
1.2 Definition of safety component of a product or system
1.3 High-risk AI systems

2. Requirements for high-risk AI systems
2.1 Data governance
2.2 Post-market monitoring and surveillance
2.3 Incidents reporting
2.4 Information sharing

3. Standards and organizations
3.1 Harmonised standards and common specifications
3.2 Organizations

4. Others
4.1 Codes of conduct
4.2 Digital skills

Please find more detailed feedback in the attached document below.",PUBLISHED,large,Angeliki,COM(2021)206,2021-08-05 14:24:31,withinfo,24212003,closed,114467111412-38,,
en,2665441,DEU,trade_union,SEMI EUROPE,Kysela,"SEMI Europe welcomes the European Commission’s “Proposal for Regulation laying down harmonized rules on Artificial Intelligence”  and appreciates the opportunity to provide its feedback on this crucial matter for Europe’s microelectronics industry.

With the rise of Artificial Intelligence (AI), the European Union (EU) remains determined to play a leading role in setting a global standard in the responsible development and use of AI technologies. The microelectronics industry supports such efforts but with a key caveat. While the industry acknowledges that development and deployment of AI requires a coordinated approach, it asserts that AI systems and related technologies should be regulated where possible by existing EU laws and regulations, including sector-specific ones. By building on already existing frameworks such as the EU Machinery Directive and the Product Liability Directive, appropriate rules concerning the safety and reliability of novel AI systems can be introduced without creating additional regulatory and administrative burdens on AI developers, in particular SMEs. 

Building on Europe’s competitiveness and strategic position in the global electronics supply chain, sound, industry-backed approach to regulate AI should be the cornerstone of Europe’s technological ambitions. Only by creating the right regulatory environment, Europe can seize new growth opportunities arising from the next wave of AI generated in connected devices and technologies at the edge.

SEMI Europe has submitted a pdf document to make policy recommendations pertaining to Europe’s new approach to AI for the consideration of the European Commission, the Parliament and the Council.
",PUBLISHED,medium,Marek,COM(2021)206,2021-08-05 14:01:08,withinfo,24212003,closed,,,
el,2665440,GRC,ngo,Ηοmo Digitalis ,Chelioudakis,"Παρακαλούμε όπως βρείτε συνημμένη την τοποθέτηση της Homo Digitalis  στην ανοιχτή διαβούλευση της Ευρωπαϊκής Επιτροπής για τις διατάξεις του προτεινόμενου κανονισμού για την Τεχνητή Νοημοσύνη. 
Με εκτίμηση,
Η συντακτική ομάδα της Homo Digitalis
",PUBLISHED,micro,Eleftherios,COM(2021)206,2021-08-05 14:01:08,withinfo,24212003,closed,611935634267-89,,
en,2665439,ESP,company,Banco Bilbao Vizcaya Argentaria S.A.,LOZANO BELIO,"We welcome this opportunity to provide feedback to this important regulatory proposal that will undoubtedly influence the future development of AI in the EU. Although the proposed Regulation is quite balanced, we have identified some areas where there is room for improvement:
Legal uncertainty. Some provisions in this draft regulation are not specific enough, which could give rise to different interpretations by authorities and market operators and, therefore, create legal uncertainty and hinder the compliance with this Regulation.
Definitions are one of the main sources of uncertainty, either due to their vagueness or their absence.
In particular, the definition of AI proposed in Article 3 and the techniques listed in Annex I can bring any automatic system or system automating some basic tasks in scope. Other terms such as ‘natural persons’, ‘output’ or ‘(unfair) bias’ should be defined in order to make clear what is intended to be solved and ensure that the final text applies to any legal or natural person leveraging high-risk AI systems for their business activity.
With reference to the scope of the Regulation, the Commission should clarify that use cases such as collections, Anti-Money Laundering controls and biometric identification on device are not considered high-risk.
In any case, when some ambiguity remains and interactions with other regulations such as GDPR exist, guidance, recommendations or binding opinions to be developed by the Commission or the European Al Board seem paramount.
Supervision and governance. It is highly important to ensure that non-financial firms competing with credit institutions and providing or using the same high-risk AI systems as financial entities are subject to similar regulatory and oversight requirements.
Also, in order to ensure a level playing field between companies providing and using AI systems for the same purpose, the opinions and recommendations issued by the European AI Board should be binding for national authorities and be based on the criteria established by the authority(ies) with the highest expertise on each particular activity.
Finally, APIs should not be imposed as a mechanism for authorities to exercise their access rights, since this would probably require large investments without bringing significant benefits to market surveillance.
Bias and discrimination detection. We would prefer the Regulation referred to ‘unfair bias’, not bias, and a definition of the term were provided. Additionally, requirements on the examination of possible biases and the identification of any possible data gaps or shortcomings should be realistic and take into consideration the current state of the art.
Regarding the provision of a legal base for processing special categories of personal data for detecting and correcting biases, we think it is not necessary as this is not the ‘silver bullet’ of bias elimination and could reduce citizens’ trust on AI technology.
Requirements on high risk AI systems. Some of the proposed requirements are not sufficiently specific or do not reflect the actual state of the art.
For instance, it is not clear if full automation of some processes embedding AI would be possible, and criteria on how to decide the type of human oversight that is needed are not provided.
Moreover, further clarity is needed on what is meant by requiring the data has “appropriate statistical properties”, the scope of the technical documentation to be provided or what is understood by a substantial change in a system.
On the other hand, record keeping obligations should be more flexible and acknowledge that logging capabilities are not the only option to satisfy them.
We attach a document where we explain in more detail the above issues
",PUBLISHED,large,Jesús,COM(2021)206,2021-08-05 13:24:13,withinfo,24212003,closed,090350412968-04,,
en,2665438,DEU,company,Merck KGaA,Mertens,Please see attached document.,PUBLISHED,large,Tim,COM(2021)206,2021-08-05 13:16:17,withinfo,24212003,closed,49654992078-52,,
en,2665437,DEU,ngo,"ThinkTech, e.V. ",Galvagna,"Please see the attached document for a full analysis. Below is a summary of this analysis. 

The European Commission’s proposal for a regulation on artificial intelligence is motivated by two worthy goals: promoting technological innovation and protecting fundamental rights. However, elements of the proposal undermine these goals. Ambiguous key terms would result in legal uncertainty, which in turn would undermine investment and innovation, while creating opportunities for human rights violations. Hypothetically, harmonised standards would offer legal certainty by defining these terms technically and providing presumptive compliance for systems that implement them. However, the proposal does not ensure that standard-setting bodies will be sufficiently responsive, democratic, or rights-protective. Also, the legal safeguards outlined for biometric surveillance methods fall short of what is needed to prevent human rights violations, and unrealistically assume Member States and companies will exercise sufficient self-restraint to make up the difference. 

More compliance support for providers would ensure greater legal certainty and less room for human rights violations. Amendments to this proposal and related legislation should ensure that harmonised standards and common specifications are issued speedily, designed with stakeholders representing a more complete range of affected interests, and, in particular, stakeholder groups with human rights legal expertise. In place of, or in addition to, harmonised standards, providers of all high-risk systems should have access to third-party conformity assessment, rapid and complete Member State compliance advice, and a user-friendly virtual compliance tool.

Also, large-scale biometric identification, categorisation, and emotion recognition systems should generally be prohibited for state and business uses, or at least limited to combating serious violent crimes. 
",PUBLISHED,small,Christine,COM(2021)206,2021-08-05 13:05:19,withinfo,24212003,closed,,,
de,2665436,DEU,business_association,Verband der TÜV e. V.,Kröhnert,"Der TÜV-Verband begrüßt die Initiative der Europäischen Kommission, ein verbindliches Rahmenwerk zur Regulierung von Systemen, die auf dem Einsatz Künstlicher Intelligenz basieren, zu schaffen. Ein robuster Regulierungsrahmen ist die Grundvoraussetzung für das notwendige Vertrauen der Menschen in KI-basierte Produkte und Systeme und damit die Akzeptanz dieser neuen digitalen Technologie.

Aus Sicht des TÜV-Verbands ist der vorliegende Regelungsentwurf jedoch nicht hinreichend ambitioniert und bleibt hinter dem eigenen Anspruch der EU-Kommission zurück, ein „Ökosystem für Vertrauen“ zu schaffen. Ein Ökosystem für Vertrauen kann nur durch ein Primat der Sicherheit von KI bei der Ausgestaltung des Regulierungsrahmens geschaffen werden. Der Kommissionsentwurf bedarf folgender Nachbesserungen:

- Risikoklassen nachvollziehbar herleiten und effektiven Rechtsgüterschutz priorisieren
- Unabhängige Drittprüfung bei high-risk KI-Systemen durchgehend vorsehen
- Risikoadäquate Klassifizierungsvorschriften für high-risk Anwendungen einführen
- Risiken für schützenswerte Rechtgüter müssen alleiniger Maßstab zur Ergänzung der Liste der high-risk KI-Systeme sein
- Einspruchsmöglichkeiten gegen Entscheidungen notifizierter Stellen konkretisieren und europaweit einheitlich regeln

Beigefügt finden Sie unsere Stellungnahme, in der wir unsere Verbesserungsvorschläge umfassender erläutern. 
",PUBLISHED,small,Johannes,COM(2021)206,2021-08-05 13:03:48,withinfo,24212003,closed,45013506457-28,,
en,2665433,BEL,company,Splunk,Lemaire,"The development of artificial intelligence has major potential to improve business processes and services for citizens, but there are a number of complex issues associated with its adoption. Splunk supports a flexible policy framework that builds confidence and trust in AI systems, encourages investment in research and development, strengthens cybersecurity and privacy protections, and takes into account different types of AI and machine learning to avoid hampering innovation unnecessarily.  

With its strong risk-based approach, the EU’s proposal for an AI Act is largely in line with these principles. We welcome the focus on regulating high-risk AI systems and the proposal’s approach to conformity assessments (done through internal checks for all high-risk AI except remote biometric identification). We do however have some concerns about the sharing of responsibilities between AI providers and AI users and about the exact nature of requirements introduced for high-risk AI systems.  

For more details about Splunk's position, see attached position. ",PUBLISHED,large,Clara,COM(2021)206,2021-08-05 12:21:26,withinfo,24212003,closed,676974536971-63,,
en,2665432,BEL,ngo,BEUC - The European Consumer Organisation,SILVA,Please find BEUC's response to the consultation attached.,PUBLISHED,small,Frederico,COM(2021)206,2021-08-05 12:18:47,withinfo,24212003,closed,9505781573-45,,
pl,2665431,POL,business_association,Związek Pracodawców Business & Science Poland ,Matyba,"Business & Science Poland (BSP) welcomes the possibility to comment on the proposal 
for a Regulation of the European Parliament and of the Council on laying down harmonized rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain union legislative acts (COM (2021) 206 final). We represent leading Polish entities particularly from energy-intensive industry, air transport, cyber and IT sector, postal services, and financial markets employing over one hundred thousand employees in Poland, other European Union (EU) the Member States, and outside the EU. BSP is also in a partnership with R&D, academic, and SME organizations. Topics related to Artificial Intelligence are within the scope of our interests.
Please find attached all of our comments due to regulation.
",PUBLISHED,small,Natalia,COM(2021)206,2021-08-05 12:18:41,withinfo,24212003,closed,548212735276-89,,
en,2665430,BEL,other,Vrije Universiteit Brussel (VUB) and CY Cergy Paris University,Kiseleva,"While generally appreciating and supporting the AI Act, I suggest six topics that invite  for additional considerations and hopefully, might help making the AI Act even better. These topics are: 
1.	THE ROLE OF THE SUBJECTS WHO ARE AFFECTED BY THE DECISIONS MADE WITH THE USE OF AI SYSTEMS SHALL BE DEFINED 
2.	THE CONCEPT OF AI USER SHALL BE CLARIFIED (ESPECIALLY IN MULTI-STAKEHOLDERS ENVIRONMENTS)
3.	‘TRANSPARENCY’ TERMINOLOGY SHALL BE USED CONSISTENTLY 
4.	‘TRANSPARENCY-INTERPRETABILITY’ REQUIREMENT IN ARTICLE 13 SHALL BE CLARIFIED  
5.	THE HIERARCHY BETWEEN TRANSPARENCY AND THE RELATED CONCEPTS SHALL BE DEFINED - TRANSPARENCY AS A LEGAL PRINCIPLE AND THE OTHER CONCEPTS AS THE MEASURES SUPPORTING THE PRINCIPLE 
6.	CLARIFICATION ON THE CONCEPT OF ‘BIAS’ IS NEEDED 

I am attaching the document that explains all the topics in details. 
The expressed position is my individual position and does not present the official position of affiliated organisations (Vrije Universiteit Brussel and CY Cergy Paris University).
",PUBLISHED,large,Anastasiya,COM(2021)206,2021-08-05 12:11:56,withinfo,24212003,closed,,,
en,2665425,DEU,consumer_organisation,Federation of German Consumer Organisations (vzbv),Blinn,"vzbv welcomes that the European Commission proposes a regulation laying down har-monised rules on artificial intelligence (AI) in the (Artificial Intelligence Act (AIA)). The AIA must mitigate AI-related risks for consumers. To achieve this goal and accomplish the European Commission’s stated objective of a ‘trusted AI’, vzbv recommends to in-crease the focus on consumers of the AIA and to strengthen the possibilities for inde-pendent assessments of high-risk AI systems.",PUBLISHED,medium,Miika,COM(2021)206,2021-08-05 10:49:48,withinfo,24212003,closed,2893800753-48,,
en,2665424,ESP,company,Glovo,Gurman,"Please find Glovo's contribution attached 

Glovo’s contribution to the public consultation on the Artificial Intelligence 

Glovo welcomes the European Commission’s proposal for an Artificial Intelligence Act as this represents an opportunity for the EU to make the best of AI applications and a step forward towards more innovation and algorithmic transparency. Our response to the public consultation will outline Glovo’s role in AI innovation as well as recommendations for specific elements of the proposal to be improved or maintained.
About Glovo 

Glovo is a Spanish start-up active in 23 countries and +900 cities that aims to give everyone easy access to anything in their city. Glovo is the “everything app” that connects customers, shops and couriers - one of Europe’s largest and one of Spain’s two “unicorns”. Since its birth in 2015 in Barcelona, it has grown to operate in Europe, Asia and Africa, helping European technology and industry compete successfully on the global market and at home.

Glovo has assisted in the digitization of various trade verticals, such as restaurants, supermarkets, bookstores, florists, clothing stores, accessories, toy stores, stationery stores, chocolate shops, cosmetics stores, among others. Glovo provides all these operators with solutions based on data, algorithms and digital management that are of enormous value for their growth.",PUBLISHED,large,Magali,COM(2021)206,2021-08-05 10:34:58,withinfo,24212003,closed,,,
en,2665421,POL,company,The Polish Confederation Lewiatan,Dziuba,Please find enclosed comments of the Polish Confederation Lewiatan.,PUBLISHED,medium,Elżbieta,COM(2021)206,2021-08-05 09:26:02,withinfo,24212003,closed,,,
en,2665420,BEL,business_association,European Savings and Retail Banking Group,SCHONENBERGER,Please find attached the ESBG feedback on the European Commission's draft Artificial Intelligence Act.,PUBLISHED,small,Michelle,COM(2021)206,2021-08-05 08:54:46,withinfo,24212003,closed,8765978796-80,,
en,2665416,JPN,company,NEC,Mori,Please find attached NEC’s feedback on the EU Draft AI Regulation,PUBLISHED,large,Ukyo,COM(2021)206,2021-08-05 06:14:40,withinfo,24212003,closed,,,
en,2665415,JPN,business_association,Japan Electronics and Information Technology Industries Association(JEITA),Suzuki,"As Japan’s leading ICT association, JEITA supports the European Commission’s will to adopt risk-based approach and harmonize the Regulation with the existing EU  legislations. Our views are as below and attached position paper.

1. Basic approach 
• Assessment of AI risk 
A growing number of cases have proven that machine can provide sufficiently low accident rates in area such as automated driving and medical systems. Based on the concept of risk-based approach, any new regulations should be designed with risk assessment of use of AI and should not be unnecessarily restrictive. 
•  Balancing innovation and regulation
In terms of ensuring sound market formation and making the EU a global AI development hub, over-application of conventional regulatory instruments such as third-party conformity and CE marking will slow rollout and dissemination. The raft of pre- and post-market requirements for high-risk AI in the proposed Regulation impose heavy compliance costs (time, labor) each time a new technology is introduced or an upgrade is undertaken and could reduce incentive to invest in Europe and hamper the development of AI startups. 

2. Views on particularly important articles
The following revisions and clarifications should be made to boost the stable implementation  and predictability of the Regulation.

•  Clarification of scope of prohibited AI and high-risk AI and grounds for risk 
- The key aspect for remote biometric identification is not physical distance but the targeted individual awareness; the definition should be adjusted accordingly. 
- Regarding the data governance, in case the template protection is applied to anonymize the biometric data, any sensitive personal data such as gender, ethnicity or age cannot be inferred. Such discrimination-free AI systems should be exempted from high-risk classification.
- In cases where AI systems demonstrate sufficient safety and fairness under existing laws (medical equipment to which EU-MDR (EU2017/745) already applies, key infrastructure where failsafe, fault tolerance, and other systems have been designed to eliminate risk, and where the risk has been demonstrated to be sufficiently low using AI regulatory sandboxes, etc.), AI systems should be exempted from high-risk classification or the requirements eased, and operational consistency with related sector regulations ensured.   

•  Requirements for prohibited and high-risk AI based on use case 
The regulatory scope and application of requirements to AI systems should be determined based on the risk for the particular use case. It will be critical to create effective AI system guidelines in conjunction with the relevant industries, including providing typologies by use case.

• Responsibilities between AI system providers and users 
The appropriate division of responsibilities between providers and users must be explicitly stated here to require appropriate use by and cooperation from users and management also of data provenance, while also ensuring consistency with the approaches to the division of responsibilities in existing laws and regulations such as the GDPR.

• AI governance through multi-stakeholder and international collaboration 
Detailed consideration in relation to enforcement of this Regulation by the EAIB, etc., Annex updating, and assessment and review following implementation of the Regulation, must involve multiple stakeholders, including industry and AI experts, so as to be effective. International collaboration, Japan included, should be necessary to establish and maintain a global governance mechanism which ensures equitable conformity assessment that does not obstruct market participation.

•  Adjustment of penalties 
More detailed and accurate information must be provided in relation to penalty standards and scope. Sufficiently long periods on retention of documents/materials necessary or used to assess compliance with the Regulation should be provided (longer than for logs in Article 20, etc.).
",PUBLISHED,medium,Hisaki,COM(2021)206,2021-08-05 02:11:14,withinfo,24212003,closed,,,
en,2665411,DEU,business_association,VDMA,PETERS,"The Mechanical Engineering Industry Association (VDMA) appreciates the opportunity to provide feedback to the consultation on the proposal for the Artificial Intelligence Act. For our companies, AI is a key technology for competitiveness and sustainability. Our export-oriented sector brings production equipment with embedded AI to a variety of industrial customers in EU and worldwide. 

VDMA, therefore, supports the goal of creating a harmonized EU-framework for AI, which is a prerequisite for catching up with global competition in AI and protecting fundamental rights. In general, we welcome the graduated approach that differentiates according to risks and avoids a “one size fits all”-regulation. We also support the envisaged use of the proven and technology-neutral principles of the ""New Legislative Framework"" (""NLF"") and harmonized standards. In our view, however, the AI Act must be fundamentally streamlined by focusing more strictly on the protection of fundamental rights, excluding already regulated areas (such as industrial applications) and formulating less-prescriptive requirements. The AI Act should leave more room for risk-based and use-case-oriented implementation of the requirements and leave descriptions of technical details and state of the art to standardization. The introduction of an ""autonomy criterion"" should be considered to avoid regulating non-decision-making AI functions. 

The broad high-risk classification and the depth of detailed technical prescription go too far to be considered a fully risk-based and innovation-friendly approach. Because of the generally broad definitions and criteria, there is a risk that the act will cover many AI-applications that are actually no “high risk” and that should not fall under the protective goals of the AI-Act – either because the actual decision-making autonomy of the system is low or the risks are already regulated. There is no evidence that AI in industrial applications is leading to substantial new risks which are not yet covered by already existing legislation. Consequently, the proposed law has the risk of creating an overly prescriptive legal framework with an extensive coverage that will hamper AI innovation in the EU. 

The aims of creating a horizontal law and at the same time integrating existing legislation have resulted in a proposal that is in principle purposeful, but also very complex. Not least the multitude of annexes and references to other regulations shows this complexity, which will be a challenge for companies and authorities. With a view to the desired technological sovereignty of the EU, AI must not be over-regulated. We, therefore, call upon the EU legislators to shape a reliable and less complex legal framework, which protects human rights and encourages the use of AI by keeping the burden for AI-developers and users reasonable. 

Our detailed assessment and comments are described in the position paper, which has been submitted together with this feedback. 
",PUBLISHED,large,Kai,COM(2021)206,2021-08-04 22:53:06,withinfo,24212003,closed,976536291-45,,
en,2665410,USA,business_association,Developers Alliance,Stan,"Developers Alliance welcomes the opportunity to provide feedback on the AI Act proposal.

We commend the objectives and the risk-based approach of the proposal. We call on the EU co-legislators, however, to address a series of critical issues so the regulation will be fit for purpose and to reduce the competitive disadvantage for European developers:
- set legally clear definitions and limit the scope to clearly defined high risk use cases of AI, in line with the declared objectives of the regulation,
- provide clear and reasonable rules for high risk use cases of AI, focused on the deployment phase, and less preemptive requirements for the development phase when the intended purposes might not be obvious (as in the case of general-purpose and open-source AI solutions),
- ensure reduced regulatory burdens for startups and SMEs and set up a solid framework for regulatory sandboxes as incentives for innovation and entrepreneurship,
- remove the proposal’s extraterritorial reach to avoid potential barriers to trade on products and services created outside the EU that contain no AI themselves,
- recognise that AI systems are by definition modelled on human decision making, with all their weaknesses and strengths. Holding AI to an absolute standard where an equivalent human process is viewed under a reasonableness standard is legally inconsistent.

Please find attached our detailed position paper.",PUBLISHED,micro,Karina,COM(2021)206,2021-08-04 22:39:34,withinfo,24212003,closed,​135037514504-30,,
en,2665406,DEU,ngo,AW AlgorithmWatch gGmbH,Aszodi,"AlgorithmWatch welcomes the European Commission’s efforts to develop a framework for the governance of AI-based systems that is based on European values and the protection of fundamental rights. However, we fear that in its current version, the draft Artificial Intelligence Act would not reliably and comprehensively reach the objectives it is intended to. We therefore call upon the Council and the Parliament to take appropriate measures to correct its shortcomings, clarify its ambiguities, and enhance its consistency, ultimately turning the Act into an effective means for using AI-based systems to the benefit of people—and not to their detriment. 

The Commission’s proposal will profoundly shape AI regulation in the next decades, not only in Europe but across the globe—through its direct legal extraterritorial effects as well as through its political implications. We appreciate the agenda-setting character of the proposal, which will likely stimulate the urgently needed debate on the governance of ADM systems. The fact that the EU is now proactively engaging in and promoting this debate presents an opportunity—both within the EU and beyond—for the development of consistent governance approaches. 

As to its overall substantial approach, whereas the White Paper’s narrative suggested a worrisome reversal of EU priorities, putting global competitiveness ahead of the protection of fundamental rights, the new proposal sets out with the prohibition of AI practices it declares to be in breach with Union values and fundamental rights protected under Union Law. At the same time, the draft Act is a complex piece of regulation with a variety of interdependencies with already existing norms and it is not easy to foresee the effects it is going to have and the ways it is going to interact with these other regulatory frameworks. At closer inspection, many of the instruments it proposes turn out to remain vague or risk becoming teethless or even self-defeating in practice, which very much contradicts the idea of a regulation that aims to increase legal certainty and that puts fundamental rights first. 

As AlgorithmWatch, we oppose the narrative that citizens’ trust serves as a means to innovation, implicitly classifying the latter as the ultimate end. While we very much agree with the importance and recognize the potential benefits of innovation, we regard individual autonomy and the common good as the ultimate benchmarks against which to set legal standards. It is now up to the Parliament and the Council to clarify the Act’s ambiguities, correct its shortcomings, and to enshrine reliable safeguards. 

In the attached position paper, you find our comments and recommendations on selected issues in more detail.
",PUBLISHED,small,Nikolett,COM(2021)206,2021-08-04 21:34:23,withinfo,24212003,closed,,,
en,2665403,BEL,business_association,ACCIS,Velazquez,"The Association of Consumer Credit Information Suppliers (ACCIS) represents the largest group of credit reference agencies in the world. ACCIS brings together 40 members across 28 European countries and 11 associate and affiliate members from all other continents.

Building on the vast experience of the credit reference industry with data management and data analytics, ACCIS acknowledges the Commission’s efforts to address both the benefits and challenges of AI in its draft regulation laying down harmonized rules on this technology. We believe, however, that more work is needed to strike the right balance between promoting innovation and protecting European citizens and their rights. We fear that the current draft adopts a disproportionate approach that could curtail the use of socially beneficial applications of AI/ML, in particular as regards creditworthiness assessments and credit scoring. 

We have identified three main critical issues in the draft Act and recommended solutions: 

1.	the definition of AI systems should be narrowed so that it does not include low risk, understandable and explainable techniques such as logistic regression; 
2.	AI for credit scoring and creditworthiness assessments should not be deemed high-risk because borrowers are already adequately protected by existing legislation and regulations; 
3.	the concept of creditworthiness should be clarified; 

We would like to also share our views and recommendations on three additional issues:
 
4.	the proposed obligations on providers and users of AI systems intended to be used to evaluate the creditworthiness of natural persons or establish their credit score are not appropriate;
5.	the role of voluntary codes of conduct should be strengthened; and
6.	the governance framework should ensure harmonized supervision and enforcement of obligations.

Please read more detailed comments in the attached file.",PUBLISHED,micro,Enrique,COM(2021)206,2021-08-04 20:43:28,withinfo,24212003,closed,21868711871-63,,
en,2665397,GBR,academic_research_instittution,"Centre for Commercial Law, University of Aberdeen",,Please find attached our feedback to the consultation on the proposal for a regulation laying down harmonised rules on Artificial Intelligence. This response is submitted on behalf of the Centre for Commercial Law of the University of Aberdeen.,PUBLISHED,small,,COM(2021)206,2021-08-04 19:37:54,anonymous,24212003,closed,,,
en,2665395,SWE,company,Confederation of Swedish Enterprise,Brånby,"Confederation of Swedish Enterprise represents 49 sector-member organisations and 60,000 member companies and would like to state the following in light of the European Commission’s consultation on the artificial intelligence act, AIA.

Confederation of Swedish Enterprise  
-	welcomes the approach to protecting the users of AI systems and creating trust in the use of AI
-	criticize the proposal as it has extensive negative consequences for competitiveness, innovation, and entrepreneurship; the consequences of the proposal need to be investigated in detail
-	emphasize that investments and compliance are ensured by clear, principle-based and technology-neutral rules that last over time
-	advocates the use of NLF principles and a legal framework limited to necessary requirements that leave technical implementation to product-specific and updated (state-of-the-art), voluntary standards developed by stakeholders.
-	requests compliance with the Better regulation agenda: make simpler and better EU laws with special attention to the implications and costs of applying legislation
-	believes that the proposal has a negative impact on the freedom to conduct business and employers relevant AI use
-	states that AIA overlaps with other regulations and needs to be harmonised with the Data Protection Regulation, GDPR, regarding data processing and storage
-	proposes more extensive initiative for regulatory sandboxes and experimentation to promote test opportunities for innovative solutions in high-risk AI areas
-	questions the unjustifiably high levels of sanctions
-	would like to point out the considerable need for AI experts and staff with AI skills that must be addressed before the proposal enters into force

The use of AI solutions is important for competitiveness, security, good customer experience and the provision of effective and correct service for citizens. The European Commission’s proposal for the AIA, is based on the concept of trust, which is vital to realising the increased use of AI. To strengthen confidence in AI applications and promote rapid technological development, the use of ethical guidelines, certifications, and codes of conduct should be encouraged and used as much as possible.

There is significant concern amongst our members that the draft AIA is far too comprehensive and vague to boost AI use and EU competitiveness. The proposal is difficult to apply, disproportionate and not technology neutral. The proposal needs stronger innovation and development approaches and stronger ambition to strengthen the competitiveness of European companies. In addition, the proposal entails increased costs for companies and governments. 


Please read further comments in the attached file.",PUBLISHED,medium,Carolina,COM(2021)206,2021-08-04 19:23:48,withinfo,24212003,closed,,,
pl,2665383,POL,company,Związek Cyfrowa Polska,Kanownik,"W związku z prowadzonymi konsultacjami w sprawie w sprawie Rozporządzenia Parlamentu Europejskiego i Rady Ustanawiającego zharmonizowane przepisy dotyczące sztucznej inteligencji (Aktu w sprawie sztucznej inteligencji) i zmieniającego niektóre akty ustawodawcze Unii z dnia 21.4.2021 r., przedstawiam w załączeniu stanowisko Związku Cyfrowa Polska, reprezentującego polski sektor cyfrowy i nowych technologii.",PUBLISHED,micro,Michal,COM(2021)206,2021-08-04 18:24:46,withinfo,24212003,closed,,,
en,2665345,AUT,consumer_organisation,Kammer für Arbeiter und Angestellte für Wien,Ernst,"Dear Madam, Dear Sir,

Enclosed you will find the statement from the Vienna Chamber of Labor.

The Federal Chamber of Labor (BAK) is the legal representation of the interests of around 3.7 million employees and consumers in Austria. It represents its members in all social, educational, economic and consumer policy matters at national and EU level. In addition, the BAK is part of the Austrian social partnership. 

Chamber for workers and employees for Vienna
Prinz-Eugen-Strasse 20-22
1040 Vienna
Austria

Kind regards
",PUBLISHED,large,Brigitte,COM(2021)206,2021-08-04 15:31:56,withinfo,24212003,closed,23869471911-54,,
en,2665331,BEL,company,AstraZeneca,,"AstraZeneca (AZ) welcomes the proposed draft Artificial Intelligence (AI) Act and believes that harmonised EU rules on AI can pre-empt a possible fragmentation of the single market and foster the safe and responsible development, use, and uptake of AI in the European Union. 

Tightening the definition of ‘AI’ and ‘Systems’ to exclude legacy activities: The language used to define AI in the draft legislation is very broad and includes all statistical computations and analyses. Like many of its peers in the biomedical, pharmaceutical, and healthcare sector, AZ is a science- and data-driven business. AZ has routinely used the scientific method, statistical analyses, and computational methods for decades. Applications include biological discovery science, the enabling of clinical trials, interventional research, drug product approval, and post-marketing activities. The currently proposed inclusive definition of AI would encompass a large portion of the daily work of many biomedical and pharmaceutical companies. These activities were historically neither thought of or classed as ‘AI’ nor identified as conferring additional risks (often to the contrary). 

We would appreciate further dialogue on a definition of AI that does not capture legacy activities and takes into consideration existing requirements in the field of diagnostics and medical products.  Similarly, further guidance on the definition of a ‘system’ would be beneficial to data-driven industries to understand what activities are in and out of scope for this regulation. 

AZ’s AI applications: 
- AI systems put into service for own use: internal systems that include machine learning, language processing, and decision assistance and automation using AI. The developers of our AI solutions are often also the end-users 
- Upstream use cases, where sector-specific processes or regulations mitigate risk: include an AI modelling tool that predicts compound safety and efficacy prior to adding the compound to the company portfolio
- Medical software products developed using or containing AI: use of AI and machine learning technologies to enhance the medical device development process and indeed as a core component of the medical device itself. We have developed quality management systems for medical device design, development, and manufacturing and have recently endeavoured to supplement these systems with good machine learning practices. 
- AI systems that support patient stratification e.g. diagnostics
- Facial Recognition for employee and site security

Determining AI risk categories: AZ supports the premise of the proposal that in the health sector where public health is paramount, AI supporting human decisions must be reliable and accurate. 
The proposal lays down a risk methodology to define “high-risk” AI systems and the European Council has called for a clear determination of those AI applications that should be considered high-risk. AZ’s supports this approach to ensure legal certainty and facilitates innovation and continued investments in AI technologies. We also strongly encourage a proportionate application of all relevant requirements.
Our reading of the proposed Article 6 in conjunction with Annex II and Annex III of the draft Artificial Intelligence Act suggests a limited amount of AZ’s current applications of AI would be categorised as high-risk, including the following:
- Software as a medical device;
- AI systems acting as or supporting diagnostics (e.g. that support patient stratification); and
- AI systems used in facial recognition for employee and site, and information security.

Codes of Conduct: AZ develops and uses AI technology in line with its Principles for Ethical Data and AI as well as its Code of Ethics and Ethics & Transparency Commitment under its Sustainability Ambition. AZ is committed to review its Principles against relevant requirements incorporated in the final AI Act.",PUBLISHED,large,,COM(2021)206,2021-08-04 14:19:41,anonymous,24212003,closed,249402638316-62,,
nl,2665329,NLD,other,Netherlands Normalisation Institute (NEN),Meuleman,The Medical Device / AI Expert Group (MD-AIG) established by Netherlands Normalisation Institute (NEN) welcomes the European Commission’s (EU) proposal for a Regulation laying down harmonised rules on Artificial Intelligence to ensure the safety and security of European consumers in the area of Artificial Intelligence. Attached document provides full feedback on the AI Act seen from the medical device sector perspective and an analysis of the AI Act in light of the Medical Device Regulation.,PUBLISHED,large,Lysette,COM(2021)206,2021-08-04 13:51:28,withinfo,24212003,closed,,,
en,2665323,FRA,company,Laboratoire national de métrologie et d'essais (LNE),LOMMATZSCH,"In the context of the European Commission's call for opinions on its proposed regulation on artificial intelligence (the so-called ""AI Act""), the LNE thought it would be useful to communicate a number of remarks which are presented in the document attached.",PUBLISHED,large,Thomas,COM(2021)206,2021-08-04 13:48:40,withinfo,24212003,closed,,,
en,2665321,DEU,business_association,Gesamtverband der Deutschen Versicherungswirtschaft e. V. ,Lange,Please find our position in the attached position paper.,PUBLISHED,large,Maximilian,COM(2021)206,2021-08-04 13:35:40,withinfo,24212003,closed,6437280268-55 ,,
en,2665315,FIN,academic_research_instittution,Finnish Center for Artificial Intelligence,Myllymäki,"The highly commendable goal of the proposed act is to provide a legal framework in Europe that encourages innovation and investments in artificial intelligence, while ensuring that the results are lawful, safe and trustworthy, respecting human rights. In order to reach this goal, the legal framework needs to be understandable, transparent and adequately measured. In its proposed form, the act does not properly fulfill these requirements.

The main cause for unclarity lies in the scope of the regulation, which is not technology neutral, but is based on a definition of AI. By giving a definition of AI, the act implicitly defines a class of ""non-AI systems"", which together with the categorization of use cases, partitions the landscape into four classes:
A. High-risk (or prohibited) use case and an AI system.
B. High-risk (or prohibited) use case and a non-AI system.
C. Low-risk use case and an AI system.
D. Low-risk use case and a non-AI system.
It is evident that C and D are not the target of regulation, while A clearly is, but what about B? What is the message here: are for example social scoring systems allowed as long as the underlying technology does not match some definition of AI? As the answer from the moral perspective clearly must be ""no"", the proposal tries to circumvent this dilemma by making the definition of AI extremely broad, and by listing numerous sub-fields of AI, but this does not remove the basic problem: as long as there is a technical definition of AI, it forms a serious loophole by creating a class of (non-AI) software systems that by definition are not regulated by the proposed act. If the idea is to make the definition so broad that it covers all digital systems, then the formulation is just confusing.

A simple solution: define an AI system to mean any software system that is applied in a high-risk or prohibited use case defined in the act. This definition is easy to understand, unambiguous, technology neutral and completely future proof, covering all new technologies that may emerge in the future.

The above formulation also removes a serious cause for unclarity, since the attempt to define AI by listing a number of subfields of AI does not constitute a useful definition that can be used for unequivocally determining whether a certain technology is within the scope of the field or not, as the subfields themselves are not defined. The possibility to add more subfields in the list later does not improve the situation, and any attempts to ""define"" AI in this way are doomed to fail, and just create loopholes and are a serious cause for confusion. Luckily, as explained above, this is not required at all.

Other questions / causes for unclarity:
- What is the rationale for selecting the high-risk use cases listed in the proposal, why these sectors? The underlying logic would be important to understand as the EC reserves itself the right to amend the list later.
- What are the ""essential public and private services"" listed as a high-risk case? Are for example internet search engines essential public services? What about social media, for example for people whose income depends on their social media exposure?
- The spirit of the proposed act seems to be to regulate products or services that will be brought to the market, not scientific research, but this should be stated more clearly: what exactly is the scope, and what about joint research (for example company-university research projects), at which point does this type of activities enter the scope?
- What are the proposed ""regulatory sandboxes"" like: who defines them, who builds them, who maintains them, who can use them?

FCAI supports the risk-based approach adopted in the proposal, but would make this view even more central and make the act completely technology neutral by removing the futile and unnecessary attempts to define the technologies that can be used. This would create a more adaptive, understandable and future proof basis for regulation.",PUBLISHED,large,Petri,COM(2021)206,2021-08-04 12:45:09,withinfo,24212003,closed,039049138497-12,,
en,2665314,NOR,public_authority,Ministry of Local Government and Modernisation,ONGRE,See attachment.,PUBLISHED,large,Camilla,COM(2021)206,2021-08-04 12:41:53,withinfo,24212003,closed,,national,authority
en,2665299,CYP,academic_research_instittution,University of Central Lancashire Cyprus campus,LAULHE SHAELOU,"The Regulation is an important step towards ensuring that AI is used in a transparent manner, abiding to specific qualitative requirements.  Nevertheless, the Regulation in its current from appears disassociated with other initiatives which also touch upon AI, while not doing enough to capture a bigger picture of the impact of AI.  The Regulation does not clarify how it interacts with the Digital Services Act (DSA) in relation to the AI requirements present in the Regulation. As regards the risk-based approach adopted in the Regulation, two main points should be addressed.  Firstly, the Regulation leaves (intentionally?) a gap between ‘high risk’ AI and ‘low risk’ AI, as there is no mention of an ‘average AI’. As a result, tech companies may use this gap to abide to fewer or no requirements.  Secondly, the Regulation is almost exclusively focused on ‘high risk’ AI, as all the prerequisites, save the one Title IV on the ‘Transparency Obligations for Certain AI systems’, only applies to ‘high risk AI’ systems.  Thus, the European Commission should take steps to introduce similar/equivalent/proportionate standards for ‘average’ and ‘low or minimal risk’ AI in the current Regulation and/or by virtue of a new legal instrument of secondary legislation or otherwise specifically for this AI.   In addition, the Regulation through this risk-based approach appears to fail to recognise the competition implications that the ‘low risk’ AI entails, where big tech companies may misuse their AI systems to promote their own platforms or services.  As implications reach intellectual property and other business rights as well as inclusive citizen engagement, the Commission should take further steps to create a pluralistic digital environment. Last but not least, the introduction of AI in numerous aspects of daily life should hinder neither fundamental rights, nor a human centric approach.  The Commission should consider introducing new rights in the Regulation, with the rights enshrined in the EU Charter of Fundamental Rights as a basis, similar to the right to be forgotten in the GDPR.  For instance, the Regulation does refer to transparency obligation by AI systems, although the magnitude of certain situations merit genuine human contact, such as medical decisions. The rights proposed are the right not to be manipulated, the right to be neutrally informed online and the right to meaningful human contact.",PUBLISHED,medium,Stephanie,COM(2021)206,2021-08-04 11:13:41,withinfo,24212003,closed,,,
sv,2665297,SWE,business_association,BIL Sweden,BACKLUND,"BIL Sweden is thankful for the opportunity by the Commission to submit comments on the proposed AI Act, and is pleased to be available for further discussions or any clarifications.
Attached please find BIL Sweden’s response to the European Commission’s public consultation on ”Proposal for a regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain union legislative acts”, 2021/0106 (COD).


",PUBLISHED,small,Maria,COM(2021)206,2021-08-04 10:33:30,withinfo,24212003,closed,,,
en,2665296,BEL,other,"Data, AI and Robotics aisbl",Trino,"BDVA/DAIRO welcomes the opportunity to provide feedback to the European Commission’s proposal for AI Regulation as there is a clear need for a solid AI European approach based on European values.

BDVA/DAIRO supports the idea that there should be a balance between regulation and innovation, and that new rules should facilitate investment and innovation. For this reason, the specific objective of the proposed regulatory framework to “ensure legal certainty to facilitate investment and innovation in AI” is highly supported by the BDVA/DAIRO community. However, the cost, time, infrastructure and knowledge needed to implement this regulation may be burdensome. Small companies, research and education organizations might not be able to easily follow the regulatory developments and might be affected negatively e.g. by a lack of sufficiently qualified personnel, resulting in hard implementation that can hinder innovation and competitiveness. Very solid investments for the implementation of this regulation are needed and in this respect BDVA/DAIRO observes that the European Commission’s Coordinated Plan on AI considers and supports the implementation of the future AI Regulation.

To accelerate the adoption of Industrial and Trustworthy AI in Europe, the association stresses the need to develop and invest in engineering and standardisation frameworks for Industrial and Trustworthy AI (such as the one BDVA is developing in collaboration with the Franco-German initiative) and calls for strong collaboration with national initiatives working on Industrial AI to align and coordinate efforts on this matter.

The association supports and highlights the importance of Data quality and Data governance for AI as paramount, and underlines the fundamental connection between AI and Data. BDVA/DAIRO welcomes also the link to other legislative acts, such as the GDPR, and underlines the alignment needed with the Data Governance Act and the upcoming Data Act. In relation to Data quality, the BDVA/DAIRO community has also emphasized the topic as a mean to, among other benefits, achieve fairness and avoid discrimination. The need for different metrics has been stressed by the community in areas such as Data quality, interpretability, explainability, predictive accuracy, robustness, fairness, computational complexity and very importantly in risk-assesment in alignment with standardisation efforts and results. 

In addition to the above-mentioned topics (Data and Data Governance for AI, the need to develop frameworks and tools to speed up the adoption of industrial AI, and the balance innovation-regulation) the BDVA/DAIRO community has stressed the importance of and provided feedback and recommendations to i) the scope and impact of the draft AI regulation; ii) the notion and components of trustworthiness in the regulation; iii) risk assessment and classification: iv) life-cycle of products, and v) opportunities for SMEs.

The recommendations consider also the need for the establishment of safe environments for better testing. Access to knowledge, testing, experimentation and ecosystem are key factors for success in particular for SMEs. The association acknowledges that the proposed plan for an AI Regulation comes with a new updated Coordinated Plan on AI addressing these topics, but calls up for the importance of investing in federated experimental networks such as the European Federation of i-Spaces or Big Data Innovation Hubs. These networks can not only provide access to knowledge, innovation, testing and ecosystem to SMEs and startups in context of the new AI regulation, but can offer opportunities to legislators in supporting collaboration between innovators and standardisation experts.
",PUBLISHED,micro,Mattia,COM(2021)206,2021-08-04 10:25:50,withinfo,24212003,closed,,,
en,2665293,CHE,business_association,European Test Publishers Group (ETPG),Florance,"The European Test Publishers Group (ETPG) was founded in 1991. It’s a not-for-profit industry body, focused on improving psychological assessments and their impact on European society.  27 European psychological test publishers are members. They are required to meet legal and professional distribution standards and scientific development methods. They meet annually to look at ways our industry can reflect changing ethical, professional, and scientific needs. 

We thank the Commission for their work and welcome rules on Artificial Intelligence (AI) implementation to harness the full potential and benefits of this technology, while continuing to “put people first” in all instances. 
However, we do have serious concerns to be considered as the regulations are debated, given the current standards for fairness and privacy already used by our members.

(1)	We suggest, as have several other organizations, that the risk-based approach in Title III be revised to contain more levels than the three now proposed. This will provide better differentiation between the types of industries and organizations using AI. We agree with many comments that the current High-risk category is too inclusive and should be more granular, so that the precautions mandated, and regulations implemented, are appropriate for different industries. Psychological, occupational, and educational assessments have for decades utilized automated scoring processes (to minimize human error) which could be considered as “AI” technology within the current definition. However, the interpretation of the results of this scoring can only be carried out by a professional and never left to mere digital automatism. 

Assessments in our industry are created by scientific professionals for use by practicing professionals in psychology, education, and human resources, and we are committed to continued steps to bring attention to the importance of responsible and ethical AI use. We promote this through the distribution of publications (see attached white paper on “Artificial Intelligence and the Testing Industry” from the Association of Test Publishers) and  conferences featuring discussions and engagement on AI in assessment. 

(2)	We encourage reviewers to take more care in imposing documentation regulations on AI systems, particularly those related to psychological assessment. Fairness and privacy are central to our work, and we regularly conduct research to ensure our assessments are fair for all individuals, particularly when used for high-stakes decisions (such as clinical diagnosis, employment, or university acceptances). Technical manuals ensure all our procedures are transparent. AI used in our assessments will always be built on the foundations of rigorous science; we would never make a decision that affects an individual solely based on AI. 

However, to ensure fairness in test administration and interpretation we often must rely on AI technology, particularly in a world which necessarily operates remotely. In these cases, we keep clear records of all applied methodologies and data models, employ human oversight, and apply positive framing to any outcomes. The regulations need to consider and reflect the current oversight and standards for fairness and privacy already utilized by assessment publishers.

(3)	We urge regulators to hold public discussions with organizations and associations that have taken the time to provide specific feedback before regulations are approved. These discussions can be part of your presentation to the European Parliament and Council, as the information we have provided, for example, would be critical in understanding the need for more differentiation in the guidelines. Any legislative debate would therefore be more informative for all concerned, and result in subsequent regulations that are transparent, reasonable, enforceable, and effective.
",PUBLISHED,micro,Ianr,COM(2021)206,2021-08-04 10:19:07,withinfo,24212003,closed,,,
en,2665292,DEU,business_association,Bitkom e.V.,Klingholz,"Dear Madam/Sir,

attached you find the Bitkom position regarding the AI Act proposal.

Best regards
Lukas Klingholz & David Adams",PUBLISHED,medium,Lukas,COM(2021)206,2021-08-04 09:58:47,withinfo,24212003,closed,535 183 0264 - 31,,
en,2665289,FRA,other,ETSI,DOR,"ETSI welcomes the draft Regulation on AI presented by the European Commission (EC) at 
https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206. 
The draft Regulation builds on the processes of the New Legislative Framework (NLF) putting harmonised European Standards into the focus of demonstrating compliance with the regulatory requirements. 
This is in line with the view of ETSI that ""The NLF should be used for technical regulation in new areas including AI and data."" (e.g. expressed by the Task Force Bildt Report, https://www.etsi.org/images/files/ETSI-Report-of-the-Task-Force.pdf, p. 11). 

(see full text in file enclosed0",PUBLISHED,medium,margot,COM(2021)206,2021-08-04 09:29:49,withinfo,24212003,closed,474710916419-15  ,,
en,2665287,FRA,eu_citizen,,,"This proposal for a legal act has  option 3 towards  establishing ''MANDATORY REQUIREMENTS'' instead of ''PROHIBITION or OBLIGATIONS''.  Eu is not a technology organisation to  specify  procurements or standards or ''encouraging the development of standards'' to be for the sole EU island. The standards are found & recognized to be the best worldwide . 

This wireless law is based among other things ''on delegated acts to be adopted under the RED'' defined in Ares(2019)476926-28/01/2019.  Where, the last parapraph states: ''...this initiative does not require adoption by the Member States of new transportation/implementation measures''

A  wireless law  ought to  list the wrong doings affecting ''protection of personal data, non- discrimination, product safety and product liability'' as far as any simple EU citezen is concerned and a  technical  industry/ies alone. 
I noticed:
GRDP is reduced by Google, Microsoft,  … & even by  ISP's to tick many times, when a  publication is forwarded to authorize cookies: Advertizers, Geolocalisation, etc...
 if an IBAN can be  known by a party, without authorisantion a withdrowal from  bank account is possible. 
Tried to  order from my recurrent  supplier an article. For some reason the order canceled  no way to be erased. The Accounting bot lists debits and the IA claims settlements ...
So, I think any IA developped should be tested by a  Single, secure & Unique Trust Centre (SSUTC) to verifying the  Machine Learning rights and obligations... Such  SSUTC can replicated & identical  in each EU Member State and be upadated real-time M2M...",PUBLISHED,,,COM(2021)206,2021-08-03 22:12:57,anonymous,24212003,closed,,,
en,2665284,USA,ngo,Thorn,Slifer,"Thorn welcomes the opportunity to provide feedback on the Commission's proposal for artificial intelligence regulation. Thorn is a US-based nonprofit organization that builds technology to defend children from sexual abuse and online exploitation. At Thorn, we believe in the power and potential of government, NGOs, and tech companies working together to eliminate child sexual abuse material online. That goal cannot be achieved by just one of these entities alone, and we are grateful for the European Union’s leadership. We believe that tailored and precise artificial intelligence legislation is necessary to ensure that child users' privacy and safety is protected.
Legislation on artificial intelligence should allow for innovation and growth in technology that protects children online.  If regulation does not allow for this kind of innovation, then we will continue to be behind the curve of individuals exploiting children online.  At Thorn we are always working to innovate and refine our technology to protect children. We, and our partners in the child protection ecosystem, need the flexibility to continue to create cutting edge technologies to eliminate child sexual abuse material from the internet. If regulation becomes indiscriminate, or doesn’t provide necessary flexibility for this specific use case, it can create unintended consequences that could deter the development of new technologies to protect children online.  
In the space of child protection, there are well established technologies that have been tested and refined for over a decade but many of the most cutting edge technologies still need the space for further innovation.  These technologies have proven results of finding and saving children from online sexual exploitation. Tailored technological solutions in this space are the future of protecting children from online exploitation and there needs to be a legislative framework that allows for this crucial work to continue.  We understand the concerns that some AI applications could lead to the invasion of individual users' privacy but child advocacy organizations have always worked towards surgical and balanced solutions in order to protect children online.  
AI and machine learning are important tools when it comes to online child protection - from text analysis that can prevent the grooming of a child for abuse, to facial recognition technology that can identify missing and/or exploited children's photos.  When a child is missing or exploited law enforcement needs the tools that can help them find these children in the quickest and most efficient ways possible. When leveraged safely and responsibly, facial recognition can be one of the tools used to identify child victims of sexual abuse.  The ability to use tailored, specific, and regulated “after the fact” facial recognition technology on images and videos of child victims is vital to expedite law enforcement efforts to find and protect children.  
Child sexual abuse detection technologies are cutting edge and designed to protect the most vulnerable population in our society.  Given this sensitivity, we agree that safeguards and greater transparency are necessary for artificial intelligence technology used in this space. The artificial intelligence regulation must find a balance that protects the general public’s privacy while still allowing for technology designed to protect children. We must not allow offenders the ability to reverse engineer technologies designed to keep our children safe. Any enhanced transparency should not impede the development of technologies that are used to protect children online.
",PUBLISHED,medium,Emily,COM(2021)206,2021-08-03 20:49:25,withinfo,24212003,closed,854246640306-96,,
en,2665281,BEL,business_association,ACT | The App Association,Bosch,"Please find attached the feedback of ACT | The App Association (Transparency Reg. # 72029513877-54) to the European Commission’s proposed Artificial Intelligence Act.


Anna Bosch
Policy Associate
ACT | The App Association (Transparency Reg. # 72029513877-54)
Rue de Trèves 45
B-1040 Brussels",PUBLISHED,small,Anna,COM(2021)206,2021-08-03 20:11:27,withinfo,24212003,closed,72029513877-54,,
en,2665266,GBR,ngo,5Rights Foundation,Barrington-Leach,"5Rights Foundation warmly welcomes the aims of the AI Act to ensure the development, marketing and use of artificial intelligence in conformity with Union values and a high level of protection of health, safety and fundamental rights.

In particular the provisions in Recital 28, Article 5.1 and Article 9 together make for ground-breaking legislation, which both recognises and defends children’s existing rights in the digital world, in line with the EU Strategy on the Rights of the Child 2021-2024. This has the potential to make the EU a world leader in ensuring children’s rights in the digital world and, together with a similar recognition in related EU regulation, transform the digital sphere into a place where children can be safe and prosper, in Europe and beyond. It will be critical to maintain them in the final adopted law. 

Regarding Title II - Article 5.1.a and 5.1.b – We welcome the ban on AI systems that use subliminal techniques to manipulate users and those that exploit the vulnerabilities of children. However, there is no means to enforce these provisions in the current text. It will be critical to fill this gap. 5Rights is working on a detailed framework for the oversight of AI systems in order to ensure transparency and the rights of the child and will share with the Commission in the coming months.

Regarding Title III on High Risk AI Systems:
- 5Rights maintains that all AI systems likely to be accessed by or have an impact on children should be considered high-risk and subject to risk assessment and mitigation measures. Children may be less able to recognise that they are interacting with and impacted by AI and less able to fully understand the implications thereof. They often lack the resources to respond to instances of bias or to dangerous content which has been amplified by AI technology. Children may be less able to react, manage stressful situations or seek redress. Their negative impacts can be more severe and longer-lasting on children than for adults.
- The legal framework defining which AI systems are effectively high risk must be robust and future-proof. The complexity of the proposed system (based on the application of secondary legislation) may lead to loopholes.

5Rights Foundation also calls for the inclusion of the following elements in order to ensure children’s rights:
- In Article 3 a clear definition of children as all those under the age of 18, as per UNCRC Article 1.
- The broader recognition of children as de-facto or likely users, even if they are not the intended users. This is critical for training data and for innovation sandboxes. Recital 44 could be amended to include “likely” users. A para could be added further to Recital 72 and Article 53 to the effect that sandboxing schemes should systematically use a diverse set of use cases and users, and consider the specific rights and needs of children.
- Ensuring that children are not subjected to the same level of personal responsibility as adults for understanding risk – children require an exemption to Recital 58 and Article 29, to ensure that when high-risk AI systems are likely to be used by children, the burden of responsibility for the safety and respect of fundamental rights of child users rests primarily upon the AI providers and operators.
- Ensuring that information and training for systems likely to be used by children is in a format and language that children can easily access and understand. (Recital 70, Article 13)
- Ensuring that data collected in post-market monitoring includes the age or age ranges of end users. (Article 61)
",PUBLISHED,small,Leanda,COM(2021)206,2021-08-03 17:09:25,withinfo,24212003,closed,373653640889-82,,
en,2665262,AUT,public_authority,"Federal Ministry for Social Affairs, Health, Care and Consumer Protection",REIFFENSTEIN,"Eine rechtliche Regulierung von KI-Systemen ist dringend notwendig. Das AIA wird daher grundsätzlich begrüßt, bedarf aber einer ausführlichen Diskussion insb aus Sicht der betroffenen Konsument*innen. 
Der risikobasierte Ansatz ist sinnvoll, berücksichtigt aber leider keine wirtschaftliche Risiken. Dies greift zu kurz, wenn man an zunehmendes personalisiertes Marketing/Profiling denkt, einschließlich personalisierter Preise. Konsument*innen erhalten immer öfter nur Ausschnitte der Angebotswelt. Vieles wird ihnen vorenthalten oder zB bei smarten Geräten für sie entschieden, ohne dass sie häufig den Grund dafür kennen. Bonitätsscoring ist gem Annex III auch nur dann hochriskant, wenn es nicht durch Klein(st)unternehmer erfolgt. Das Risiko steht aber in keinem Verhältnis zur Größe eines Unternehmens. Auch Emotionserkennung gilt offenbar nicht als hochriskant, sondern unterliegt ausschließlich der Informationspflicht gem Art 52. Die Definition der KI-Systeme ist zwar breit. Der Kern der Regelungen gilt aber nur für hochriskante KI. IoT-Anwendungen erfordern – mit Ausnahme von Infrastrukturleistungen – bedauerlicherweise keinerlei Verpflichtungen für den Anbieter.
Die Liste der verbotenen KI-Systeme greift ebenfalls zu kurz. So verbietet Art 5 Abs 1 lit a KI-Systeme mit unterschwelliger Beeinflussung nur dann, wenn damit ein Schaden einhergeht, während die AVMD Richtlinie etwa Schleichwerbung generell verbietet. Das Abstellen auf einen Schaden sollte daher gestrichen werden. Dasselbe gilt für Art 5 Abs 1 lit b. Auch das Verbot von social scoring, das grundsätzlich begrüßt wird, ist zu eng gefasst (kein Verbot der Primärnutzung von Daten für social scoring, wenn „verhältnismäßig“, kein Verbot KI-basierter sozialer Auslese durch Unternehmen). Überwachungsmaßnahmen außerhalb von Echtzeit-Fernidentifizierungssystemen werden ebenfalls nicht erfasst. Diese Liste bedarf einer grundlegenden Überarbeitung.
Ein weiterer grundsätzlicher und schwerwiegender Kritikpunkt liegt darin, dass Betroffene keinerlei Informations- oder sonstige Betroffenenrechte außerhalb der DSGVO haben und Diskriminierung daher kaum einschätzen können (zB bei Bonitätsprüfung).
Noch schwerer wiegt, dass der VO Vorschlag keinerlei Rechtsbehelfe oder haftungsrechtliche Ansprüche für Betroffene vorsieht. Die Kontrolle erfolgt damit ausschließlich behördlich. Der zivilrechtliche Vollzug wird in keiner Weise adressiert. Unklar ist auch, wer Recht auf Einspruch gegen Entscheidungen notifizierter Stellen hat („Beteiligte…, die ein berechtigtes Interesse an einer solchen Entscheidung haben“, ohne Erläuterung des berechtigten Interesses). Wenn die Produkthaftungsrichtlinie nicht in ausreichendem Ausmaß geändert wird, müsste das AIA jedenfalls durch Haftungsregelungen ergänzt werden. Hinsichtlich der Rechtsdurchsetzung sieht der VO Vorschlag auch keine Änderung der VerbraucherbehördenkooperationsVO (CPC) oder der VerbandsklagenRL hinsichtlich einer Ausweitung des Anhangs vor, sodass auch hier kein erweitertes zivilrechtliches Schutzniveau für Verbraucher*innen geschaffen wird.

Schließlich bedarf es einer umfassenden Diskussion der vorgeschlagenen Konformitätsbewertungsregelungen. KI-Systeme, die in den Annex II fallen, sind gem Art 6 Abs 1 nur dann als KI mit hohem Risiko zu qualifizieren, wenn sie einer externen Zertifizierung unterliegen. Ob dies der Fall ist, muss nach der jeweiligen New Approach Richtlinie geprüft werden. Die New Approach Richtlinien sehen aber nur in wenigen Fällen eine Drittzertifizierung vor. Das Risiko durch Verwendung eines KI-Systems konnte aber zum Zeitpunkt der RL noch nicht mitbedacht werden (s. zB die Puppe Cayla). Auch bei Annex III-Anwendungen wird eine interne Konformitätsbewertung häufig nicht ausreichen, wie dies Art 43 festlegt. Jedenfalls für Biometrie und Bonitätsbewertungssysteme darf eine interne Bewertung nicht ausreichen. Biometrische KI-Systeme sollten einer Zustimmung der Betroffenen bedürfen.
",PUBLISHED,large,Maria,COM(2021)206,2021-08-03 15:36:07,withinfo,24212003,closed,,national,authority
en,2665260,ESP,business_association,Adigital,,Please find attached Adigital´s contribution to the European Commission´s proposal on the Artificial Intelligence Act,PUBLISHED,small,,COM(2021)206,2021-08-03 14:48:41,anonymous,24212003,closed,972127919039-72,,
en,2665256,BEL,ngo,Wikimedia (FKAGEU),DIMITROV,"Wikimedia operates a number of online platforms that offer access to and re-use of the world's largest freely available datasets. These are used widely as training data for machine learning algorithms. 

Wikimedia also develops and operates machine learning algorithms, mainly with the goal to make volunteer editors' work more efficient. One such example is ORES (https://www.mediawiki.org/wiki/ORES), a service that helps detect vandalism on Wikipedia and its sister projects. 

Wikimedia worries about inherent biases in our data that are magnified by machine learning services. We constantly strive to recognise and remedy these. In 2019 we published a research on ""Ethical and Human Centered AI"" that we commissioned. It looks into the challenges and biases we were able to make out on our own projects. 

From our experience  one of the key tasks we have as a society in applying ML/AI software is to detect and correct biases. Biases are real-life discrimination practices that are also present it the datasets and magnified by the operation of AI tools. 

One feature our ORES has is a so-called ""feature-injection"". Basically anyone can test the system by providing a an input (a user's edit history and a new edit) and checking whether the ""vandalism"" flag would be triggered. This can happen with either the live data from the projects or synthetic data provided by the tester. From our experience this opportunity to test the system for concrete biases (""Are female sounding editor names more likely to be flagged as vandals?"") helps in detecting discriminatory practices. 

Article 52
This is why we would like to consider if it would be possible to offer civil society groups or the general public a ""testing option"" for AI applications run by the public sector. This was a Roma rights organisation, for instance, could check whether software run by the local police force is unfairly biased against this group. One place such a ""testing option"" could be included is in Article 52 and address uses of AI software by the public sector.

Article 3
A further comment we have is on the definition of providers in Article 3. Many of the obligations of the Regulation are attached to providers, yet it seems very unclear who this is - the software developer, the reseller of the software, the service provider or the organisation using it? It seems very important to have a crystal clear definition that will easily be understand by a Finnish police authority, a Bulgarian software developer and an Irish trademark owner, alike. 

Oversight authority
To ensure human oversight it is important for users and civil society to have access to enough information about how AI systems work. But is is also important to know which authority is responsible and what redress mechanisms citizens have. The Regulation remain unclear about how and where users can get help or file complaints/demands. It might be a good idea to specify this. The European Data Protection Board is a EU level authority that already has considerable knowledge and experience in the area of large datasets, models and algorithms. 
   ",PUBLISHED,micro,Dimitar,COM(2021)206,2021-08-03 13:29:20,withinfo,24212003,closed,191538712765-84,,
en,2665252,BEL,trade_union,COV (Christelijk Onderwijzersverbond),De Vocht,Attached you can find COV's position on the European Commission's proposal on the Artificial Intelligence Act. We thank the EC for giving us the opportunity to give feedback. ,PUBLISHED,small,Babs,COM(2021)206,2021-08-03 12:49:05,withinfo,24212003,closed,,,
en,2665249,BEL,trade_union,industriAll European Trade Union,Brauburger,"IndustriAll Europe welcomes the European Commission’s draft regulation on AI, as it is the first proposal of its kind and will set new standards when it comes to addressing the challenges of high-end technology and human rights implications. We appreciate the opportunity to provide feedback on the draft regulation. 

Although we share doubts, with other trade unions and civil rights organisations, that the risk-based approach is fit for purpose, and although we think that a rights-based approach to the Regulation would have been preferable, we acknowledge that the risk-based approach is thoroughly applied in the proposal. In that sense, we welcome that “AI systems used in education or vocational training, notably for determining access or assigning persons to educational and vocational training institutions, or to evaluate persons on tests as part of or as a precondition for their education should be considered high-risk (...)” (35). We further welcome that “AI systems used in employment, workers management and access to self-employment, notably for the recruitment and selection of persons, for making decisions on promotion and termination and for task allocation, monitoring or evaluation of persons in work-related contractual relationships, should also be classified as high-risk (...)” (36). 

The current proposal, however, does not take into consideration the impact of AI on workers rights and the need to anticipate change. Human rights considerations in the wider sense are also neglected. The potential adverse impact of Artificial Intelligence and Machine Learning systems on the environment is missing altogether. The current proposal should therefore be complemented by additional and tailored Regulations that can address the gaps that pose risks to those who are subjected to AI/ML systems.

industriAll Europe further suggests to include the following points in the AI Regulation:

- A comprehensive chapter on AI at the workplace in the Regulation, or a stand-alone Regulation on AI at the workplace, which should be drafted upon consulting cross industry social partners as well as sectoral social partners;
- Any chapter on AI at the workplace must include the roles of trade unions and works councils, as well as that of collective bargaining;
- Every worker should be aware of the exact nature of such a system monitoring their performance, and of the parameters used to evaluate them;
- Works councils should be provided with the means to hire software engineers to support them in their analyses of the AI/ML systems;
- Explainability must be guaranteed by using a language that is understood by those subjected to AI/ML systems;
- Consent to the processing of worker-related data should only be given collectively; individual consent should not be considered sufficient in a situation of employment or of dependent work;
- It should be clarified that platform workers should also be covered by the relevant collective agreements;
- Clearly define the questions of liability and redress in accidents and incidents involving AI systems. The current general rule, whereby the employer is by default liable for any accident in the workplace (in the absence of any wrongdoing by the worker) should remain;
- Ban all types of emotional recognition software, as they are highly unreliable and their outcomes have the potential to be more harmful than helpful;
- Any AI application dealing with personal data, with workers’ data and/or which affects working conditions should be classified as ‘high-risk’ and subject to a third-party conformity assessment.

We thank the European Commission for giving us the opportunity to comment on the proposed Regulation in more detail. Please find attached a document in which we elaborate our position in more detail. It is the result of extensive discussions with our member organisations. We would be grateful if the points that are raised in the document could be taken into account.",PUBLISHED,small,Jan Peter,COM(2021)206,2021-08-03 12:09:07,withinfo,24212003,closed,358284014848-82,,
en,2665242,BEL,ngo,"Centre for Democracy & Technology, Europe",Nosak,"The Centre for Democracy & Technology, Europe (CDT) welcomes the EU AI Act and the high priority it aspires to give to protecting fundamental rights. All AI systems should be subject to a human rights impact assessment and subject to regulation proportionate to the risks identified in that assessment. A risk-based approach can be helpful in ensuring proportionate regulation, but in order to appropriately protect human rights, this needs to integrate a rights-based approach. The theory of risk should recognise that risk increases when the likelihood, or the seriousness, of infringement on rights increases. The proposal’s hierarchy of risk at times focuses on the technology and at times on the context. The category of ‘certain AI systems’ (low-risk, Art. 52) includes biometric categorisation and AI systems to prevent and investigate crimes. These AI applications are actually both high-risk, with historic and current examples of rights abuse. The prohibition on social scoring only applies to governments, but private entities are at the same risk of using such systems to infringe human rights whether they are performing an outsourced public service or using such a system in their own services.

The proposal rightly classifies biometric surveillance by law enforcement in publicly accessible places as an ‘unacceptable risk’, but the derogations include some of the highest risks to human rights and will swallow the rule. For instance, permitting its use to combat terrorism is wrought with risks because of human rights loop-holes in European and national counter terrorism legislation. Any law enforcement use of biometric surveillance is inherently high-risk and should be prohibited or subject to robust regulation.

The proposal has ad hoc references to content moderation. CDT has documented how automated content analysis can be inaccurate and perpetuate discrimination. But there is no specific reference to this danger. In terms of legal clarity, there is a risk of confusion between the due diligence provisions of the draft Digital Services Act and the AI Act.

The draft proposal limits avenues for individual redress or access to remedy. Remedies are almost exclusively accorded to vendors of AI and professional users (including governments) and not individual users nor marginalised or at-risk groups. Because the proposed legal basis of the draft proposal is Article 114 TFEU, the governance and enforcement mechanisms are rooted in product safety and market surveillance logic. Given the goal to better protect fundamental rights, Art. 2 TEU should be added as an additional basis. This could allow for a mandate for equality bodies, national human rights institutions and ombudspersons to be integrated into the governance system. Their expertise on human rights impact assessments could better inform risk assessment. The draft should also provide end-users concrete and actionable rights to object to being subject to AI. To ensure the compatibility and enforcement of EU equality legislation with the draft act, an individual or civil society organisation should have an avenue to make claims of discrimination and the burden should shift so the entity using the AI system is required to disprove that discrimination.

The draft AI Act also gives disproportionate power to private actors. It is unclear how the proposed standards could be enforceable, in particular with regard to individual or group complaints. The self-assessment and standardisation approach risks absolving public authorities from  policy-making  (and offloading it to privatised standards instead). The current European standard-setting process is inaccessible to most public interest actors and the harmonisation process threatens to weaken whatever standards are set. This is a further example of why it is important to formally and purposefully bring more human rights and public interest actors into the process of risk-assessment, enforcement and policy-making.",PUBLISHED,small,David,COM(2021)206,2021-08-03 10:50:06,withinfo,24212003,closed,57305017757-64,,
en,2665235,BEL,company,SAP,SCHULZE,"Attached please find SAP’s position on the European Commission’s proposal on the Artificial Intelligence Act. We thank the Commission for the opportunity to share feedback, and look forward to a continued dialogue on a future-proof, balanced and risk-based AI regulation in the EU.",PUBLISHED,large,Corinna,COM(2021)206,2021-08-03 09:58:55,withinfo,24212003,closed,,,
en,2665234,BEL,ngo,European Digital Rights (EDRi),Policy,"European Digital Rights (EDRi) is Europe’s biggest network defending rights and freedoms online. EDRi welcomes the European Commission’s globally significant step towards regulating the development and deployment of artificial intelligence systems. However, we would like to make suggestions to ensure that the AI Act is in line with the Charter of Fundamental Rights of the EU, future proof, and a role model for rights-protective future AI legislation around the world.

We raise concerns about the extent to which the Act protects fundamental rights and addresses broader structural, political and economic issues. In the attached paper, we highlight how the use of AI systems can systematically target, harm and exclude marginalised communities. We question how far the act can address these structural harms, due to its tendency towards de-regulation of all but the most narrowly-defined ‘unacceptable’ uses; the lack of obligations on users; and the lack of individual and collective redress for those subject to AI systems. EDRi thus recommends the following improvements:

1. Ensure effective protection against prohibited practices and address the full scope of unacceptable risks:
a) Strengthen art. 5 prohibitions to provide meaningful protection against fundamental rights violations and individual and collective harms;
b) Comprehensively prohibit biometric mass surveillance practices (RBI and automated recognition of human features in publicly-accessible spaces for any purpose);
c) Prohibit additional practices which are incompatible with fundamental rights and democracy: uses of AI in law enforcement or criminal justice that purport to predict future behaviour; in migration control in ways that undermine the right to claim asylum; to implement invasive surveillance, monitoring and algorithmic management in an employment and educational context; to categorise people on the basis of their human features; for emotion recognition; and in uses that constitute mass surveillance.

2. Adapt the AIA to ensure a holistic, democratic and future-proof framework:
a) Introduce a democratic, inclusive and accessible process for the insertion of new prohibitions, including criteria for ‘unacceptable risk’ and the addition of future prohibitions;
b) Ensure high risk ‘areas’ can be updated under art. 7;
c) Fix gaps relating to economic and environmental impact, structural inequality, migration control, law enforcement (LE), worker surveillance, mass surveillance and exports;
d) Remove loopholes in arts. 2.4 and 83 (international LE agreements);
e) Remove art. 47 broad public security exemption for conformity assessment;
f) Remove art. 54.1.a purpose limitation exemption in criminal justice.

3. Enhance obligations on users of all AI systems:
a) Mandate ex ante human rights impact assessments for high risk systems before putting into use;
b) Implement duty to cooperate with national authorities under arts. 65 and 67 for all systems;
c) Implement meaningful consultation duty;
d) Implement public authority notification requirement.

4. Implement meaningful public transparency for high risk systems:
a) Registration in EU database (art. 60) for AI systems that are put into use;
b) Add instructions for use for LE, migration, asylum and borders to public database;
c) Require providers to include access to conformity assessment as per article 13.2-3 under art. 60;
d) Require more thorough detail from providers to users under art. 13.3;
e) Remove art. 52 exemptions for transparency in detection, prevention or prosecution; for art. 52 uses, suspects should be notified post factum.

5. Facilitate accountability: Include oversight and enforcement infrastructures that work for people:
a) Ensure cohesive national enforcement structure;
b) Include individual and collective flagging and redress mechanisms;
c) Implement a more democratic governance infrastructure and greater independence for EAIB.

See EDRi’s attached paper for further analysis and recommendations.",PUBLISHED,small,EDRi,COM(2021)206,2021-08-03 09:54:23,withinfo,24212003,closed,16311905144-06,,
en,2665233,USA,company,Workday,Jeppesen,"Workday’s response to the European Commission’s proposal for regulation on artificial intelligence (the AI Act) draws on our contributions to the Commission’s preparatory consultations as well as earlier work developing Ethics Guidelines for Trustworthy AI. We highlight important areas of alignment between the Act and Workday’s January 2021 white paper in which we proposed a horizontal approach to regulation, self-assessment, life-cycle analysis, transparency and provision of necessary information to deployers and end-users, as well as consideration of the ethical implications and impact of the technology on users. 

Notwithstanding significant areas of alignment, we take issue with certain aspects of the Act and make recommendations for improvement. Most importantly, we call into question the product-safety framing of the Act, noting that AI is not a product, it is a tool or method that can manifest in products, software, or services. We suggest that the Commission should consider adopting a framework more closely tied to ethical and trustworthy AI rather than product safety, at least for standalone software systems. ",PUBLISHED,large,Jens-Henrik,COM(2021)206,2021-08-03 08:27:11,withinfo,24212003,closed,021253717373-58,,
en,2665231,USA,company,OpenAI,Solaiman,"The attached submission from OpenAI provides feedback on the Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence and Amending Certain Union Legislative Acts. We previously submitted feedback on the White Paper on Artificial Intelligence and feedback on the Ethics Guidelines for Trustworthy AI’s Technical Robustness and Safety section; we will draw from our previous feedback throughout this response. We offer recommendations for consideration when taking action towards shaping trustworthy and responsible artificial intelligence (AI). We aim to help the EU technically inform regulations, so they are most applicable to existing systems.",PUBLISHED,medium,Irene,COM(2021)206,2021-08-02 22:02:48,withinfo,24212003,closed,,,
en,2665227,USA,company,Biogen,Corazza,"We welcome the Commission’s first ever legal framework on AI and its goal to promote Europe’s innovation capacity in AI, which has the potential to speed drug development and improve healthcare outcomes across the continent. We support a risk-based approach to AI with an oversight proportionate to the intended use and led by defined risk categories. 

Biogen supports the Commission’s approach to regulate AI based on risk, overall acknowledging CE marking system for high-risk applications in healthcare which is already addressed by MDR/IVDR. We would welcome an alignment of the risk levels described in the proposal with MDR for consistency. Under a risk-based regulatory system and in consideration of the scope of use, some AI-driven software may be categorized as low or intermediate risk under the MDR, whereas the draft proposal appears to classify all such devices as high risk. The inclusion of AI software into the EU’s product compliance framework could also decrease the competitiveness on the market due to providers having difficulty in fulfilling market access regulations and therefore jeopardizing the uptake of AI-driven solutions by operators. AI has a broad application in the pharma supply chain including R&D, manufacturing, launch, post marketing surveillance and patient support. To incentivise the investments in AI solutions in Europe, we recommend a thorough assessment of the proposed requirements versus the support offered to providers to navigate through the new rules. 

In order to assure high quality AI solutions and to safeguard the EU’s competitiveness in the international AI marketplace, easy yet compliant access to a significant amount of high quality and representative data will be prerequisite to reducing bias, discrimination and ensuring highest levels of safety and robustness of AI. We support the mapping and “FAIR-ification” of existing databases to build up on existing sources and to make the machine-readable metadata findable for automated discovery.  The Commission’s plans to create a EHDS promises to unlock the power of data for AI-enabled healthcare and a well-functioning EHDS should incentivise data sharing and harmonise applicable rules to remove barriers to the collective benefits across Europe. In order for all stakeholders, including providers and operators, to train, test, develop and apply a trustworthy, reliable AI system, clear rules on data access and processing should be laid out; and full benefit should be made of all measures in support of innovation such as AI regulatory “sandboxes” which may involve the further processing of personal data for a limited period of time and under well-defined conditions. Emerging AI methods involving the decentralised development of machine learning algorithms would enhance user and stakeholder privacy and should be investigated.

In line with our health equity commitments, Biogen emphasizes the importance of access to not only high quality but diverse datasets to proactively counter the potential for bias in AI-base systems as an important protection for patients.
 
We should avoid fragmented implementation. Therefore, we welcome the creation of a European AI Board tasked with coordination of collaboration on AI across the EU. We emphasise that the evolving positive ecosystem of AI must be built together. Biogen wishes to underline the importance of all key stakeholders being engaged in this initiative at the highest level and encourages early agreement on public-private platform to allow for structured dialogue with the Board. 

Finally, we call the EU policymakers to engage with international partners throughout the legislative process to take into consideration how the rules would apply internationally.  Any regulation should ensure an equal playing field for both local and global players while not stepping away from core European values. 
",PUBLISHED,large,Andrea,COM(2021)206,2021-08-02 17:37:50,withinfo,24212003,closed,966165310889-60,,
fr,2665226,FRA,business_association,ASF - ASSOCIATION FRANCAISE DES SOCIETES FINANCIERES,denaeyer,"L’Association française des Sociétés Financières (ASF) représente les métiers de financement spécialisé en matière de crédit ainsi que les services financiers et d’investissement. Les établissements membres de l’ASF financent plus de 20 % des crédits au secteur privé.

Observations liminaires 
L’Association française des Sociétés Financières (ASF) accueille avec intérêt la consultation publique lancée en avril dernier sur le projet de règlement établissant des règles harmonisées concernant l’intelligence artificielle.
Ce projet de règlement est selon nous un point positif dans la mesure où il prend en compte l’utilisation des systèmes d’IA dans la relation entre les entreprises et leurs clients.
Néanmoins, l’ASF rappelle que l’utilisation des données personnelles nécessaires au fonctionnement des IA est déjà encadrée quant aux conditions de collecte et finalités de traitement et de conservation des données, ainsi qu’à l’exercice des droits des personnes (droit à l’information, droit d’opposition, droit d’accès, droit de rectification, …) et ce, afin de protéger la vie privée et les libertés des personnes concernées.
Enfin, des dispositions juridiques interdisent différentes formes de discrimination, de sorte que nous disposons déjà d’un arsenal apportant une sécurité juridique dans l’utilisation des IA. 
 ",PUBLISHED,small,corinne,COM(2021)206,2021-08-02 17:24:15,withinfo,24212003,closed,97303386616-87,,
en,2665225,DEU,business_association,eco - Verband der Internetwirtschaft e.V.,EHMANN,"About eco: With over 1,100 member companies, eco is the largest Internet industry association in Europe. Since 1995 eco has been instrumental in shaping the Internet, fostering new technologies, forming framework conditions, and representing the interests of members in politics and international committees. eco’s key topics are the reliability and strengthening of digital infrastructure, IT security, and trust, ethics, and self-regulation. That is why eco advocates for a free, technologically-neutral, and high-performance Internet.",PUBLISHED,medium,Philipp,COM(2021)206,2021-08-02 16:53:52,withinfo,24212003,closed,483354220663-40,,
en,2665222,AUT,ngo,European Society of Radiology,Campbell,"The European Society of Radiology (ESR) welcomes the European Commission’s initiative to create the first AI regulation worldwide. However, it recognises the importance of taking into account the specificities of the healthcare field, which require a specific regulatory approach for AI, complementing the currently proposed horizontal, cross-sector regulation. Also, a detailed definition of AI in healthcare is needed in order to be able to tackle regulatory aspects in an effective manner.

• Care must be taken not to prevent or slow down innovation due to overregulation and thus prevent Europe’s patients from access to cutting edge healthcare solutions and causing Europe to lose its status as a leader in technological innovation in healthcare.

• Alignment with the requirements laid down in the Medical Device Regulation is needed in order to ensure legal certainty.

• Access to training data is considered to be important from a clinical perspective but requires significant technical infrastructure and repositories. Such data repositories could be coordinated by a centralised body or through cooperation from European medical scientific societies. 

• The current provision to use only “error-free and complete” data for training, validation and testing is neither practical nor desirable if testing takes place under real-world conditions. Data sets need to be sufficiently accurate and complete to meet the intended purpose, however the claim for freedom from error is considered inappropriate and would potentially prevent the use of real-world data in healthcare.

• Clinical users of AI need a sound and efficient feedback mechanism with providers of AI solutions for healthcare, such as a direct messaging feature within AI applications to providers, available for radiologists during reporting. 

• Human oversight is considered crucial for AI applications in healthcare and the user’s autonomy should not be restricted.

• Training of health professionals needs to be adapted to include the use of patient-centred AI in healthcare and to ensure a well-trained workforce.

• The use of AI in healthcare must respect the principles of the GDPR. Additional European guidance and standards would be helpful, including on anonymisation techniques of health data, quality and safety standards etc.

The European Society of Radiology is an apolitical, non-profit organisation, dedicated to promoting and coordinating the scientific, philanthropic, intellectual and professional activities of Radiology in all European countries. The Society's mission at all times is to serve the health care needs of the general public through the support of science, teaching and research and the quality of service in the field of Radiology. Founded in 2005, the ESR is the largest radiological society in the world, with over 122,000 members from 181 countries.  
",PUBLISHED,medium,Alistair,COM(2021)206,2021-08-02 15:03:23,withinfo,24212003,closed,,,
en,2665221,GBR,academic_research_instittution,Oxford Commission on AI and Good Governance,Neudert,"Submission by the Oxford Commission on AI and Good Governance (OxCAIGG) The Oxford Internet Institute, University of Oxford. Contact: Professor Philip Howard: Philip.howard@oii.ox.ac.uk. Lisa-Maria Neudert: Lisa-maria.neudert@oii.ox.ac.uk

Summary: A list of questions for consideration and reflections on views on AI in EU MS. A look at the interplay between regulation and digital technical standards. An overview of skills and capacity for regulators and participants in standards. Reflections on the proposed extra-territorial effect for the draft AI Regulation – risks and benefits.

FURTHER RESEARCH AVAILABLE IN THE APPENDIX

Questions to be asked:
 
•	What types of bodies or public agencies are needed? Are existing bodies fit to regulate AI? 
•	At what level(s) should AI be regulated? How should power be balanced between the EU Institutions and the Member States?
•	Could there be some type of resource agency that brings together knowledge and expertise to assist governments in adopting AI? 
•	What bodies will provide oversight into regulation?
•	What bodies will be the final arbiter on violations of the regulations?
o	An arbitration board with appeal’s mechanisms?
•	How will the regulatory body maintain its independence?
•	How will all the elements be funded?
•	How will the EU collaborate with industry?
o	Should be independent of funding. 
•	What kind of data protection principles will there be?
•	How can good governance norms be correctly implemented?
•	How will the EU keep up with the pace of development so that regulation remains relevant? How will it anticipate and deal with loopholes?
o	Regulatory sandboxes might be an approach 
•	How will ethics considerations be incorporated into the regulator?
•	How will the skills set be maintained and developed to run regulation?
•	How will the EU engage with other key players in particular those disconnected from the mainstream discourse such as China? How will it share best practice and lessons learned?

About OxCAIGG

The Oxford Commission on AI and Good Governance investigates the procurement and implementation challenges surrounding the use of AI for good governance faced by democracies around the world, identifying best practices for evaluating and managing risks and benefits, and recommend strategies in an effort to take full advantage of technological capacities while mitigating potential harms of AI-enabled public policy.
Drawing from input from experts across a wide range of geographic regions and areas of expertise, including stakeholders from government, industry, technical and civil society, OxCAIGG will bring forward applicable and relevant recommendations for the use of AI for good governance. 

https://oxcaigg.oii.ox.ac.uk/
",PUBLISHED,large,Lisa-Maria,COM(2021)206,2021-08-02 14:58:43,withinfo,24212003,closed,,,
en,2665210,ITA,business_association,Italian Banking Association,Attanasio,"Il documento rappresenta il contributo dell'Associazione Bancaria Italiana (ABI) al dibattito in tema di Intelligenza Artificiale ed è stato redatto con l'attivo contributo dei gruppi di lavoro composti dalle banche associate e da esperti in materia.
Nel ringraziare per l'opportunità, rimaniamo a disposizione per ogni approfondimento.
Con i migliori saluti,
Silvia Attanasio",PUBLISHED,medium,Silvia,COM(2021)206,2021-08-02 10:50:58,withinfo,24212003,closed,915519211566-03,,
en,2665208,BEL,ngo,European Association of Hospital Pharmacists,Marzal Lopez,"The European Association of Hospital Pharmacists (EAHP) welcomes this legislation proposal on the ethical and legal requirements on Artificial Intelligence (AI). The proposed regulation lays down a set of harmonised rules for the development, placement on the market and use of AI systems in the Union, this is particularly important (as explained in the proposal) for systems that can pose signifiable risks on the health of citizens. EAHP also recognizes the importance of labelling as high-risk AI systems that can have a significant impact or risk on the health of the user (like medical devices) and the importance of having a set of ethical and legal requirements on the development, placement and use of these devices in the market (adding to all requirements established in the medical devices regulation).

However, despite these positive features, EAHP believes that the legislative proposal should be more specific for health-related AI systems and include as high-risk systems all those that interact with patients and are linked with patient’s treatments and research ..or are being used to generate treatment suggestion based on misinformation.. For instance, AI systems that analyse health data to better diagnose diseases. All AI systems that imply the use of health data or have an impact on patients’ treatment whatsoever should be considered as high risk not only the ones that according to the draft regulation can harm or cause a threat to the health of the users. All health-related AI systems should be included as high-risk system or should have a separate set of rules to ensure the correct use of these systems. In addition, the legislation should include a mention to nanomedicine and nanotechnology, when being used to effectively diagnose, treat, and prevent various disease with the support of AI. 

The proposed legislation does not treat the algorithms used on mobile apps or mobile operating systems as high risk, and it’s important to keep in mind that there are health apps and m-health application that could be linked with AI systems and that should also include as high risk to protect patients and the safety of their data. It is possible that some algorithms used in ad tracking or recommendation engines might be prohibited as manipulative or exploitative practices.

The legislative proposal lacks detailed legal requirements on the right for patients to have access to the information processed by the AI systems and to understand the use of their data. The draft regulation is not very concise on the information that must be disclosed to the people who are affected by AI systems. The draft regulation requires that people be informed when they “interact with” an AI system or when their emotions or gender, race, ethnicity or sexual orientation are “recognised” by an AI system. This doesn’t not include all data and interactions that patient can have and provide to AI systems. This shortcoming should be addressed. 

The proposal allows providers of AI systems to use sensitive data to ensure that there is no bias on the data used and the results provided by the AI systems, but this information and the use of this data is only accessible to regulators upon request. In our opinion the legislative proposal should, however, have a stronger definition and better requirements when it comes to using data from the AI systems, especially for sensitive data like health information.

Regarding more technical comments, From EAHP we believe that in page 33 paragraph 68 the conformity assessment needs to be done at the latest 3 months prior the AI system is places in the market. In addition, we believe that the legislation should also apply to AI systems developed for military purposes. Finally, a guidance needs to be included on chapter 1 article 57 to describe the background required for the Board.
",PUBLISHED,micro,Gonzalo,COM(2021)206,2021-08-02 10:25:43,withinfo,24212003,closed,82950919755-02,,
en,2665207,BEL,business_association,Alliance for Internet of Things Innovation (AIOTI),Filipovic,"The Alliance for Internet of Things Innovation (AIOTI) appreciates the use of a Regulation as a legal instrument to help building an EU digital Single Market for trustworthy AI and to avoid regulatory fragmentation.

AIOTI welcomes the risk-and New Legislative Framework (NLF) based approach and to introduce specific rules for specific uses, rather than legislating the technology as such. We recommend that this framework is maintained during the legislative work on the file.

AIOTI supports that the proposal seeks to create obligations for all entities involved in the design, development, and deployment of AI. However, given that extensive exchanges between stakeholders across the AI value-chain will be necessary for the purposes of appropriate compliance and enforcement mechanisms, we stress that the corresponding obligations should be placed on the entities best placed to respond to them.

We welcome that the Proposal allows for self-assessment in most cases, which benefits the development of industrial AI, a strategically important sector in Europe. We also support the use of the principles of the NLF which has proven to be a technological-neutral way of regulation.

More we say in our enclosed contribution.
",PUBLISHED,micro,Damir,COM(2021)206,2021-08-02 08:59:44,withinfo,24212003,closed,380738729287-22,,
en,2665205,DEU,trade_union,German Education Union (GEW),Ludwig,"The German Education Union (Gewerkschaft Erziehung und Wissenschaft, GEW) supports the position of the European Trade Union Committee for Education (ETUCE) on the EU Regulation on Artificial Intelligence.",PUBLISHED,medium,Carmen,COM(2021)206,2021-08-02 08:00:59,withinfo,24212003,closed,778397734974-89,,
fr,2665170,FRA,business_association,Hub France IA,Chopinaud,"Le Groupe de travail Banque et Auditabilité du Hub France IA, qui regroupe des experts IA de trois grandes banques françaises, BNP Paribas, la Banque Postale et Société Générale, souhaite apporter une réponse à la Commission Européenne dans le cadre de la consultation ouverte sur le projet de régulation des systèmes IA.

Les membres du groupe de travail soulèvent un ensemble d’interrogations et apportent des cas d’exemples de systèmes IA qualifiés à « haut risque » relatifs aux applications déployées dans le secteur bancaire, en analysant les problèmes identifiés.

Veuillez trouver en pièce jointe un document de réponse.
",PUBLISHED,micro,Caroline,COM(2021)206,2021-07-30 16:40:15,withinfo,24212003,closed,,,
en,2665169,ITA,company,Intesa Sanpaolo,PASSAMONTI,"Intesa Sanpaolo, one of the top banking groups in Europe, welcomes the opportunity to respond to the European Commission’s (EC) Artificial Intelligence Act (AIA). This position paper is intended to present its key recommendations to the EC in the context of the have your say procedure on the AIA. The paper is divided into the following three sections: (i) general comments on the application of the AIA to Financial Services and Codes of conduct; (ii) technical comments on the requirements laid down in Title III of the AIA; and (iii) detailed remarks concerning the definitions provided for by Art. 3 and some requests for clarification.
Integration into the Capital Requirements Directive (CRD):
The conformity assessment procedure and some of the providers’ procedural obligations under the AIA are integrated into the procedures under the CRD. We request further clarity as to how the interaction with the CRD is intended.
o A clarification is needed concerning what is meant by “limited derogations” in Recital 80, since we do not see any other explicit reference to this in the AIA. If it is referred to Art. 17.3 and 29.4 the wording should be less ambiguous.
o We believe that it should be added the reference to “point 5(b)” in the provisions that require an integration with the CRD. Otherwise, the AIA would be extending the material scope of the CRD also to non-credit-related use cases. For this reason all the references to the CRD should be explicitly limited to “creditworthiness and credit scoring”.
o To ensure legal certainty, we suggest clarifying the concept of creditworthiness assessment and credit scoring in the AIA and to specify which consumer credit product are in scope.
o We consider that, in line with Recital 37, it should be considered as high-risk only those systems used to evaluate the access to credit lending and not the system put into service for phases following the initial disbursement of the loan.
Voluntary codes of conduct:
The requirements for high-risk AI systems are likely to be disruptive applied to non-high-risk AI systems and impose disproportionate obligations. 
o We suggest replacing the reference to “Title III, Chapter 2” in the first paragraph of Art. 69 with proportionate and tailored requirements for non-high-risk AI systems, agreed on a voluntary base.
AI techniques and approaches and amendments to Annex I:
o We suggest the EC to eliminate letter (c) Annex I in order to exclude at least the statistical approaches from the definition of AI.
o We appreciate that the EC acknowledges the need to update the list of AI techniques. However, we recommend the EC to establish a non-retroactive application of the law in order to avoid disservices and inefficiencies.
Data set requirements:
These requirements represent a concern in our view, since it is still highly disputed, and somehow unsolved, also at scientific level, how to achieving such a result.
o Standards that are now under development on bias mitigation should be shared by the EC.
o The EC should specify the meaning of “relevant, representative, free of errors and complete” as well as the “appropriate statistical properties” and how to concretely execute it.
o In particular, we would like to ask the EC to specify the meaning of “free of errors” and whether it is referred to ‘data collection and errors recording’ or instead to ‘detection of potential biases’. In any case, it is technically almost impossible to have a dataset free of errors.
Transparency and provision of information to users and human oversight:
o Since this is a complex and debated matter, we would like to ask for a clarification on how the interpretation of the system’s output should be granted and if there are specific techniques that should be preferred to others. 
o Human-oversight could be useful to support the monitoring, but it seems unpractical to expect it for every decision made by an AI system.(Please Refer to the Attached Paper)",PUBLISHED,large,Francesca,COM(2021)206,2021-07-30 16:34:47,withinfo,24212003,closed,24037141789-48,,
en,2665168,DEU,business_association,etami,van der Smagt,"etami is the European organisation for “ethical and trustworthy artificial and machine intelligence”, set up as a joint project in 2021. Members of etami include ABB, Atos, AVL, Continental, Deutsche Bahn, DFKI, ELTE University Budapest, KU Leuven, Leonardo, Siemens, TU Berlin, UnternehmerTUM, Volkswagen, and Zalando. The consortium is led by Volkswagen.

The goal of etami is to lead on trustworthy and ethical AI, to create an industry standard of the same and pilot its certification strategies.

The attached document describes joint feedback of the etami consortium with respect to the AI Act, adopted on April 21, 2021.",PUBLISHED,medium,Patrick,COM(2021)206,2021-07-30 16:28:53,withinfo,24212003,closed,,,
pt,2665167,PRT,ngo,APDSI - Associação para a Promoção e Desenvolvimento da Sociedade da Informação,Secretariado,"1.           Esclarecer o balanço de responsabilidades entre prestadores de IA, distribuidores e utilizadores, especialmente para API’s de propósito geral e modelos de open source: como foi explanado, o AIA não distingue suficientemente as responsabilidades dos utilizadores de IA quando no desempenho do papel de distribuidor e as responsabilidades dos prestadores para com os seus consumidores. A não ser que tal seja claro na medida razoável, arrisca-se a ter um efeito dissuasor na publicação de modelos em open source e em API’s, que é tão importante para a inovação de IA e na sua adoção pela indústria. Deixamos algumas recomendações:
•            A diretiva de IA não fornece uma definição de ""distribuidor"". Por questões de clareza, seria útil que o fizesse. Uma definição sensata seria a de “distribuidor” para nos referirmos à entidade que faz com que o sistema de IA esteja disponível para uso num contexto operacional específico. Por vezes (ex: se o sistema está construído de forma personalizada para o distribuidor pelo programador) o distribuidor pode ser o mesmo que o prestador. Mas, noutros casos, tal não acontecerá se sistemas IA de propósito geral forem utilizados.
•            Os distribuidores devem suportar a responsabilidade primária de observância, conformidade, avaliação e monitorização post-market, pois só eles podem verificar as aplicações finais para as quais os seus sistemas estão a ser usados e outra informação adicional que tenha sido introduzida da formação do seu sistema. Ao contrário, seria igual a responsabilizar os fabricantes de tijolo por assegurar a integridade estrutural de uma torre, ao invés de os arquitetos, engenheiros e construtores que desenharam e construíram a mesma.
o            Para ser claro, o ónus deve ser colocado aos distribuidores em todas as circunstâncias, independentemente da marca ou da maneira precisa em que o sistema de IA foi obtido. Caso se esteja a utilizar IA de propósito geral, tirado da prateleira, numa operação de alto risco ou caso o sistema tenha sido modificado, só a organização que utiliza o sistema de IA é que terá conhecimento sobre como estará a ser utilizado o sistema.

2.           Rever a linguagem usada em standards inviáveis: É importante manter requisitos realistas, em concordância com as boas práticas e práticas viáveis da indústria. Enquanto concordamos com a direção dos requisitos para sistemas de IA de risco elevado, consideramos que alguma linguagem utilizada merece atenção acrescida de forma a evitar a criação de standards que são de facto impossíveis para qualquer fornecedor alcançar. Em particular: (Ver documento anexo)

3.           Esclarecer praticalidades de ‘due diligence’: existem várias áreas onde é necessária uma maior orientação quanto às expectativas de conformidade. Por exemplo: (Ver documento anexo)

4.           Reenquadrar requisitos desproporcionais. Em alguns casos, os requisitos são geralmente, ou até extremamente, desproporcionais, devendo ser alterados.
Especificamente:
•            O Artigo 64 (2) refere que “...mediante pedido fundamentado, deve ser concedido às autoridades de fiscalização do mercado o acesso ao código-fonte do sistema de IA”. Porém o código-fonte encontra-se protegido pela diretiva europeia de segredos comerciais, havendo sempre a possibilidade de métodos alternativos para verificar a performance de um sistema de IA (ex. auditorias internas/externas) tornando o acesso ao código-fonte supérfluo.
•            Consideramos como melhor opção a alteração parcial do nº2 do artigo 64 para o seguinte: “após solicitação justificada, os operadores ou implantadores de IA devem apoiar e equipar autoridades de fiscalização de mercado com os meios necessários de forma a facilitar uma testagem robusta (ex. auditoria internas/externas) nos casos que exijam conformidade com os requisitos”.",PUBLISHED,micro,APDSI,COM(2021)206,2021-07-30 16:27:46,withinfo,24212003,closed,TR 435520331024-09,,
en,2665165,CHE,company,Hoffmann La Roche,Brookland,"PLEASE SEE ATTACHMENT FOR FULL RESPONSE AND COMMENTS
",PUBLISHED,large,Thomas,COM(2021)206,2021-07-30 16:15:35,withinfo,24212003,closed,18940431725-51 ,,
en,2665164,NLD,business_association,STM,Russo,"STM welcomes the possibility to provide feedback on the European Commission’s proposal for an Artificial Intelligence (AI) Act, and its ambitions to lead globally in promoting the uptake of new technologies whilst ensuring that the highest levels of excellence and trust are guaranteed.

STM publishers are in a unique position in that they are both producers and users of data for AI purposes.

First, publishers are key providers of information and data on which AI is run. Relevant, high-quality input and training data for AI developers and systems form one of the key ingredients for high-quality, trustworthy and ethical outputs. Providing this corpus of data in required digital formats is a core expertise of publishers. By validating, normalising, tagging and enriching content, delivering material in robust, interoperable and globally consistent formats, and creating domain-specific ontologies, publishers ensure that information is a trustworthy high-quality input source with tremendous potential for use by AI systems across a broad range of applications.

Second, publishers use AI, either developed in-house or supplied by third parties, to support internal workflows and services for authors, editors, and reviewers. For example, AI is used in recommending journals to authors based on sections of manuscripts, streamlining submissions by carrying out technical and language checks, and identifying suitable peer reviewers. Many publishers use AI to detect plagiarism, spotting suspicious patterns in content. New attempts include using AI to identify image and data manipulation.

Efforts should be dedicated to promoting positive, understandable examples of AI applications to build public understanding and support and underpin sustainable long-term government investment in AI. To that effect, we offer some use cases related to our sector in our White Paper, attached herein.

The STM White Paper is an attempt to lay a solid foundation for an ethical and trustworthy implementation of AI in STM publishing and in scholarly communications at large. It includes best practice principles for ethical and trustworthy AI, including 1) Transparency and Accountability, 2) Quality and Integrity, 3) Privacy and Security, 4) Fairness and 5) Sustainable Development.

As for the proposal for the AI Act, STM would respectfully propose the following comments:
1.	We note that the definition of an AI system as provided in Article 3(1) in conjunction with Annex I and the definition of a high risk AI system as provided in Article 6(2) in conjunction with Annex III may be too broad. We would welcome further discussions about these definitions and in general about the scope of the AI Act.

2.	Compatibility with existing legislative instruments, including Union rules on fundamental rights, intellectual property rights and product safety, will need to be ensured. In many instances, sector-specific frameworks will still be applicable to new technologies - with no need to re-invent the wheel nor to have new rules supersede existing ones. The perception that new legislation could override what already in place, e.g. the acquis on data protection, could erode users and producers’ trust.

3.	We welcome the focus put in the proposal on non-legislative efforts. In particular, we believe that existing industry standards and protocols will be fundamental in moving these efforts forward, and appropriate funding and support should be channelled towards initiatives developed over time by practitioners and communities of use. Industry schemes, as well as intellectual property rights, might also play a role in incentivising and guaranteeing the high-quality of data sets as aimed under Article 10. Coordination across stakeholders to ensure alignment, minimise burdens and streamline activity should be ensured.

STM already contributed to previous public consultations and looks forward to further discussing the proposal and engaging with the co-legislators.",PUBLISHED,micro,Claudia,COM(2021)206,2021-07-30 15:39:25,withinfo,24212003,closed,98356852465-08,,
en,2663486,BEL,trade_union,ETUCE,Di Ridolfo,"The European Trade Union Committee for Education (ETUCE), Social Partner in education at the EU level,  representing 127 Education Trade Unions and 11 million teachers in Europe, welcomes the Commission’s proposal for the AI Regulation as it sets the ground for the first comprehensive EU regulation on Artificial Intelligence to ensure a controlled development of AI tools in education and address the risks connected to their use by teachers, academic, other education personnel and students. 

Education as High-Risk sector: 
ETUCE welcomes the classification of AI tools in education as high-risk and the setting of stricter legal requirements for the AI tools used in the education sector. As for the setting up of a risk management system, ETUCE calls for clear and binding measures, including ethical guidelines, to address the risks that AI tools pose concerning transparency, accountability, intellectual property rights, data privacy, cyber-safety, equality and environmental protection.

Governance: 
An effective implementation of the AI legislation in education requires the meaningful involvement of teachers, academics and education staff as co-creators of AI tools in education. It is therefore crucial that education social partners are actively involved in the activities of the proposed European Artificial Intelligence Board through regular consultations and meaningful social dialogue to monitor the implementation of the Regulation and address the risks related to the use of AI in education. 

The role of teachers: 
ETUCE calls for the AI Regulation to interdict the AI tools that are designed to replace education personnel or can damage the social value and the quality of education. Besides, AI tools should not reduce the role of teachers to mere providers of instructions but rather serves as a supporting tool for the teaching profession while preserving the professional and pedagogical autonomy and academic freedom of teachers and academics. 

Transparency and AI literacy: 
ETUCE highlights the need to improve the importance of digital skills, AI literacy and data literacy in educational curricula and raise awareness on the risks related to the use of AI tools in education. While the AI Regulation blandly mention to the possibility of providing users with training on Artificial Intelligence, ETUCE emphasises that it is crucial that sustainable public funding are provided at national and European level to ensure that education personnel receive up-to-date and free of charge continuous training and professional development on the use of AI tools in accordance with their professional needs.

EdTech, IPR and data privacy : 
ETUCE calls for further public responsibility from national governments to develop and implement public platforms for online teaching and learning to protect the public value of education. Public platforms should respect professional autonomy of education personnel without creating pressure on teachers and education personnel regarding the education material and pedagogical methods they use. It is also essential to protect the accountability and transparency in the governance of public education systems from the influence of private and commercial interests and actors.  

Equality and inclusion: 
The lack of diversity among professionals responsible for designing, testing and training the algorithms and data of AI tools translate in the presence of biases in AI tools, leading to a detrimental impact on inclusion and equality in education. ETUCE urges the provision of adequate public investment to encourage more diversity in the STEAM sector and ensure that AI tools are designed and used with the full representation of the wide society. ETUCE also suggests to further explore how AI systems can act as supporting tools to detect and counter cyber-violence, cyber-bullying and cyber-harassment.

Full ETUCE feedback available in the attachment.
",PUBLISHED,small,Martina,COM(2021)206,2021-07-30 11:38:05,withinfo,24212003,closed,72197913011-06,,
en,2663405,FRA,company,IDEMIA,,Please find attached IDEMIA's comments. ,PUBLISHED,large,,COM(2021)206,2021-07-30 11:27:40,anonymous,24212003,closed,,,
en,2663398,USA,company,ResMed,Korwek,Please see ResMed's consultation response for the Artificial Intelligence Act attached below. We thank the Commission for the opportunity to provide feedback on the proposed legislation and welcome further engagement on the regulation of AI in Europe.,PUBLISHED,large,Justine,COM(2021)206,2021-07-30 10:54:21,withinfo,24212003,closed,209143033529-94,,
en,2663395,BEL,business_association,European Federation of Pharmaceutical Industries and Associations (EFPIA),Tyszkiewicz,"EFPIA applauds the Commission for launching the first ever legal framework on AI that elaborates on a European approach for AI with an aim to promote Europe’s innovation and industry capacity in AI that could bring benefits to entire healthcare system. We welcome a risk-based approach to AI with an oversight proportionate to the intended use and led by defined risk categories. Healthcare is one of the most regulated industries with strict ethical and governance rules and evidence-based regulatory decision-making. EFPIA supports the Commission’s approach to regulate AI based on risk, overall acknowledging CE marking system for high-risk applications in healthcare which is already addressed by MDR/IVDR. We would welcome an alignment of the risk levels described in the proposal with MDR for consistency. Under a risk-based regulatory system and in consideration of the scope of use, some AI-driven software may be categorised as low or intermediate risk under the MDR, whereas the draft proposal appears to classify all such devices as high risk. A generalised approach can decrease the competitiveness on the market due to providers having difficulty in fulfilling market access regulations and therefore jeopardising the uptake of AI-driven solutions by operators. AI has a broad application in the pharma supply chain including R&D, manufacturing, launch, post marketing surveillance as well as opportunities across the patient journey. To incentivise the investments in AI solutions in Europe, we recommend a thorough assessment of the proposed requirements versus the support offered to providers to navigate through the new rules. To assure high quality AI solutions and to safeguard the EU’s competitiveness in the international AI marketplace, easy yet compliant access to a significant amount of high quality and representative data will be prerequisite to ensuring highest levels of safety and robustness of AI and reducing bias, discrimination. Industry invests (e.g. via IMI) and supports the “FAIR-ification” of data to build up on existing sources and to make the machine-readable metadata findable for automated discovery.  The Commission’s plans to create a EHDS promise to unlock the power of data for AI-enabled healthcare and a well-functioning EHDS should incentivise data sharing and harmonise applicable rules to remove barriers to the collective benefits across Europe. In order for all stakeholders, including providers and operators, to train, test, develop and apply a trustworthy, reliable and ethical AI system, clear rules favouring industry data access and processing within EHDS should be laid out; and full benefit should be made of all measures in support of innovation such as AI regulatory “sandboxes” which may involve the further processing of personal data for a limited period of time under well-defined conditions. Emerging AI methods involving the decentralised development of machine learning algorithms would enhance user and stakeholder privacy and should be investigated. We should avoid fragmented implementation. Therefore, we welcome the creation of a European AI Board tasked with coordination of collaboration on AI across the EU. We emphasise that the evolving positive ecosystem of AI must be built together. EFPIA wishes to underline the importance of all key stakeholders being engaged in this initiative at the highest level and encourages early agreement on public-private platform to allow for structured dialogue with the Board. Co-creation would empower regulators and policymakers to gain vital insight as much as it would encourage stakeholders to gain ownership of the process and its effectiveness.Finally, we call the EU policymakers to engage with international partners throughout the legislative process to take into consideration how the rules would apply internationally.  Any regulation should ensure an equal playing field for both local and global players while not stepping away from core European values. ",PUBLISHED,medium,Aneta,COM(2021)206,2021-07-30 10:47:24,withinfo,24212003,closed,38526121292-88,,
en,2663391,ESP,business_association,ASNEF,Establecimientos Financieros de Crédito,"ASNEF, the Finance Houses Association of Spain, represented by the Secretary General, Mr. Ignacio Pla Vidal, and duly registered in the Transparency Registry with nº 11218815591-29, submits the following observations:

1. Concerning recital 37, the proposed Regulation mentions the following:
""Considering the very limited scale of the impact and the available alternatives on the market, it is appropriate to exempt AI systems for the purpose of creditworthiness assessment and credit scoring when put into service by small-scale providers for their own use”.
We consider it of utmost importance that the meaning of ""small-scale suppliers"" and ""for their own use"" be specified in the context of this exception to consideration as high risk and the application of the consequent requirements and obligations.

2. Secondly, we ask for further clarification on how those “high-risk” systems that are already in use by the institutions will need to adapt to the new regulation.

3. Finally, we request a wide transitional period to be included, so that entities that are already using Artificial Intelligence for the creditworthiness assessment and credit score can adapt to the new requirements.
",PUBLISHED,small,Asociación Nacional de,COM(2021)206,2021-07-30 08:45:55,withinfo,24212003,closed,11218815591-29,,
en,2663380,USA,ngo,The Information Accountability Foundation,ABRAMS,Regulation needs to be amended. ,PUBLISHED,micro,martin,COM(2021)206,2021-07-29 22:34:17,withinfo,24212003,closed,,,
en,2663375,FRA,business_association,Secure Identity Alliance,de Labriolle,"Facial Recognition Technologies: What’s at Stake and Why

On April 21, 2021, the European Commission proposed new rules and actions on the development and use of artificial intelligence (AI) systems with the stated objective to “turn Europe into the global hub for trustworthy artificial intelligence”.  

Following a risk-based approach, the proposals will ban those AI systems considered a clear threat to the safety, livelihoods and rights of people. Others will be rated on a risk scale from high to minimal. In particular, all remote biometric identification systems are considered high risk and subject to strict requirements.

Secure Identity Alliance (SIA), whose members are European and global leaders in this fast-emerging space, are supportive of the principles to ensure excellence and trust in AI systems, and believe a fit-for-purpose framework has the potential to become an adopted standard around the world – as has been the case with the General Data Protection Regulation. 

However, SIA believes that the rules governing the development and provision of AI systems should be proportionate. 

As the proposals stand today, there is a concern that some rules threaten to stifle innovation and therefore undermine the stated goals of the Commission and the position of Europe as a leader in this critical field of technology. 

ex-ante conformance assessment threat to IP & sovereignty  

AI systems intended to be used for real time, remote biometric identification are classified as high-risk under the proposals and would require an ex-ante conformance assessment. This, SIA believes, represents a threat to Intellectual Property if not properly managed with trusted third parties – a concept to be clarified.

This may have a negative impact on the levels of AI system functionality and performance available to European users and government agencies such as national security and law enforcement agencies as developers may choose to bring lower performance, less IP-sensitive versions of their technology to the EU in order to avoid the risk of IP leakage.

In addition: 

•	The rules imply a requirement to grant assessors access to sensitive data that are covered by non-disclosure agreements, and would seem to run contrary to GDPR compliance 
•	Similarly, the rules include a clause granting access through APIs. This represents a significant vulnerability in terms of IT security, and a significant expense for developers 

The rules lack critical definitions when it comes to identifying what a ‘relevant, representative, free of errors and complete’ training dataset is. At the same time, there is little scientific evidence that simply balancing training datasets removes the need for ex-post performance assessment, (ex post assessment being the state of the art practice for performance but also since a few years for bias measurement as well).The EU should facilitate the access to high data quality for training purposes, for instance by fostering the set-up of a large EU test database, with the adequate control / encryption systems, that could be used by the respective EU players to train their AI algorithms

AI definitions creating an uneven playing field 

While the definition of AI is necessarily wide in the existing draft, trying to define an emerging technology is challenging. Any definition could be rendered obsolete as new techniques emerge and are implemented. 

This would create a significant distortion of competition between developers of AI applications as per definition of the draft, and those developers leveraging other, more modern techniques, not directly addressed by the regulation for the same, potentially high-risk applications. By specifying what AI is and what it is not, the proposals threaten the EU goal of a wide-reaching, technology agnostic regulation.


Building an advanced ecosystem 

SIA supports the provision of legal, trusted identity for all and the development of inclusive digital services. (...) See attached doc",PUBLISHED,large,Stephanie,COM(2021)206,2021-07-29 21:01:30,withinfo,24212003,closed,,,
en,2663367,BEL,business_association,Agoria,,"Full position as attachement

First, Agoria suggests refining the scope, by adjusting the proposed definition of AI systems, the classification of identified high-risk AI applications and the allocation of roles. The scope of the regulation is essential, requiring clear definitions that are easily interpretable and applicable. The relevant stakeholders must be able to assess whether their systems and applications are subject to the regulation and which rules apply. 

Second, Agoria proposes to clarify the possibilities to amend the annexes to the regulation. Additional clarification is required as to the relevant criteria, consultation procedures and implementation process. Modifications to the regulation and in particular to the scope would compromise legal certainty and predictability for the stakeholders, as well as their application of the regulation. Therefore, modifications to the regulation would require adequate consideration and consultation, which should be duly specified in the draft. It is vital that industry stakeholders are involved in the process of modification in order to provide legal clarity and bring trust. 

Third, the timeline will be important for the providers, for the application of both the regulation in its initial version and the modifications made afterwards. 

Fourth, Agoria suggests focusing on the intended uses of the AI applications, as determined by the
providers. The concept of “reasonably foreseeable misuse” will undoubtedly make the application of
the regulation more difficult for providers and result in discussions between the stakeholders. Also,
the providers should not be liable in case of the users’ misuse of the AI application, especially if such
misuse is intentional or if the provider has no means of avoiding this risk.

Fifth, as regards the support for innovation, Agoria notes that the proposal recognized the
administrative and financial impact it has on providers of AI systems. Agoria expects that the
proposed support mechanisms will not suffice to ensure that our European companies can continue
to innovate. In general, most SMEs believe that this regulation will put them at a huge disadvantage,
as their size and limited resources will make it difficult to comply with this regulation. This matter
should get more attention, as often it is the small-scale providers that drive AI innovation and build the
applications needed to improve the quality of our lives.

On the measures for small scale providers and users, Agoria supports the proposal although we
consider that these measures need to be extended. While these small-scale providers and users have
priority access to legal sandboxes, participating in them will require a huge time investment from their
side and Agoria is of the opinion they should be financially encouraged to participate. It is positive that
there will be a reduction of the cost on third-party conformity assessment, although this should be
further extended to specific funding for companies that undergo a self-assessment procedure. In
Belgium we see that start-ups, scale-ups and SME’s develop many of the most innovative AI
applications. We think that this regulation will put them at a competitive disadvantage compared to
bigger companies that already have a lot of experience with compliance mechanisms.

As a final point, the European Union should consider that this regulation will have a considerable
impact on its place in the AI world. A negative impact on AI applications that could have a positive
impact on our society should be avoided at all costs. If the regulation hinders innovation and deters
investments in the EU markets, the providers will offer their applications elsewhere.
The aim to build trust in AI is crucial and so is mitigating the potential negative impact of AI, while this
should not prevent the EU from reaping all the positive benefits that AI can bring to our society.


",PUBLISHED,medium,,COM(2021)206,2021-07-29 16:01:54,anonymous,24212003,closed,68004524380-10,,
en,2663366,BEL,academic_research_instittution,Centre for Information Policy Leadership (CIPL),Laneret,"CIPL welcomes the Consultation on the European Commission’s Proposal for a European Artificial Intelligence Act to feed into the EU legislative process. CIPL is pleased to see that the AI Act incorporates several recommendations made in CIPL’s paper on Adopting a Risk-Based Approach to Regulating AI in the EU. In particular, CIPL welcomes the Act’s risk-based approach that would apply to high-risk AI use cases while not regulating the AI technology itself or entire sectors. CIPL also welcomes the proposed use of harmonised standards and industry self-assessment of product conformity, as these mechanisms have proven successful in driving innovation and developing safe and trusted technologies in the EU market. CIPL also welcomes the measures designed to support innovation, in particular by providing a statutory basis for regulatory sandboxes. Finally, CIPL is pleased to see that some of the requirements outlined in the AI Act align with some existing industry practices, which set a high bar to ensure that AI is developed and used responsibly.

CIPL regrets, however, that the AI Act does not sufficiently account for imperatives such as providing for outcome-based rules; clearly enabling organisations to calibrate compliance with the requirements based on risks and benefits of the AI system; rewarding and encouraging responsible AI practices; leveraging takeaways from regulatory sandboxes; and clarifying that the AI Act’s oversight and enforcement provisions should also be risk-based.

CIPL reiterates that for the AI Act to be effective in protecting fundamental rights while also laying a foundation for a new era in EU innovation, it needs to be flexible enough to adapt to future technologies. Further, the Act must not be overly restrictive so as to avoid suppressing valuable and beneficial innovations and uses of AI across a range of industries and sectors including public health or environment. Finally, the AI Act would benefit from targeted adjustments to better clarify the balance of responsibilities of AI providers, deployers and users, particularly for general purpose AI and open source AI models.",PUBLISHED,micro,Nathalie,COM(2021)206,2021-07-29 15:55:38,withinfo,24212003,closed,980643334936-37,,
en,2663361,DEU,other,EIT Health,Conning,"•	EIT Health welcomes the proposed AI Regulation and its ambition to create horizontal legislation overseeing AI and to maximise the safety and trustworthiness of this technology. We welcome the creation of a risk-classification of AI and prohibition of certain AI in alignment with the OECD, as this type of risk management is vital to ensuring uptake of this important technology.
•	With the importance of AI in the healthcare context in mind, the AI Act should support – not supress – innovation. Additional requirements and conformity assessments imposed by the foreseen legislation should therefore be combined or simplified to the greatest extent possible, and accompanied by clear guidance and support to innovators of all sizes to provide certainty and ensure they are not inhibited by additional regulatory requirements.
•	To avoid delays both in terms of innovations coming to market and patient access to new technologies and treatments, notified bodies and other authorities should be supported by the EU to ensure they are able to assess AI coming to market quickly whilst complying with the necessary safety assessments. 
•	We welcome the European AI Board but stress the need for cohesion with other EU level boards to ensure complete harmonisation of legislation to reduce regulatory burden. The proposed independent expert group is a positive inclusion, and EIT Health would welcome the opportunity to participate to provide insight and real-world evidence generated from its network of health innovators.  
•	Other stakeholders within the healthcare ecosystem need to be supported in the uptake and use of AI, notably patients and healthcare professionals. Appropriate education and upskilling on all the relevant aspects of AI and its utility in healthcare should be incorporated within existing and future curricula, and continued professional education for healthcare professionals is a vital foundational step. EIT Health’s education pillar has trained or educated 43,000 graduates and professionals, through activities including Summer Schools, Fellowships and Master’s/PhD programmes. On AI specifically, the EIT Health project HelloAI RES Online (https://eithealth.eu/project/helloairis/) was designed to introduce medical professionals to AI and equip them with the skills needed to allow them to benefit from the ongoing evolution of the healthcare field.
•	The proposed Regulation must have appropriate flexibility to accommodate sector specific issues, notably in health, where the risks are greater and the level of regulation already high. The proposal finds itself overlapping with many health sector specific and horizontal legislation (DGA, EHDS, MDR, IVDR, EHDS) and appropriate measures must be taken to ensure that health innovation is still allowed to thrive whilst of course ensuring patient safety. 
•	Further clarity is needed on how healthcare AI outside of the scope of the MDR and IVDR will be regulated, notably when used for research purposes.
•	Finally, EIT Health welcomes the assurances created by looking at the whole value chain, and calls upon the Commission to find the right balance between support for innovation and building public trust in clear regulation, enabling safe and secure exchange of data and the development of new innovative AI.
",PUBLISHED,medium,Sameena,COM(2021)206,2021-07-29 15:18:15,withinfo,24212003,closed,781385241347-65 ,,
en,2663359,DEU,company,Bayer AG,van Randenborgh,"Mastering the new opportunities and challenges of innovation in life sciences is key for societies in the 21st century. One important element is a supportive policy and regulatory framework that fosters scientific advancement while at the same time ensuring trust and a high safety standard for human health and the environment. Bayer stands at the forefront of the “Bio-Revolution” with Artificial Intelligence (AI) being a crucial technical enabler for drug development, applications in Cell- and Gene-Therapy, New-Breeding-Techniques or Data-Driven-Farming.
 
The way AI is regulated is a decisive factor for EU’s future competitiveness in the international field. Bayer shares the EU Commission’s ambition of making Europe a world-leading destination for the development and deployment of trustworthy AI.  In order to maintain Europe’s competitiveness, the Act should not impose unproportionate duties on European AI providers. To create an even playing field for European AI providers with the rest of the world, the Act itself must be competitive to non-European AI legislation. 

As an active member of the EU Commission’s AI High-Level Expert Group 2018-20, Bayer has been working on human centered policy proposals for the regulation of AI that ground on a risk-based approach combined with a sector-specific regulation. We therefore highly welcome that the EU Commission has put the risk-based approach at the center of the regulation and are convinced that looking at the risk of applications is key to strike the balance between necessary regulation and innovative freedom. 

Below, we present our initial assessment of the proposal:

Definition
We welcome that the definition of AI in the proposal builds on the work of the OECD with its technology-neutral character allowing for the fast developments of AI technologies. However, parts of the proposed definition focus on outdated aspects of AI, such as logic- and knowledge-based approaches and optimized searched methods (Annex 1 (b) and (c), respectively). Rather, the definition of AI should focus and include current and future aspects of AI to avoid overly broad application.

Regulatory Environment
We understand that AI is a cross-cutting technique that falls under various existing legislation but are concerned that an overly complex regulatory environment is created. Especially for AI systems that are in scope of sector specific legislation (e.g. medical devices) three regulatory dimensions would apply: the proposed AI Act, the Medical Devices Regulation and the General Data Protection Regulation (GDPR) with overlapping requirements, such as for technical documentation. Furthermore, the proposal does not solve the tension between GDPR principles, such as data minimization, purpose limitation or storage limitation, and the full deployment of the potential of AI. Clear guidance from data protection authorities is required on the processing of personal data in the context of AI.

Validation – Human domain – Risk approach 
High quality data is essential for the successful uptake of safe AI in Europe. To ensure the consistency between training data and production data, the validation mechanisms could extend to include ground-truth evidence to reproduce the underlying data generating process. Further, the inclusion of human domain experts in certain cases would also be beneficial to detect incongruences between AI output and rational conclusions. 

Governance structure & penalties
The successful development and deployment of AI in Europe will also depend on the governance requirements for the underlying data. Bayer asks for more clarity on the governance structure and to whom, for instance, serious incidents must be reported.
",PUBLISHED,large,Charlotte,COM(2021)206,2021-07-29 15:02:22,withinfo,24212003,closed,3523776801-85,,
en,2663358,FRA,company,ORANGE,DOUTRIAUX,"Orange’s preliminary position on the Artificial Intelligence Act

Orange welcomes the opportunity to comment on the European Commission’s proposal for an Artificial Intelligence Act. This proposal, a first of a kind as highlighted by the EC, will have significant implications for AI ecosystems once it enters into force. And with AI being increasingly part of manufacturing, logistics and services sectors, it will have far reaching implications for EU economies. It is hence of utmost importance that the AI Act strikes a right balance between the need to ensure that AI applications implemented and used in the EU follow clear ethical principles, including strong governance and transparency rules, and the necessity to ensure that EU players thrive and innovate in the global race for AI leadership without undue burdens.

Orange is committed to applying ethical principles in AI. We have been actively involved in the High Level Expert Group that crafted ethical principles to advise the Commission. Orange and Arborus  have also revealed the first International Charter for inclusive AI whose aim is to ensure AI is designed, deployed and operated in a responsible and inclusive way. More recently, Orange set up a “Data and AI Ethics Council”  made up of 11 independent recognized experts of the field. 

As a general comment, we support the risk-based approach adopted by the EC in its proposal, in line with Orange’s initial reactions to the Commission’s white paper on Artificial Intelligence . In particular, Orange considers that the prohibition of some unacceptable-risk AI as outlined in the proposal is aligned with EU values and ethical principles. In addition, Orange approves the EC’s proposal to set transparency obligations on low-risk AI that interacts with humans.

There are however some significant concerns, which could put undue and possibly excessive burden on significant parts of the European AI ecosystem:
- The definition of AI as currently drafted includes a number of techniques, including statistical methods, or expert systems, which have long been used in a number of IT developments without raising any specific issue. This means there is a risk that a significant part of existing digital activities would be considered as AI and have to comply with new regulation especially for high-risk, whereas this is not the core objective of the AI Act; 
- The requirements for high-risk AI may be costly and difficult to achieve. Record-keeping obligations might for instance represent excessive amounts of data to be stored. 
There is also a risk of deterring innovation in the EU since burdensome requirements could possibly hinder the development of AI activities in the EU compared to other parts of the world that have no such constraints. The EU could turn out to be an importer of innovation developed elsewhere. 

Orange therefore advocates that:
- The AI definition should be streamlined, in order to avoid capturing standard IT systems or algorithms, and focus solely on the categories that are the most prone to ethical issues ; 
- The requirements should be streamlined to a principle-based approach, while specific aspects should be tackled through existing product legislation or the forthcoming revised versions of the product safety and product liability directives; 
- To promote innovation, the requirements should favor a fair and sustainable competition between European companies and non-EU competitors.

More details are available in the attached preliminary position paper. 
",PUBLISHED,large,AURELIE,COM(2021)206,2021-07-29 14:57:46,withinfo,24212003,closed,76704342721-41,,
de,2663356,DEU,business_association,Vereinigung der Arbeitgeberverbände der Deutschen Papierindustrie e. V.,Dr. Berger,"Wir begrüßen den risikobasierten Ansatz der Kommission, der im Verordnungsvorschlag über künstliche Intelligenz skizziert wird, und teilen das Bestreben, KI sicher, rechtmäßig und im Einklang mit den EU-Grundrechten zu gestalten. 

Weitere Einzelheiten: Siehe beigefügte Stellungnahme der VAP.",PUBLISHED,small,Hans-Peter,COM(2021)206,2021-07-29 14:55:35,withinfo,24212003,closed,009318117132-67,,
en,2663348,BEL,company,Eurosmart,Dornier,"Eurosmart, the Voice of the Digital Security Industry, welcomes the Commission’s proposal for an Artificial Intelligence (AI) Act. Our association has been advocating for years for EU requirements covering AI systems. 

Eurosmart would like to comment on the following items -each of them is further detailed in the attached file:

•	Definition of AI systems
The current definition of an AI system is very narrow as it covers only software and not hardware. In addition, the list of techniques mentioned in Annex I covers technologies that would not usually be considered AI (e.g. traditional data analysis tools). 

•	Cybersecurity certification
Eurosmart calls on the European Commission to request ENISA to prepare European cybersecurity schemes for AI. Genuineness of data, access rights, security of supply chain, and cross-industry relations are elements to consider. 

•	Predictability
Standards and technical specifications must be clear regarding the necessary threshold of predictability. It is important to fund research projects in this area. 

•	Standards
Eurosmart enjoins the European Commission to issue a standardisation request to European Standardisation Organisations (CEN-CENELEC and ETSI) to support the AI Act. 

•	GDPR certification
There is currently no solution to GDPR-certify a device. Operational and technical requirements are missing for GDPR certification.

•	Auditability of data
Access to source code for auditability of data must be strictly regulated. 

•	Data sharing
Eurosmart supports the current initiatives to foster data sharing, such as the Data Governance Act. Data sharing should take the aging process into account. 

•	Risk-based approach
Clear guidelines are needed to help manufacturers know whether their products fall within the high-risk category. Personal authentication systems should not fall within this category.

•	Diverted/non-intended use of AI systems
Eurosmart enjoins the European Commission to consider situations whereby the initial intended use of an AI system is diverted to another use that falls into the high-risk or banned categories.

•	Real-time biometric identification in publicly accessible spaces
Eurosmart questions allowing for private companies a use case that is prohibited for public authorities.

•	Competitiveness
The EU needs its own structure to certify algorithms. It should also put the emphasis on an AI trust mark so that the AI Act becomes a competitive advantage and not an obstacle to export.


Please find below Eurosmart’s detailed feedback on the AI Act.
",PUBLISHED,micro,Camille,COM(2021)206,2021-07-29 13:27:17,withinfo,24212003,closed,21856815315-64 ,,
en,2663341,FRA,business_association,GFII,Marquet,"First of all, thanks to the European Commission to enable the GFII to answer this consultation on the draft regulation on AI.
 1) Readability of the text
The draft is rather complex to understand; understanding difficulties may then generate difficulties for being compliant, especially for SMEs and start up. 
 a) We invite then the legislator:
- to amend the structure of the document by separating:
- requirements dedicated to AI systems defined in the article 6 (1) / annex II and possibly Annex III (1)
- requirements dedicated to AI systems defined in the article 6 (2)
- requirements dedicated to AI systems defined in the annex III (6 to 8)
- to define the requirements from the AI provider point of view, ie the operator that will have to comply with the future regulation, with a logical ie process oriented redactional architecture, step by step, from the ex-ante conformity assessment, the permanent monitoring procedures to the ex-post controls.
 b) If the requirements integrate the intervention of a notified body, its role and responsibilities should be provided then and not in a separate chapter.
- not to require from the reader to jumb from one article to another to understand the one being currently read,
- to consider that not all AI providers that will have to comply with this future regulation are aware and comfortable with the Union Harmonization Legislation, regulation 2019/1020 etc.
 2) The definition of AI system (art 3 (1)) is unclear. As a matter of fact, not all users install an AI software internally. Some of them access the output (prediction, recommendation, decision, indicator …) only, the output being provided by the AI provider that has developed the AI system. In other words, we can have the software and the use of the software, the output only and the use of the output only, or both.
A clarification is then required.
3) Unclear notions and functions
a) In article 83(2), a clarification of what are “significant changes in their design or intented purpose” would be welcome, as well as a more precise definition of “substantial modification” (art 3 (23)). What is significant / substantial is rather subjective and difficult to comply with. 
b) The respective role of national supervisory authority (ies) and market surveillance authorities is not clear, especially for IA systems defined in the article 6 (2) and annex III. 
c) Relation between the AI regulation and the different Data legislations : cf the article 10 on data and data governance, a strong coherence between the future AI regulation and the existing (GDPR, PSI III) or future (Data Governance Act, Data Act)  legislations or other sectoral legislations will be key.
Since the beginning of the Open Data directive recast process, GFII underlines that, as well mentioned in the “Strategy for Data”, “the value of data lies in its use and re-use”, in particular by AI systems.

Practically speaking, it means that one same and unique right, identical for all re-users of data, generates de facto a restrictive right for some of the use cases, which looks quite counterproductive according to the objective of development of the data economy.

There are different categories of re-users, whose economic and legal environments are different, which can / could justify that data could be made available and re-used differently.
Of course, rights and duties should remain equitable, practical and explicit and data should circulate in the EU as well as between sectors.

This is why we would consider pertinent to have different levels of data access / re-use rights as well as different levels of legal rights and duties according to the purpose of re-use and reusers’ end-users.
Let’s take an example : notion of “natural persons” in Annex 3 :“AI systems intended to be used to evaluate the creditworthiness of natural persons or establish their credit score, with the exception of AI systems put into service by small scale providers for their own use” are considered as high risk AI",PUBLISHED,micro,Anne-Claire,COM(2021)206,2021-07-29 12:40:02,withinfo,24212003,closed,,,
en,2663340,FRA,company,SOCIETE GENERALE,GEORGES,Please see the attached file,PUBLISHED,large,Bernard,COM(2021)206,2021-07-29 12:39:21,withinfo,24212003,closed,34369111614-57,,
en,2663339,BEL,business_association,European Automobile Manufacturers Association (ACEA),Marazzi,"ACEA is thankful for the opportunity provided by the Commission to comment on the proposed AI Act, adopted on 21 April 2021. 

The present ACEA response, attached hereunder, builds on ACEA’s previous contributions to the Commission’s White Paper (12 June 2020, document No. F530044, link: https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12527-Artificial-intelligence-ethical-and-legal-requirements/public-consultation_en) and Commission’s inception impact assessment (9 September 2020, link: https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12527-Artificial-intelligence-ethical-and-legal-requirements/F550804_en).

The paper is structured into four main sections presenting the industry’s remarks on the current Proposal and questions to be clarified. 
The first section addresses our general remarks and questions on the notion of provider and user, their obligations and the scope of the AI Act. 
The second section includes our comments and requests for clarification as regard to motor vehicles and their safety components. 
The third section presents the remarks concerning all other AI systems falling within the direct scope of the AI Act and with relevance for automakers.
The fourth and final section addresses important questions on the mandatory technical requirements for high-risk AI systems and other provisions, in view of clarifying ambiguous aspects and improving the current regulatory text.

Please kindly refer to the document attached below.
",PUBLISHED,small,Marta,COM(2021)206,2021-07-29 12:28:11,withinfo,24212003,closed,,,
en,2663328,FRA,company,France Industrie,,Please find our position enclosed.,PUBLISHED,small,,COM(2021)206,2021-07-29 10:47:28,anonymous,24212003,closed,60974102057-03,,
en,2663324,BEL,company,EUCOPE,Louette,"Representing small and medium-sized companies active in life sciences, EUCOPE acknowledges the Commission’s efforts in proposing a legal framework for Artificial Intelligence (AI). We particularly welcome the commitment from the Commission to strive for a balanced approach, as stated in the explanatory memorandum of the proposal.
For AI to realise its potential, a key prerequisite is the establishment of a balanced and fit-for-purpose regulatory framework ensuring trust and adhesion of the European citizens.
Many of the AI-based solutions used in healthcare and by the health sector will be classified as medical devices (MD) or in-vitro diagnostic (IVD) medical devices, and therefore already covered by the Medical Devices Regulation (MDR) 2017/745 and the IVD Regulation (IVDR) 2017/746. We understand from Article 47.6 from the proposal that those devices will be exempted from an additional ex-ante conformity assessment, as, indeed, they would have already gone through a substantial conformity assessment most often via a Notified Body, before entering the market. We understand that this single Notified Body assessment will address conformity with the MDR or IVDR, whichever is applicable, as well as the AI regulation.  EUCOPE would welcome, however, more clarity on the interplay between the Commission proposal and Regulations 217/746 and 217/746 in that sense, as well as on the obligations for serious incidents reporting (Article 62.3 of the proposal) on what exactly the Commission intends as breach of obligations under Union law intended to protect fundamental rights. 
To avoid disconnects when sponsors attempt to comply with the different regulations, we call on the Commission to ensure good collaboration with its Medical Devices Coordination Group (MDCG) to discuss and iron out the interplay challenges between the different set of laws. We suggest that linkage between the various provisions of these regulations is explicit, so this interplay is clear.  We similarly suggest that the linkage between the AI regulation provisions and the General Data Protection Regulation (GDPR) provisions is explicit and clear, so that our industry can assess potential impacts of the interplay between these two regulations. 
We acknowledge the suggestions laid out by the Commission in Recital 68 and Article 47 of the proposal to allow for exceptional derogation from conformity assessment procedure. Such regulatory flexibility is crucial to cover for any emerging and urgent crisis, as demonstrated by the COVID-19 pandemic where the flexibility showed by the Commission and the EMA proved essential in developing vaccines or continuing clinical trials in Europe. 
We would also welcome more information on the relationship between the database as mentioned in Article 60 of the proposal and the newly established EUDAMED database for devices, as per the new devices Regulations. 
Also, we ask the Commission to take into account the administrative burden faced by small and medium-sized companies when different legal frameworks have so close connection. Any duplication of efforts should be avoided.
Finally, we would welcome more clarity on the annexes of the proposal and the definition of high risk, in particular confirmation that AI-based solutions used in healthcare and medical product development are not considered high-risk AI systems.  We would also suggest that the Regulation considers the potential for unforeseen consequences to the health care system, as well as scientific research and development of new and innovative pharmaceuticals and medical devices, when evaluating the AI regulation.  As the regulation is broad in nature and attempts to address the use of AI across all sectors, there is the potential for unanticipated impacts that could hinder scientific research and development. To avoid these unintended consequences, we respectfully ask that the Commission consider our industry when reviewing and revising the proposed regulation.
",PUBLISHED,micro,Laurent,COM(2021)206,2021-07-29 09:17:49,withinfo,24212003,closed,87600691525-93,,
en,2663310,USA,academic_research_instittution,Center for AI and Digital Policy (CAIDP),Rotenberg,"The Center for AI and Digital Policy (""CAIDP"") welcomes the opportunity to provide feedback as amendments to the text of draft Artificial Intelligence Act (“Proposal”) are considered.  This statement follows from CAIDP’s Statement to the European Commission, the European Parliament, and the European Council on April 20, 2021. 

	As we wrote in our initial statement on the Proposal, “this initiative may be the single most important legal framework for the digital economy to ensure the protection of fundamental rights.” We also wish to express support for the work of the European Commission to seek public comment on the Proposal. We believe that meaningful public participation in the development of AI strategies is a key indicator of the health of democratic institutions.

	The Center for AI and Digital Policy is a global research organization. In 2020 we published Artificial Intelligence and Democratic Values, a comprehensive review of the AI policies and practices in 30 countries.  We also created a methodology to assess AI national strategies. Our aim is to promote a world where technology promotes broad social inclusion based on fundamental rights, democratic institutions, and the rule of law.

In the CAIDP 2020 report AI and Democratic Values, we stated:

1.	Countries must establish national policies for AI that implement democratic values

2.	Countries must ensure public participation in AI policymaking and also create robust
mechanisms for independent oversight of AI systems

3.	Countries must guarantee fairness, accountability, and transparency in all AI systems


4. Countries must commit to these principles in the development, procurement, and implementation of AI systems for public services

5. Countries must halt the use of facial recognition for mass surveillance

	Our assessment of the draft EU AI Regulation is favorable. The draft EU AI Regulation will promote AI policies and practices and will bring more transparency to allow for ongoing evaluation and monitoring.  We also recognize the important step forward in the evolution of the internal market with the integration of fundamental rights compliance. As AI systems become more critical role for the digital economy, compliance with fundamental rights should be a necessary precondition for market participation. 

	We further fully support the objective to prohibit certain AI systems. Our review of country AI practices found that the clearest distinction between AI systems in authoritarian countries and AI systems in democratic countries is the use of facial recognition for mass surveillance. Such indiscriminate ongoing surveillance is intended precisely to coerce social behavior and to control populations. This AI technique has been used against political protesters and religious minorities, and will almost certainly be more widely deployed unless a clear prohibition is adopted. 

	Attached are the further recommendations of the CAIDP for the deliberations of the EU AI Regulation.",PUBLISHED,medium,Marc,COM(2021)206,2021-07-28 21:22:26,withinfo,24212003,closed,,,
en,2663295,BGR,other,No,Sotirova,"I need to talk from position to the roma muslimah. 
Im not used traditional clodres ekzatley fron scared. 
So this the conect with this new project about AI 
IF i use hijab like ekzample the sistems can understand mi face more easy. 
Automatic i wile have problems al ao mi dark face can be used by bad wey. 

I need freedom and i hope neq technology to be used not opresed people and give me him better life ",PUBLISHED,micro,GenoWeva,COM(2021)206,2021-07-28 16:54:33,withinfo,24212003,closed,No,,
en,2663289,GBR,other,UK Information Commissioner's Office,Hubbard,Please find feedback from the UK Information Commissioner attached.,PUBLISHED,large,Alex,COM(2021)206,2021-07-28 15:25:49,withinfo,24212003,closed,,,
fr,2663276,FRA,public_authority,ACPR,Dupont,"L’ACPR accueille très favorablement l’approche de la Commission qui recherche un cadre équilibré, garantissant les droits fondamentaux et encourageant le développement d’une IA de confiance.
Elle note comme particulièrement positif le projet de confier aux autorités de contrôle du secteur financier la mission de surveillance des IA à risque du secteur. Cette mesure permettra en effet d’assurer la cohérence du contrôle, d’une part, des exigences découlant de ce projet de règlement et, d’autre part, des obligations sectorielles de contrôle interne et de gouvernance qui s’appliquent aux processus intégrant les algorithmes d’IA.
L’ACPR souligne que la règlementation financière inclut non seulement des mesures de protection des clients mais également des mesures de gouvernance et de contrôle des risques destinées à assurer la stabilité et la confiance dans le secteur financier. Les risques de l’IA dans le secteur doivent donc être envisagés sous ces deux aspects, alors que le projet insiste principalement sur les droits fondamentaux et donc sur certains des risques seulement susceptibles d’affecter directement les clients du secteur financier.
En outre, l’ACPR s’interroge sur les raisons de la classification de l’activité de crédit dans la catégorie à haut risque. La législation européenne n’a pas institué de droit au crédit et les risques de discrimination, déjà existants avec les techniques traditionnelles de sélection et de tarification, lui paraissent comparables à ceux d’autres activités financières (épargne, assurance). Ils n’ont jusqu’à ce jour pas été considérés comme justifiant des mesures législatives européennes particulières.
L’octroi de crédit, tout comme d’autres activités du secteur financier, doit être maîtrisé par les entreprises du secteur au double titre du contrôle des risques et de la protection de la clientèle. C’est dans ce cadre qu’il conviendrait d’inscrire le contrôle de l’usage de l’IA dans ce domaine.
Au plan technique, l’ACPR approuve les principes édictés au chapitre II du projet. Leur mise en pratique, s’agissant notamment des risques résiduels (article 9), des jeux de données (article 10) ou de la mesure des biais (article 15), dépend d’un état de l’art très évolutif. Une attention particulière devra donc être portée aux standards techniques de mise en œuvre et à leur actualisation, afin qu’ils ne dénaturent pas l’équilibre entre protection et développement technologique recherché par le règlement. Un point peut-être à préciser dans le secteur financier sera la répartition des responsabilités entre fournisseur et utilisateur d’IA lorsque l’utilisateur est un établissement réglementé et que son fournisseur est un prestataire de services essentiels.
Les sujets que l’ACPR a identifiés comme prioritaires dans ses propres travaux sur l’IA sont ceux de la gouvernance, de l’auditabilité et de l’explicabilité des algorithmes (rapport de 2020 joint à cette réponse). À cet égard, la question de l’interaction homme-machine se pose dans de nombreux cas, même pour l’IA qui n’est pas à haut risque. L’ACPR se félicite que le projet de règlement aborde cette question dans l’article 14 (« contrôle humain »). L’ACPR s’interroge toutefois sur la justification du point 5 de cet article, qui requiert l’intervention d’« au moins deux personnes physiques » dans les processus d’identification biométrique. En premier lieu, le champ couvert par cette exigence semble englober de très nombreux cas d’usage du quotidien et pas seulement les cas de reconnaissance faciale. En second lieu, pour donner de réelles garanties, l’intervention humaine doit être pertinente compte tenu des processus effectivement mis en œuvre. En d’autres termes, imposer un critère quantitatif à l’intervention humaine est davantage susceptible d’augmenter les coûts que de garantir la maîtrise des risques liés à ces processus.",PUBLISHED,large,Laurent,COM(2021)206,2021-07-28 14:17:46,withinfo,24212003,closed,,national,authority
en,2663268,BEL,ngo,European Disability Forum (EDF),Hakobyan,"We welcome the European Commission’s proposal for regulating Artificial Intelligence (AI) in the EU. The proposed Regulation for AI will help ensure protection of fundamental rights of persons with disabilities in the context of new technologies. The Regulation can also help promote AI that will improve accessibility for persons with disabilities and support their participation in society. To ensure this, however, the Commission proposal needs significant improvements with strong safeguards against discriminatory AI systems and practices, and proactive measures to promote AI that will benefit accessibility and equality of persons with disabilities. 
In view of this, the EU AI Regulation must:
•	address discrimination, privacy, and safety and liability-related risks posed by AI 
•	include horizontal and mainstreamed accessibility requirements for AI systems irrespective of level of risk, including for AI-related information and instruction manuals, consistent with exiting EU accessibility legislation, notably with the European Accessibility Act
•	ensure that privacy and data protection of all persons with disabilities, including of persons with intellectual, psychosocial disabilities, or mental ill-health, are protected when their data is processed by AI systems
•	set effective, accessible for persons with disabilities, measures for individuals to be informed when their data is being gathered and possibility to inquire and object to processing of such data by AI systems. The possibility of objecting to one’s data collection should offer this option in a meaningful way. 
•	forbid:
o	use of AI for remote biometric identification by public and private entities in publicly accessible spaces
o	development and use of AI systems for military purposes
o	use of AI for emotion recognition by public authorities and private entities
o	use of AI for predictive policing 
•	limit the definition of biometric categorisation by AI systems to biometric features which are tangible and do not risk reinforcing stereotypes and false assumptions (e.g. assumption of someone’s gender based on pitch of their voice; categorizing subjective characteristics such as political opinion, sexuality)
•	limit the areas of high-risk AI use and instead ban several of the practices listed in Annex III after meaningful discussion between EU lawmakers and relevant civil society organisations, including organisations of persons with disabilities 
•	ensure independent third-party conformity assessment for all providers and users of (potentially) high-risk AI systems
•	include accessibility checks as part of conformity assessment 
•	include measures for third-party independent assessment of impact of AI systems and their use on fundamental rights
•	include a chapter dedicated to the protection of fundamental rights of citizens within the context of AI application, including measures to flag issues, file complaints, including collective complaints and complaints launched by civil society actors on behalf of individuals, and seek remedies in case of abuse 
•	ensure that such measures are accessible for persons with disabilities
•	ensure that EU-based AI providers and users whose outputs effect individuals outside of the European Union are subject to same requirements as those whose outputs effect persons within the Union
•	require AI developers to meaningfully involve experts with disabilities, accessibility experts, and other rights-holders, in the design and development of AI systems, and to ensure diversity of development teams
•	include requirements for proactive measures by the EU and Member States to support excellence of European AI development for the benefit of citizens, such as allocating funds for the development of AI-based assistive technologies for persons with disabilities, accessibility skill-building and digital rights awareness campaigns, and prioritizing projects led by organisations of persons with disabilities and accessibility experts. ",PUBLISHED,small,Mher,COM(2021)206,2021-07-28 12:37:26,withinfo,24212003,closed,57868523887-16,,
de,2663263,DEU,business_association,BDA - Bundesvereinigung der Deutschen Arbeitgeberverbände,,Please see our feedback attached.,PUBLISHED,medium,,COM(2021)206,2021-07-28 11:54:42,anonymous,24212003,closed,7749519702-29,,
fr,2663256,FRA,business_association,AFNUM - Alliance Française des Industries du Numérique,Grojean,Please find attached the feedback from AFNUM - The French Alliance for Digital Industries - for the Artificiation Intelligence Act. ,PUBLISHED,micro,Clara,COM(2021)206,2021-07-28 10:14:15,withinfo,24212003,closed,,,
en,2663157,CHN,company,"Hangzhou Hikvision Digital Technology Co., Ltd.",Chen,Please find HIKVISION submission in attachment. ,PUBLISHED,large,Vivian,COM(2021)206,2021-07-27 16:36:50,withinfo,24212003,closed,,,
en,2663127,BEL,ngo,Eurocities,bordelot,"The Artificial Intelligence Act proposal is a crucial legislative initiative to create the conditions for a safe development and deployment of AI, based on European democratic values and in respect of people’s fundamental rights across Europe’s cities. 

AI is an enabler of change for local governments. City authorities recognise the great potential of AI technologies for public services development and delivery in sectors such as health, education and transportation. City authorities positively evaluate the clear set of horizontal mandatory requirements for trustworthy AI allowing for both more ethics-oriented and effective application of AI. However, AI adoption in cities is a long and costly process. Good data collection, processing, management and opening is expensive and requires specific, high-level competences as well as new governance approaches. A broader uptake of AI technology locally requires intensive financial investment from the national and EU level to ensure adequate technological and skills development in city administrations.

Local governments are in favour of having an extensive definition of AI which encompasses any AI system that touches the single market allowing for a complete and effective regulation. Local governments positively evaluate the risk-approach taken in the artificial intelligence act and the position taken for some uses that are unacceptable and must be prohibited. City authorities also recognise the proposal has taken a significant step forward on facial recognition banning practices such as social credit and any other forms of social scoring. However, on large-scale surveillance in its provisions regarding real-time biometric recognition systems, an outright ban on mass biometric surveillance systems should be put in place until there is evidence that these systems are respectful of fundamental rights. 

Local governments positively value high-risk systems must comply with European health, safety, and environmental protection standards and stress the need for an inclusive and coordinated approach for the development of new common specifications that includes cities. For local governments citizens’ needs must be the starting point of the AI standardisation and common specification development process. City authorities are also in favour high-risk systems undergo conformity assessment however they consider the proposal allows a too wide scope for self-regulation by companies. The majority of requirements in the proposal rely on AI developers to implement technical solutions to complex social issues, which are likely self assessed by the companies themselves. For local governments, conformity with AI standards cannot be left in the hands of private companies providing AI technology. Therefore, for cities, the EU has to adequately invest in the appropriate, independent mechanisms to guarantee that all AI providers meet EU standards.

Local authorities support the creation of a European Artificial Intelligence Board, chaired by the European Commission, to issue recommendations and opinions related to the implementation of the regulation as well as to collect and share best practices. Through experimentation and early adoption of AI, city governments identify possible safety and fundamental rights risks and provide opinions and key recommendations for a correct implementation of the EU rules. With their access to universities, companies and infrastructures, and acting as open participation and collaboration platforms, using and making data and information available are best place to collect and share good practices.  Therefore, local governments should explicitly be involved as official stakeholders for consultation in the Board. 
",PUBLISHED,medium,Federica,COM(2021)206,2021-07-27 12:57:11,withinfo,24212003,closed,12493392840-79,,
en,2663125,BEL,business_association,APPLiA (Home Appliance Europe),ZAKRZEWSKI,"APPLiA would like to use the opportunity given in this consultation and comment on two important elements of the draft AI Regulation: 
1.	Article 6 - Classification rules for high-risk AI systems - Only products which basic safety features are secured ONLY by the AI system should be considered high-risk in this classification. If basic safety functions required by e.g. LVD and RED are fully secured outside AI system and AI systems provides ONLY additional, non-essential safety function, then the product should not be considered high-risk. Detailed proposal for amendments in the Attachment 
2.	Article 41 - Common specifications - All actors involved in the standardisation process should have very clear overview in what situations the “implementing acts path” can be initiated. It cannot be arbitrary or left to subjective assessment of one party in the system. Detailed proposal for amendments in the Attachment 
",PUBLISHED,small,Michał,COM(2021)206,2021-07-27 12:40:05,withinfo,24212003,closed,04201463642-88,,
fr,2663064,FRA,trade_union,Numeum,,"Numeum soutient la Commission européenne dans son ambition de stimuler le développement et l'adoption de l'IA et des nouvelles technologies, tout en veillant à ce que les risques potentiels soient traités de manière adéquate. La volonté de la Commission de créer un système européen à même de garantir la confiance des citoyens et stimuler l’adoption des usages IA, tout en assurant celle des entreprises dans le déploiement de leurs produits et applications IA et la capacité d’innovation en Europe, nous apparaît une stratégie ambitieuse et adaptée aux enjeux de développement du potentiel de l’IA en Europe. 

Compte tenu de la diversité des applications et des technologies de l'IA, nous saluons le fait que la Commission adopte une approche ciblée et fondée sur les risques. Une telle approche devrait être basée sur des définitions claires et prendre en compte le risque posé par le déploiement d'un système d'IA, le domaine d'application, le type de déploiement et la nature des risques. 

Vous trouverez ci-joint le détail de notre position.",PUBLISHED,small,,COM(2021)206,2021-07-26 13:53:18,anonymous,24212003,closed,565078326484-60,,
en,2663061,NLD,ngo,European Center for Not-For-Profit Law Stichting,Skoric,Please find ECNL submission in attachment. ,PUBLISHED,small,Vanja,COM(2021)206,2021-07-26 13:20:07,withinfo,24212003,closed,,,
en,2662944,BEL,company,BlackBerry,GOTEV,Please find attached our response to the consultation on the draft AI Act. We are thankful to the Commission for giving us the opportunity to contribute.,PUBLISHED,large,Goran,COM(2021)206,2021-07-23 17:26:44,withinfo,24212003,closed,251259032370-30,,
en,2662941,DEU,company,SIEMENS AG,,"Siemens AG recognizes the European Commission’s effort to propose world’s first AI regulation, building on its last year’s White Paper and aiming at ensuring that AI systems, introduced on the EU market are safe and respect EU laws and values while at the same time creating legal certainty to facilitate investment and innovation in AI. 
On the other side we question whether such a horizontal regulation at this point in time, would not hinder innovation due to overregulation. Innovation using AI may consequently be realized and employed outside of the EU, hence this technology is more likely to advance faster in markets outside of Europe. European SMEs or Startups would also be discouraged from developing AI applications in the EU when the regulatory requirements and obligations are so burdensome. 
Since today the development of AI applications is still in an early phase, it is certainly difficult to phrase an appropriate legal framework for all kinds of AI applications, especially in the B2B area.
In general, we advise to carefully analyze all potentially unwanted consequences, side-effects, and administrative burden for industry, that specific proposed regulation articles might cause as this could lead to hindering innovation in this fast-evolving technology area and thus putting European industry in a competitive disadvantage compared to our global competitors.   
In our attached, detailed position paper, we provide our company's initial feedback for the European Commission’s consultation on this AI regulation proposal, while underlining that we will further contribute- also via sector specific feedback and positions- to the upcoming policy discussions during the legislative process in the coming months ahead of us.",PUBLISHED,large,,COM(2021)206,2021-07-23 17:05:19,anonymous,24212003,closed, 426679777031,,
de,2662935,DEU,company,EnBW Energie Baden-Württemberg AG,Semme,"Aus Sicht der EnBW Energie Baden-Württemberg AG wurde mit dem Gesetzesvorschlag der EU-Kommission zu Künstlicher Intelligenz (KI), eine Regulierung entworfen, welche Chancen eröffnet.

Nach erster Betrachtung, hiermit unsere erste kurze Rückmeldung aus energiewirtschaftlicher Perspektive sowie dem Betrieb von kritischer Infrastruktur.

Auf der letzten Seite des angehängten Dokuments findet sich eine Übersicht mit KI Use-Cases in Einordnung auf einem Risikospektrum.",PUBLISHED,large,Josua,COM(2021)206,2021-07-23 15:53:45,withinfo,24212003,closed,13324391892-74,,
en,2662931,BEL,business_association,American Chamber of Commerce to the European Union (AmCham EU),Madala,"AmCham EU commends the European Commission for the extensive consultative process that led to the publication of the Proposal for an ‘AI Act’ . We appreciate having had the opportunity to provide our perspective on this important piece of legislation. We provided input to the AI White Paper consultation and the Roadmap consultation in 2020 and are pleased that some of the points and recommendations we made are reflected in the AI Act. 
The AI Act is the first attempt to lay down a comprehensive legislative framework for the development and use of artificial intelligence and the proposal will no doubt to some extent serve as an anchor for policy proposals in other countries and regions. We support the Commission’s stated objectives of creating an ecosystem of trust and an ecosystem of excellence and to ensure that the EU becomes a vibrant hub for research, development and innovation in trustworthy AI applications. 
We offer the following initial observations on the AI Act proposal in order to help strike the right balance between the Commission’s dual objectives. We aim to highlight areas in which we see potential for clarifying and improving the proposal.
",PUBLISHED,small,Maira,COM(2021)206,2021-07-23 15:02:28,withinfo,24212003,closed,5265780509-97,,
en,2662925,BEL,business_association,European Tech Alliance,Rempala,"The European Tech Alliance welcomes the European Commission’s proposed AI Act as a good balance between supporting much needed AI innovation in Europe and having safeguards in place to ensure high safety standards and public trust in AI. We welcome the EU’s leadership in creating an AI regulatory framework while supporting Europe’s global competitiveness in that field. For EU businesses, many of whom operate cross-border, it is crucial to have a single set of EU rules on AI rather than a fragmented country-by-country approach.

With this in mind, we set out some key points attached that we believe are important for Council and Parliament to take onboard during discussions on the AI Regulation.",PUBLISHED,micro,Jan,COM(2021)206,2021-07-23 12:54:14,withinfo,24212003,closed,189607519323-76,,
en,2662904,FRA,public_authority,Commission nationale consultative des droits de l'homme,Mirallié,"The French Commission for Human Rights (CNCDH) welcomes the European Commission's proposal for a regulation establishing a regulatory framework for artificial intelligence. 
However, it regrets that fundamental rights are not sufficiently taken into account in this draft regulation, and would like to make a few observations on this issue.
",PUBLISHED,small,Charles,COM(2021)206,2021-07-23 10:59:17,withinfo,24212003,closed,0,national,authority
en,2662901,BEL,business_association,European Association of Co-operative Banks,DELL'ORO,"The European Association of Co-operative Banks (EACB) recognises that the AI proposal is the Commission’s first ever legal framework on the matter, which addresses the risks of AI and aims to position Europe to play a leading role globally. It should be recognised that this is a risky bet. If European values were not ultimately adopted on an international scale, European companies would be at a disadvantage compared to non-European players active in less restrictive regulatory environments.

We believe that the European Commission, the European Parliament and the Council should remain vigilant to ensure that European players are not unduly constrained in their prospect of developing innovative AI solutions compared to international competitors.

We would like to highlight the following points:

• The EACB welcomes the Commission’s risk-based approach as basis for a proportionate legal text.

• We appreciate the technology-neutral and future-proof definition of AI, recognising that AI is a “fast evolving family of technologies” that is constantly developing. Nevertheless, combining the definition of artificial intelligence system together with the techniques and approaches of Annex I of the proposal, we observe that the scope of the Regulation is becoming quite wide as it also includes rule-based approaches.

• We believe it is of paramount importance to make sure that the AI proposal will not add new and burdensome requirements for the banking sector and create conflicts and overlaps with existing rules: e.g., sector-specific regulation (CRD, CRR).

• Generally, some provisions of the Regulation contain somewhat vague wording, e.g., the definitions provided for “remote biometric identification system” and “user”. Moreover, we believe that the definition of ‘developer’ and ‘end user’ are missing from the legal text. These points should be further clarified in order to guarantee legal certainty for providers, developers and users of AI systems.

• Regulatory sandboxes are useful for the development of AI. However, their objectives and entry criteria should be clear and made public in order to ensure a high degree of transparency and a level playing field in the entry process.

• We wonder how AI systems can be prevented from being biased. This requirement is not realistic as it cannot be guaranteed that datasets will be fully correct or complete. We believe that errors should be minimised and that the training, validation and testing of the AI system should be as complete as possible. To ensure a fair treatment an ex-post revision should be made possible.

Attached the EACB developed position paper.
",PUBLISHED,small,Chiara,COM(2021)206,2021-07-23 09:15:43,withinfo,24212003,closed,4172526951-19,,
en,2662846,FRA,company,Sanofi,Fandel,"Sanofi supports the Commission's aim to ensure that Europeans  benefit from technologies developed and functioning according to EU values, fundamental rights and principles, and welcomes the idea that AI should be a force for good in society.

We use AI to accelerate discovery and development across R&D programs and improve the efficiency of processes across the organization. We ambition to use AI across our products lifecycle and would like to bring the following points to the EC’s attention:

1.	In the interest of a harmonized application of the AI Regulation, we recommend a centralized authorization process. If a federated approach is taken, a system of mutual recognition should be established to allow manufacturers to go to the authority of their choice or escalate to a central body to arbitrate a case, thereby introducing an instrument to align the notified bodies.

2.	The proposed obligation to monitor and report incidents and breaches (article 17, 33 and 62) can duplicate existing surveillance and reporting mechanisms in the healthcare space (e.g. pharmacovigilance and device complaints). Where AI is part of therapeutics and medical devices (MD), these existing channels should be leveraged. More specifically, the CE marking system for high-risk applications in healthcare is already addressed by the MD and IVD Regulations. Both differentiate multiple risk classes depending on patient risk and degree of intervention. It would be beneficial to further differentiate the risk categories also for AI, i.e. qualify only the higher device risk classes as high-risk AI (Class III, possible Class II).

3.	The AI definition proposed in Article 3 (1) is too broad and goes beyond currently used techniques of practical relevance. In combination with Annex I and III, the definition could be interpreted as to cover any evidence produced by pharmaceutical companies using statistical methods. 

As many other companies, Sanofi uses statistical and other methods (i.e. listed in Annex I) to develop scientific evidence. When we disseminate such evidence, we influence the environment. The proposed Regulation does not differentiate situations where the environment interacts with the software directly or whether humans distribute the evidence produced. As such, it may imply as a requirement for high-risk AI to monitor the continued accuracy of our evidence against real world data and mitigate/update for any bias observed. This would lead to unmanageable efforts and complexity for a process and controlled experiment which has been perfectly accepted and controlled without applying any new AI technology to that evidence generation process (e.g. health economics and outcomes research, efficacy studies).

We would recommend limiting the scope of the Regulation to the practically relevant Machine Learning approaches (including systems which adapt their output based on data they have learned) and adapt/expand this definition as and when new techniques become practically relevant. The Regulation provides for such an incrementally adaptive mechanism through the Delegated Acts. At a minimum, traditional evidence generation (as used in clinical studies and beyond) must not be classified as AI.

4.	Article 10 (3) includes an obligation to use data which is free of error. We should acknowledge that no data is free of error (particularly if they reflect an assessment).  Training data are usually tagged with meta data which can originate from subject matter experts. Their interpretations cannot be completely error-free and can always be enriched with additional data or meta data. Although data sets can never be complete, free of error or fully representative, their amount and quality should be suitable to meet or exceed the minimal accuracy threshold that the AI system must deliver for the intended purpose. The Regulation should therefore allow for a relative or judgmental criterion of materiality when requiring correctness (e.g. free of “substantive” error )
",PUBLISHED,large,Marie Helene,COM(2021)206,2021-07-22 18:02:37,withinfo,24212003,closed,61291462764-77 ,,
en,2662813,BEL,business_association,The Danish Chamber of Commerce,Birk Mortensen,"The proposal uses a vague definition of artificial intelligence (AI) and it should be clearer how AI is used in different respects. In addition, the proposal relies to a great extent on the use of delegat-ed acts, which increases the uncertainty for the companies affected by the legislation, e.g. the def-inition of what AI is can be changed.

In addition, the scope of the proposal is also too vague - a large part of the problems that the regu-lation is meant to resolve are already (or should be) covered under other regulation. Thus, better enforcement of existing rules is a solution to address some of the risks that the Commission seeks to address in the proposal. This also means that there are some actions where the legality should not be determined by whether it is a machine or a human that is behind. 

Some of the problems the proposal seeks to address are already clearly illegal. The EU Commis-sion has used a few examples in presenting the proposal. As an example of AI that exploits vulner-able groups (Title II, art. 5b), a puppet is used that, with an integrated voice assistant, encourages children to perform dangerous actions. Whether this voice assistant has an AI component or is simply an ""unintelligent"" recording, which at regular intervals plays a call to embark on actions with a high risk of harming themselves or others, we have a clear presumption , that this is al-ready covered by the Product Safety Directive and / or special rules for toys - otherwise it should be in any case. The examples emphasize the point that the Commission is either in the process of regulating areas already covered by other legislation, or that the Commission itself is a little un-clear on, what the purpose of the regulation is. Therefore, the Danish Chamber of Commerce be-lieves that it is beneficial to work on creating greater clarity about the regulation's intention and purpose. 

Specific remarks
Preamble (44)
It is noted that data sets for training, validation and testing of AI systems are required to be ""er-ror-free"", which seems unlikely and impossible. The Danish Chamber of Commerce supports the intention of high data quality, which is free of bias and is representative, but with the very big data sets required to train algorithms, it will be enormously time-consuming and resource-intensive to ensure that there is not even a single error in the dataset.

Article 53 
The Danish Chamber of Commerce welcomes the potential within this article, and the idea of us-ing regulatory sandboxes to provide better conditions for innovation. However, we believe that this should be followed by co-financing from the EU budget and other incentives to ensure that enough sandboxes are set up across Member States.

Article 55
It’s positive, that the Commission is aiming to support the SME’s and their use and work within AI. Paragraph 2 should set a limit (e.g. measured by number of employees or turnover), so that the smallest companies and entrepreneurs are completely exempt from paying fees and the like in connection with the regulation's requirements for conformity assessments, etc.",PUBLISHED,large,Anne,COM(2021)206,2021-07-22 15:27:18,withinfo,24212003,closed,0330934426-12,,
en,2662802,FRA,business_association,FIM,TURCANU,"One has to acknowledge that the implementation of software techniques and approaches commonly referred to as ""Artificial Intelligence"" is not a novelty, as self-driving machinery is already placed on the market and put into service especially in the industrial and agricultural sectors.
These advanced techniques also contribute to improve the reliability of components, to implement predictive mon-itoring and maintenance, to increase the lifespan of machinery, to optimise energy efficiency and to adapt production to customer demand.
The deployment of these techniques represents a cornerstone for the competitiveness of the European Union, and it is imperative to foster innovation in this sector.

In the context of the Artificial intelligence draft Regulation, FIM supports the risk-based approach.
Nevertheless, it appears that this text has been developed without considering that many regulated products incor-porating AI as a safety function, notably machinery subject to the provisions of Directive 2006/42/EC, are currently used in several sectors. Currently, the AI is considered in the framework of risk analysis, thus allowing manufacturers to take appropriate protection measures. Therefore, the articulation between this draft Regulation and the revision of Directive 2006/42/EC raises questions.
Furthermore, the IA draft Regulation establishes obligations, particularly in terms of risk management and conform-ity assessment, which are already covered by the Machinery Directive, and which go well beyond what the Machinery Directive requires, notably concerning Chapters II and III of Title III. Today, the methodology adopted, for example by the Machinery Directive, is to carry out a risk analysis, to determine the applicable Health and Safety Requirements and to take the appropriate safety measures. In this draft Regulation, the provisions of Chapters II and III must be applied in an absolute manner.
While this draft text seems to address legitimate concerns about the use of AI software, prohibiting certain uses, it is not at all adapted to the use in industrial products, especially if the industrial products are already regulated by a New Approach Regulation.
This act will have a disproportionate impact on the companies’ competitiveness and will limit the European Union's capacity for innovation in this area. Please find attached our comments. ",PUBLISHED,small,Roxana,COM(2021)206,2021-07-22 14:21:42,withinfo,24212003,closed,428581813783-89,,
en,2662780,CZE,trade_union,ČMOS PŠ,,"ČMOS PŠ  welcomes the publication of regulation regarding AI as it sets the ground for the first comprehensive EU regulation on Artificial Intelligence to ensure a controlled development of AI tools in education to address the risks connected to their use by teachers, academic, other education personnel and students. While ČMOS PŠ recognises the potential of digital technologies and Artificial Intelligence tools to bring about improvements in education, it also underlines the numerous ethical concerns related to their trustworthiness, data privacy, accountability, transparency and their impact on equality and inclusion in education. ČMOS PŠ underlines that further research at national and European level is needed to assess and address the risks connected to the use of Artificial Intelligence in education with constant and meaningful consultation with educational social partners.

ČMOS PŠ shares the ETUCE opinion regarding the regulation of Atificial Intelligence which is expressed in the attached documents: ETUCE position on the Regulation on Artificial intelligence (June 2021), ETUCE Resolution - Artificial intelligence in the Education Sector (July 2021)",PUBLISHED,small,,COM(2021)206,2021-07-22 11:20:13,anonymous,24212003,closed,493522339410-45,,
en,2662771,DEU,company,SICK AG,Schaber,"The proposed regulation is a welcome initiative but needs significant improvement in some details to be applicable in practice to the cases for which it is intended.

SICK AG urgently recommends the deletion of the ""Bayesian estimation"" from the ANNEX I (Paragraph C).  
Bayesian techniques are not artificial intelligence, but well-proven mathematical formulas. These methods are well established in several applications in the machinery sector. For these mathematical methods the proposed requirements are unreasonable and will lead to excessive costs and to disadvantages for the European mechanical engineering industry. 
Bayesian techniques are deterministic and do not require any training (e.g. machine learning), unlike the AI techniques from Annex I (a) and (b). 
Therefore the requirements for transparency (""sufficiently transparent"") as well as for the behavior of the output as stated in Article 13 (paragraph 1) are not applicable for Bayesian methods.

Article 10 Data and data governance/ Paragraph 3: ""free of errors and complete.
SICK AG urgently recommends the deletion of the point ""free of errors and complete"" from the Article 10. Absolute freedom from errors and completeness of the data obtained is not possible. The risk management measures from Article 9 (4) already take into account the residual risk sufficiently and thus also the occurrence of errors and natural statistical coverage (completeness).",PUBLISHED,large,Volker,COM(2021)206,2021-07-22 10:00:34,withinfo,24212003,closed,27481463425-10,,
en,2662770,FRA,business_association,MEDEF,,"In a context of sovereignty and international competitiveness where the development of technological and digital infrastructures has become essential, it is vital to build a European ecosystem in favour of innovation and artificial intelligence (AI) that is respectful of European values.
To build a trustworthy, efficient and sustainable AI framework, regulation must benefit citizens, but also European businesses (including the smaller ones) by allowing them to develop and benefit from cutting-edge technologies in order to remain competitive. 
A risk-based approach is essential to frame AI and maintain a margin of innovation and of competitiveness for European businesses. 
For this, MEDEF considers it is essential to have: 
−	clear and precise definitions, especially for AI and high-risk systems, for more legal certainty;
−	a consistent regulation compatible with other European texts (GDPR, Machinery Directive, etc.);
−	obligations proportionate and adapted to innovation. 

1) At this point, the definitions of AI and high-risk AI systems are very broad in that they can include all types of systems or software applications that do not involve the same risks. The inclusion of such systems or applications within the scope of the regulation would risk hampering innovation in technology companies, especially smaller ones. However, in a context of international competitiveness, it is essential to encourage technological development and not to prevent SMEs from accessing these markets.

The European Commission has the possibility to adopt delegated acts to modify the list of techniques and approaches defined in Annex I (Art 4), as well as to develop Annex III establishing a list of IA systems considered to be highly risk (Art 7). While it is necessary to take account of technological developments, this flexibility creates legal uncertainty and a degree of market predictability should be supported so as not to discourage companies from developing innovative and competitive AI solutions.

2) It is important to ensure consistency with other European texts, in particular the GDPR, the Data Act or the Machinery Directive. At this stage, certain provisions of the proposed regulation appear incompatible, even contradictory, with pre-existing obligations. For example, the systematic categorization of certain AI systems considered to be high risk does not take into account the ""accountability"" methodologies adopted by the Machinery Directive or the GDPR, which consist in carrying out a risk assessment in order to take appropriate protection measures.
Other provisions are superimposed on obligations that already exist in other texts, which is the case, for example, for personal data which comes under a complete and binding regulation (profiling, human control, explainability of algorithms, updating data, etc.), with very high penalties. A consistency with the GDPR is therefore essential.

3) Some obligations are very restrictive, even disproportionate and impracticable for a company. This is particularly the case with the obligation to use free-of-error datasets. While there is a need to strengthen data learning, it is not possible to guarantee that there are no errors in the datasets. However, the regulation provides for stronger penalties in the event of an error.",PUBLISHED,medium,,COM(2021)206,2021-07-22 09:54:35,anonymous,24212003,closed,ID#43763731235-75,,
en,2662654,BEL,ngo,Pharmaceutical Group of the European Union (PGEU),Grasso,"The Pharmaceutical Group of the European Union (PGEU), the organization representing community pharmacists in 32 European countries, welcomes the European Commission’s Proposal for an EU Regulation on Artificial Intelligence (AI) to establish a legal framework on AI Systems.

Please find the full PGEU feedback to the European Commission's Proposal attached hereto.

",PUBLISHED,micro,Antonio,COM(2021)206,2021-07-20 11:01:37,withinfo,24212003,closed,00086317186-42,,
en,2662644,USA,consumer_organisation,Consumer Reports,Sampath,"Consumer Reports applauds the European Union for putting forth a legal framework that can regulate algorithms and artificial intelligence in a systematic and comprehensive manner. While many of these algorithms and their applications are not new, they have real potential to harm the fundamental rights of EU citizens. The provisions in this proposal can help promote transparency with these emerging technologies and mitigate harmful impacts. We hope to outline the strengths of this proposal and identify potential areas of improvement. 

First, the risk-based approach serves as a robust way to categorize different applications and their respective regulations and transparency requirements. We agree that AI-enabled applications like social scoring, real-time biometric identification, and applications that violate fundamental rights and exploit children should be prohibited within the Union. 

However, emotion recognition technologies were specifically mentioned as being low-risk, only requiring the basic transparency measures as outlined by the lower-risk category requirements. We disagree that these types of technologies should be placed in the low-risk category because the ability for AI to recognize emotions universally is not substantiated by science and has the potential to be discriminatory based on characteristics like race or skin color. Furthermore, we urge the EU to outline stricter requirements for or prohibit the use of AI-enabled technology that is not substantiated by science. This includes applications where companies are claiming that the correlation between different phenomena is actually causal without sufficient evidence to back up those claims. Examples include companies performing physiognomy (trying to use a person's facial features or physical characteristics to determine their character or personality) and which has been widely debunked by present day scholars, as well as other kinds of sentiment analysis tools. We suggest that the EU require that companies must show their claims are backed up by science before allowing these products on the market. 

Furthermore, we also advise that the EU create provisions for public interest researchers to perform audits on algorithms, particularly for those classified in the high-risk category; this can be included in the ""post market monitoring"" section of the regulatory proposal. Third-party testing is an important way to ensure accountability, as it will be difficult for the government to thoroughly test and identify issues with the thousands of products on the market. Some algorithms have the potential to discriminate against individuals based on race, skin color, gender, etc., and it is important that researchers are given pathways to test algorithms for these disparate impacts that might be not properly identified and mitigated by the companies that design these algorithms. Furthermore, this framework should also provide ways for researchers and end-users of AI to notify the government and the companies building the AI-enabled technology of identified faults and/or discriminatory impacts. 

As algorithms and AI become more integrated in products, basic services, and our daily lives, it is important that governments around the world quickly adopt regulations that can mitigate the harmful impacts of algorithms while maximizing the benefits. Overall, this framework is a step in the right direction and an important model that other countries should look to when designing AI regulations of their own. Thank you for your time and consideration. ",PUBLISHED,large,Nandita,COM(2021)206,2021-07-19 23:26:52,withinfo,24212003,closed,,,
en,2662611,NLD,other,Vrije Universiteit Brussel,HILDEBRANDT,"In attach please find my feedback on the proposal for an AI Act. This is my feedback as an independent expert in AI and law, not the feedback of my organisation (Vrije Universiteit Brussel). 
Thank you!
Mireille Hildebrandt",PUBLISHED,large,Mireille,COM(2021)206,2021-07-19 11:43:09,withinfo,24212003,closed,,,
en,2662603,DEU,eu_citizen,,Glauner,"An Assessment of the AI Regulation Proposed by the European Commission

In April 2021, the European Commission published a proposed regulation on AI.  It intends to create a uniform legal framework for AI within the European Union (EU). In this chapter, we analyze and assess the proposal. We show that the proposed regulation is actually not needed due to existing regulations. We also argue that the proposal clearly poses the risk of overregulation. As a consequence, this would make the use or development of AI applications in safety-critical application areas, such as in healthcare, almost impossible in the EU. This would also likely further strengthen Chinese and US corporations in their technology leadership. Our assessment is based on the oral evidence we gave in May 2021 to the joint session of the European Union affairs committees of the German federal parliament and the French National Assembly.

To appear in the 2022 Springer book ""The Future Circle of Healthcare: AI, 3D Printing, Longevity, Ethics, and Uncertainty Mitigation"" edited by Sepehr Ehsani, Patrick Glauner, Philipp Plugmann and Florian M. Thieringer.",PUBLISHED,,Patrick,COM(2021)206,2021-07-19 10:50:45,withinfo,24212003,closed,,,
de,2662602,DEU,eu_citizen,,Glauner,Schriftliche Stellungnahme von Prof. Dr. Patrick Glauner für das am 06.05.2021 stattfindende gemeinsame Fachgespräch der Ausschüsse für die Angelegenheiten der Europäischen Union des Deutschen Bundestages und der französischen Assemblée nationale zur Politik der EU im Bereich der Künstlichen Intelligenz (KI) und insbesondere dem Verordnungsvorschlag der Europäischen Kommission zu KI (COM(2021) 206 final).,PUBLISHED,,Patrick,COM(2021)206,2021-07-19 10:28:20,withinfo,24212003,closed,,,
en,2662543,BEL,academic_research_instittution,The Guild of European Research-Intensive Universities,CHICOT,"Founded in 2016, The Guild comprises twenty one of Europe’s most distinguished research-intensive universities in sixteen countries, and is dedicated to enhancing the voice of academic institutions, their researchers and their students. The Guild is committed to the pursuit of excellence, the importance of truth-seeking and trust-building as the foundation of public life, and the creation of new knowledge for the benefit of society, culture, and economic growth.

The feedback of The Guild, informed by its Heads of AI/Digital Research, is available in the attached document.

The Guild recommends:
- The establishment of a high-level expert group, composed of academic researchers among others, whose tasks will include advising on whether any technological progress requires a revision to the annex or to the body text of the AI Act.
- Rules which ban practices, such as social scoring, instead of banning technologies that could be used for these practices.
- No additional burden on researchers to demonstrate that the AI systems that they develop, deploy or use through their research do not infringe the AI Act.
- Mechanisms to ensure that transposition and implementation of the AI Act are harmonised across the Member States.
- Support to research projects aimed at elucidating the concepts introduced in the AI Act (e.g. trustworthy AI, robust AI) and finding how AI developers can concretely comply with the AI Act.
",PUBLISHED,micro,Julien,COM(2021)206,2021-07-16 15:11:40,withinfo,24212003,closed,265942623347-90,,
en,2662492,USA,company,Google,GIEPMANS-STEPIEN,"Attached please find Google’s views on the European Commission’s proposal on the Artificial Intelligence Act. We thank the Commission for the opportunity to share feedback, and look forward to continue engaging in the debate on the proportionate, risk-based AI regulation in Europe.",PUBLISHED,large,Sylwia,COM(2021)206,2021-07-15 21:10:47,withinfo,24212003,closed,03181945560-59,,
de,2662473,DEU,other,Deutsche Sozialversicherung Arbeitsgemeinschaft e.V. ,schulz-weidner,see attached document,PUBLISHED,micro,wolfgang,COM(2021)206,2021-07-15 14:59:45,withinfo,24212003,closed,917.393.784-31,,
en,2662463,BEL,business_association,European Association of Communications Agencies (EACA),Elzer,"The European Association of Communications Agencies (EACA) represents more than 2,500 communications agencies and agency associations from nearly 30 European countries that directly employ more than 120,000 people. EACA members include advertising, media, digital, branding and PR agencies. 

We support the objectives of the Proposal for an AI Regulation, namely to proportionately address the risks associated with certain uses of AI and to ensure legal certainty to facilitate investment and innovation in the sector. However, we believe that some of the concepts used in the Proposal for an AI Regulation remain rather vague and would merit further clarification in order for providers to better understand what is expected from them and to ensure legal certainty. 

We understand that certain requirements of Proposal for an AI Regulation apply to providers of high-risk AI systems and that they will need to subject their AI systems to conformity assessments that demonstrate that the system complies with certain requirements. We believe that these requirements need to be better defined. 

You can find more details in the attached comment.  ",PUBLISHED,micro,Nina,COM(2021)206,2021-07-15 13:13:20,withinfo,24212003,closed,397482431021-09.,,
en,2662411,DEU,academic_research_instittution,International Center for Ethics in the Sciences and Humanities (IZEW),Heesen,"The International Center for Ethics in the Sciences and Humanities (IZEW) of the University of Tuebingen (Germany) welcomes the EU proposal on the regulation of AI. To achieve a comprehensive and effective result, we seek to contribute to the consultation process with the following suggestions:
1. To classify the risk of AI systems, the proposed Artificial Intelligence Act takes a risk-based approach, distinguishing between four classes of risk (see 5.2.2).
The creation of strict classes is difficult because their sound operationalization cannot be sufficiently ensured. That is because damages related to AI systems are currently insufficiently predictable and quantifiable. This issue concerns the need to distinguish damages to a society as a whole from individual damages as well as the assessment of material and immaterial damages. Especially damages to personal legal rights, such as freedom from discrimination, are difficult to measure. Currently, different fairness measures are used to test AI systems to ensure the protection of justice, equality of opportunity and freedom from discrimination. The different measures, however, correspond with different fairness concepts, which are not all equally and independently of the respective application context suitable to achieve the goal of justice or freedom from discrimination. 
In contrast to strict classes of risk classification, we therefore propose a gradual distinction of the criticality of AI systems, in which additional assessment indicators are added (Art. 7).
These indicators concern the scope of the individual's freedoms of action and question:
•	Possibilities for users to influence the design and the result of the system
•	Possibilities of the users to pause or reconfigure the system flow (Opt-out/configuration)
•	Choice between different services due to the plurality of services offered and an open market structure
In addition, the following criteria should be included to assess the criticality of an AI system:
•	Persistence of the situation where AI systems pose a threat (e. g. the permanent use of AI in public education) 
•	Controllability of the critical situation (degree of interconnectedness of the AI system and a possible domino effect)
2. We further suggest the introduction of an AI registry for all AI systems to ensure sufficient precautionary measures for safe AI use. All providers of AI systems should register their AI applications in a central AI registry. This will help users to obtain transparent information about the use of AI in various products. The central AI register serves to maintain self-determination and is essential for an ethically reflected technology development and application process. 
3. We propose supplementing the monitoring mechanisms with low-threshold, timely and effective complaint options to strengthen the (data) sovereignty of citizens and consumer rights. 
4. The proposal mainly addresses developers, intermediaries, and public agencies (Art. 3). In doing so, the proposal is primarily a B2B or B2G regulation attempt. We propose to take greater account to affected users by giving them AI participation, transparency, and redress rights in excess to standard civil rights and consumer protection law. 
5. Particular attention should be paid to strengthening the participation of employees in the introduction and monitoring of AI systems at workplace.
6. Environmental limits: In addition to software, AI systems require hardware and Internet infrastructure. This is accompanied by a high level of energy and material resources. Therefore, when developing, introducing, or expanding AI systems, it should be evaluated whether the expected benefits justify the negative effects on the environment.
7. In high-risk innovation projects it should become standard practice to ensure that sufficient (flexible) funds for integrated ethical research and reflection are available throughout the development cycle. 
",PUBLISHED,medium,Jessica,COM(2021)206,2021-07-14 20:23:57,withinfo,24212003,closed,,,
nl,2662381,NLD,public_authority,Radiocommunications Agency/ Agentschap Telecom NL,Vrieze,"Feedback Agentschap Telecom Nederland - see attached file

",PUBLISHED,large,Machteld,COM(2021)206,2021-07-14 17:02:08,withinfo,24212003,closed,,national,authority
en,2662292,DEU,ngo,Civil Liberties Union for Europe,Reich,"The Civil Liberties Union for Europe (www.liberties.eu) welcomes the opportunity to provide feedback to the AI Regulation proposal. We recognize that Artificial Intelligence (AI) can contribute positively to our societies. But AI systems can also undermine our fundamental rights, by perpetuating bias in criminal justice, manipulating public opinion and enabling mass surveillance practices. Liberties’ aim is to make sure that the Regulation protects the fundamental rights of EU citizens. 

In general, the proposal marks an improvement to last year’s White Paper. Recognizing that AI should be “a force for good in society with the ultimate goal of increasing human well-being” is the right approach. Article 5, which bans AI systems considered “unacceptable as contravening Union values”, and Article 60, which suggests the creation of a database for high-risk systems, are much welcome initiatives. However, the proposal contains certain loopholes that can lead to severe breaches of fundamental rights. Six issues are of special concern.  

First, the proposal’s prohibition of remote biometric recognition technologies contains too many exceptions. The vague wording leaves room for discretion: Law enforcement authorities can justify the use of facial recognition technologies and circumvent the obligatory authorization by a judicial authority in “a duly justified situation of urgency”. Further, the ban only concerns ‘real-time’ remote biometric identification systems, allowing law enforcement authorities to first collect the data and then mine it, using mass surveillance technologies. In addition, the ban only concerns law enforcement authorities. Other public authorities and private actors are exempt, although the risks associated with these systems remain. Similarly, the ban on social scoring systems does not apply to private actors. 

Second, the proposal fails to recognize the risks of predictive policing, although multiple studies have shown that they are, by default, discriminatory towards marginalized communities. Furthermore, there is little evidence of the effectiveness of such systems and a lack of transparency as to where they are used and by whom.  

Third, the proposal severely underregulates biometric categorization, emotion recognition and systems that manipulate image, audio or video content, such as deep fakes. Biometric categorization systems discriminate against non-binary people and minority groups and can be dangerous for people belonging to the LGBTQI+ community or political dissidents. Systems that generate or manipulate content are used to humiliate people, spread disinformation and interfere in democratic processes. Emotion recognition technologies are unreliable and prone to bias. It is extremely disconcerting to see how the proposal underestimates the potential impact they can have on a person’s life and access to opportunities.

Fourth, businesses with high stakes in seeing their products make it to the market are allowed to self-regulate. This is an inappropriate solution because businesses have no incentive or inclination to proceed with caution and respect for fundamental rights, but are rather keen to downplay the risks and release their products as soon as possible. 

Fifth, it is not clear how the Commission wants to ensure enforcement. The proposal to create an AI board is sensible. However, it is not given sufficient autonomy to act independently from the Commission and not given sufficient resources to act effectively. The proposal further suggests that each Member State create their own national competent authority, which may come into conflict with existing authorities.

Sixth, the proposal fails to address power imbalances between providers of AI systems and consumers. It lacks clarity around citizens’ rights to lodge complaints and access to a remedy for persons adversely affected by AI systems. 

Find further information and recommendations in the attached paper. 
",PUBLISHED,small,Orsolya,COM(2021)206,2021-07-14 09:53:18,withinfo,24212003,closed,544892227334-39,,
en,2662226,BEL,consumer_organisation,ANEC,GIOVANNINI,Please find attached ANEC's comments.,PUBLISHED,micro,Chiara,COM(2021)206,2021-07-13 16:24:11,withinfo,24212003,closed,507800799-30,,
en,2662219,BEL,other,EuroGeographics,Bodossian,"EuroGeographics is an independent international not-for-profit organisation representing Europe’s National Mapping, Cadastral and Land Registration Authorities (NMCAs). We are a passionate advocate for European geospatial data from official trusted sources, in particular when it is harmonised to standard specifications. We are providers of a range of products and services and expertise which supports navigation, automated vehicles management, emergency response, a reliable and secure land and property market, fiscal and many more government and business decisions and services.

NMCAs developments of AI systems is still very much at proof-of-concept stage. However, our sector has proved its ability to embrace new technologies and we reckon that what is new now will certainly go mainstream soon.

At this stage, EuroGeograpphics members are both, AI users and providers of data that are very widely feeding AI services and business. AI is used for processing geospatial information, in particular for increasing the efficiency of their quality assurance and management – the calling card of NMCA data. For example, some of our members have been combining high-resolution elevation analysis with deep learning techniques, to provide policy-makers with vital information for the transition to solar energy, exploring how to use AI for a wide range of activities including land cover mapping and the creation of 3D data is high on the agenda.

Furthermore, our members have fully embraced the European strategy for data, and they already contribute substantially to make it a success story. The crucial contribution is primarily by implementing the Open data and PSI directive, which will establish trusted mechanisms and services for the re-use, sharing and pooling of data that are essential for the development of data-driven AI models of high quality.  

The Regulation proposal confirms that some areas of AI deployment will fall under existing legislation and will be fully coherent with the Commission’s overall digital strategy. This is a strong stir for our members, since the stable governance principles and data protection established thereof, encourages our members to create and test high–risk AI systems, for example in the transport field. This possibility will provide many opportunities for NMCAs to contribute to the digital single market. The development of programmes, research, projects, or support, that will allow the growing of the diffusion of AI within our member organizations, will be highly welcomed.

We are aware that this Regulation applies to providers placing on the market or putting into service AI systems and we are not there yet. Nevertheless, our members' authoritative data, high quality datasets, are back bones to many of the critical infrastructures as defined in the proposed Regulation. In recent years, we have seen an explosion in location-based AI systems. Some of our members are already in the testing phase of geospatial data-based AI applications, and this regulatory framework is the supportive direction for that, which is considered a good start.

The Regulation proposal has positively raised awareness of AI and is intended to balance very sensitive issues such as fundamental rights, public safety, and innovation. It is highly desirable to have definitions in a flexible manner, not to hamper innovations. Legal certainty, risk assessment levels, regulated use, and concept of trust by increasing the trust for EU made AI products are the main benefits of the proposal from Eurogeographics members’ point of view.",PUBLISHED,micro,Léa,COM(2021)206,2021-07-13 15:54:48,withinfo,24212003,closed,51080067776-74,,
es,2662182,ESP,other,CENTRO ESPAÑOL DE DERECHOS REPROGRÁFICOS EGDPI (CEDRO),MORAN,"
Estas son las contribuciones del CENTRO ESPAÑOL DE DERECHOS REPROGRÁFICOS EGDPI (CEDRO) a la propuesta de Reglamento COM(2021)206 sobre Inteligencia Artificial (Artificial Intelligence Act).

CEDRO es una entidad de gestión de derechos de Propiedad Intelectual establecida en España, autorizada por el Ministerio de Cultura y Deporte, de conformidad con la legislación española, para el desempeño de sus fines. CEDRO lleva desarrollando su actividad desde 1988 y es la única entidad de gestión autorizada en España para representar a escritores, traductores, periodistas y editores de libros, revistas, periódicos y partituras. Puede obtenerse más información sobre la Entidad en https://www.cedro.org/ 

Las consideraciones que se realizan en el documento adjunto se basan en las posibles implicaciones que pueda tener la Inteligencia artificial (IA) en los derechos de propiedad intelectual del colectivo al que CEDRO representa y que, en nuestra consideración, la propuesta de Reglamento debería abordar.
",PUBLISHED,small,MERCEDES,COM(2021)206,2021-07-13 11:57:51,withinfo,24212003,closed,021880022200-45,,
en,2662176,DNK,business_association,Finans Danmark,Engbæk Larsen,"Finance Denmark supports regulation of AI. It is important to provide a regulation that protects the customer, through which trust in AI can be build. Much of what the financial sector does, is fundamentally based on trust. Therefore, it is important that AI-solution now and in the future can be trusted through a sound and appropriate regulatory framework on AI.

The proposal from the European Commission distinguishes between high and low risk. Finance Denmark recommends that a risk-based approach with several lev-els will be used instead. 

We support that the future supervision of the financial sectors use of artificial in-telligence is placed with the regulatory authorities that currently has the supervi-sory oversight of the financial sector

Finance Denmark proposes that it will be possible to apply the same regulatory risk-based approach for artificial intelligence as the approach used in connec-tion to fintech. 
",PUBLISHED,medium,Rasmus,COM(2021)206,2021-07-13 11:15:59,withinfo,24212003,closed,20705158207-35,,
en,2662175,BEL,ngo,Allied for Startups,Tabaczynsky,"AFS Feedback to the European Commission’s regulation proposal on the Artificial Intelligence Act

Artificial Intelligence has the potential to solve some of the world’s biggest challenges, ranging from combating climate change to improving people’s everyday lives. Startups are at the forefront of innovation, pioneering new products and services. Developing an AI regulatory framework that incentivises them to innovate in all fields is therefore of paramount importance. 

Allied for Startups takes stock of the publication of the Artificial Intelligence Act, which will create a common set of EU rules for AI systems but raises questions whether the European Commission’s proposal will do enough to support innovation in high-risk AI sectors. 

Clarity and clarification
Clarity is needed on the definition of high risk use, as innovation in a high risk field is not akin to a high risk use. A clarification of responsibilities and of the roles of AI providers, AI operators and users is also needed. 

Harmonised regulatory sandboxes
Startups are global from day one and as such want to comply with any legislation impacting their business from day one. Allied for Startups supports the introduction of regulatory sandboxes for AI systems in Europe and measures aimed to lower the cost of market entry for startups. However, the introduction of sandboxes should not be the only way for entrepreneurs to have the legal certainty they need to innovate with a high-risk AI application in the EU. Consideration must be given to the time necessary and the resources available to the authorities and the entrepreneurs in this process. Moreover, this framework for sandboxes should be designed and implemented across all Member States in a harmonised manner. 

Achievable high-risk requirements
Looking at the entirety of the requirements, we are worried that these will be overly burdensome for startup entrepreneurs and dissuade them from developing artificial intelligence systems in high-risk areas in Europe. More specifically, we wonder how some of the legislative requirements such as making sure that the data that an AI system is trained is free of errors can be technically feasible. In addition to that, there is uncertainty as to what an appropriate level of accuracy, robustness and cybersecurity of an AI system could be and how this would be assessed. 

Startups-friendly conformity assessment procedures 
Conformity assessment procedures should take into account the startup innovation model. Most of the time, startups do not have the resources to wait several weeks to get their AI technology approved. They should be clear and straightforward to avoid double compliance or forum shopping. The timeframe to get the approval from the notifying authorities should not take longer than 4 weeks, as more could endanger the competitiveness of startups working in high-risk AI areas. 

We ask policymakers to design an Artificial Intelligence Act that is understandable and implementable for startup entrepreneurs. The overall objective of the Artificial Intelligence Act should incentivise startups from all around the world to develop their AI systems - low and high risk - in the European Union. Future-proof AI legislation can give entrepreneurs the legal certainty they need to launch their groundbreaking tool or services in the EU Single Market.

",PUBLISHED,micro,Manon,COM(2021)206,2021-07-13 11:11:44,withinfo,24212003,closed,634665118544-37,,
en,2662109,BEL,other,HOPE - European Hospital and Healthcare Federation,GAREL,"Artificial Intelligence (AI) is a complex phenomenon interfering with the way medical research is conducted, the biomedical data is used, and the healthcare professions and organisations are regulated. AI use in healthcare would then also requires a specific regulatory approach, in addition to a strong horizontal cross-sector regulation of AI.

HOPE key recommendations to ensure that the application of AI in healthcare benefit patients and consumers are:  

- We need to agree at European level on a definition of AI for healthcare as a basis for further discussion from an ethical or legal perspective or to be used to determine requirements for the quality of AI.

- We need to build action on AI on clear citizens’ rights (and not only when they are patients): transparency, explanation and objection; accountability and control; fairness; non-discrimination; safety and security; access to justice; reliability and robustness. 
The well-being and autonomy of the AI user should have priority. AI should support the user but not restrict the user’s autonomy. The effects of AI on social and environmental aspects should also be examined. It is important not only to adopt individual perspectives but also to take society as a whole into consideration. It appears useful to establish clarity as to the conditions under which AI processes can be used, especially in view of the Medical Devices and In-Vitro Diagnostic Regulations (MDR/IVDR).

- We need professionals ready for AI
The EU and the Member States (MS) should conduct regulatory assessments of the medical professions frameworks to determine whether they are fit for the use of patient/consumer-centred AI in health. The EU and MS should put in place mechanisms to ensure professional and educational assistance to both patients and the healthcare professionals to better understand and assess AI decision-making. 

- We need good quality AI
The EU should facilitate the identification and should promote good practices ensuring robustness of AI systems in the health sector both at the stages of development and actual use to reduce potential biases and errors of AI-based decision-making. Mechanisms should be in place to make sure developers of AI systems are competent to do so in healthcare sector.

- We need adapted legislation
Ethics are a fundamental component for healthcare research and professions. However, an AI ethical code is not sufficient. AI needs that the existing regulatory frameworks are adapted and that a comprehensive AI legislation is created. 
The EU should update the EU safety and liability legislation that is relevant to the healthcare sector to ensure that it is still accurate and that citizens are well protected with regards to the use of AI.
The EU and the MS should ensure that the MDR/IVDR are implemented with a view to include artificial intelligence: guidelines and specifications or the evaluation of safety and effectiveness of software, AI and deep learning powered devices throughout the entire usage cycle.
The EU and MS must ensure that AI in healthcare is applied in full respect of EU data protection rules, while observing the balance between the interests of advancements in medical research and citizen protection. This must be achieved through diligent implementation of the GDPR principles and adequate use of provisions and exemptions on health research. 

During the evaluation and review report of the EU data protection legislation, the European Commission should specifically evaluate the need to establish rules on: anonymization techniques of health data; data access and control when it comes to use of data coming from multiply sources; and quality and safety standards for all information systems where health data is processed. 

The EU should establish a legal framework for AI integrating the specifics of healthcare.

Finally, financing such innovation and in particular adapting the hospital financing to its development will have to be considered carefully by MS.",PUBLISHED,micro,Pascal,COM(2021)206,2021-07-12 17:18:49,withinfo,24212003,closed,73872883198-91,,
en,2662075,CHE,business_association,EURALARM,Stockbroeckx,"Euralarm, the European association representing the electronic fire safety and security industry, welcomes the opportunity provided by the European Commission to give comments and suggestions on their proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS.

Euralarm welcomes the proposal from the European Commission. We are convinced that the uptake of artificial intelligence is dependent on the trust of the users and the general public for this technology and their general acceptance of it. The proposal for a legislation on the existing concerns is certainly a way to help this uptake by avoiding national legislative initiatives.

The attached position paper focuses on proposals and comments regarding the following aspects of the proposed AI Act:
-	definition of AI system
-	prohibition of real-time remote biometric identification
-	classification as high-risk
-	common specifications.

Besides these focal points, Euralarm generally supports the more detailed comments from Orgalim.
",PUBLISHED,micro,Benoît,COM(2021)206,2021-07-12 11:35:18,withinfo,24212003,closed,94201247949-87,,
en,2661982,NOR,academic_research_instittution,Norwegian Open AI Lab,,"This feedback comes from the Norwegian Open AI Lab, an AI research centre based at the  Norwegian University of Science and Technology (NTNU) in Trondheim. The Norwegian Open AI Lab is a partnership organization with partners from the Norwegian private and public sector.",PUBLISHED,small,,COM(2021)206,2021-07-09 15:03:31,anonymous,24212003,closed,,,
en,2661971,IRL,trade_union,Teachers' Union of Ireland,Duffy,"The Teachers' union of Ireland (TUI) welcomes the publication of the AI Regulation as it sets the ground for the first comprehensive EU regulation on Artificial Intelligence to ensure a controlled development of AI tools in education and address the risks connected to their use by teachers, academic, other education personnel and students. While the TUI recognises the potential of digital technologies and Artificial Intelligence tools to bring about improvements in education, it also underlines the numerous ethical concerns related to their trustworthiness, data privacy, accountability, transparency and their impact on equality and inclusion in education. The TUI underlines that further research at national and European level is needed to assess and address the risks connected to the use of Artificial Intelligence in education with constant and meaningful consultation with education social partners.

AI in education as a high-risk:
The TUI welcomes that the AI Regulation classifies the use of AI tools in education and vocational training as high-risk underlining that “When improperly designed and used, such systems may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination”. The TUI emphasises that the EU Commission initiative should ensure that the development of Artificial Intelligence in education does not infringe the human right of all individuals to have equal access to quality education. This is enshrined in the first and third principles of the European Pillar of Social Rights and the European Charter of Fundamental Rights.
The TUI supports the European Commission’s proposal to set stricter horizontal legal requirements for the AI tools used in the education sector. The AI Regulation proposal also foresees the establishment of a risk management system to analyse the risks associated with the use of the AI tools in education and monitor them during the entire lifecycle of the AI tools. In this regard, the TUI believes that the European Commission should support the development of clear and binding measures, including ethical guidelines, to address the risks that AI tools pose concerning transparency, accountability, intellectual property rights, data privacy, cyber-safety, equality and environmental protection.

Governance:
The Proposal for Regulation seeks to establish a governance system leading to the establishment of a European Artificial Intelligence Board with the involvement of national authorities to monitor the implementation of the regulation. Nevertheless, the TUI underlines that the effective implementation of the AI legislation in education requires the meaningful involvement of teachers, academics and education staff as co-creators of Artificial Intelligence tools in education. Education trade unions have a crucial role to play to addressing the risks of Artificial Intelligence in education and bring the perspective of AI users on the implementation of the regulation. 

The role of teachers in education:
Teachers, academics and other education personnel play a crucial role in fostering the full human potential of students and their role in education must be preserved. The TUI calls on the European Commission and the Member States to interdict the Artificial Intelligence tools that are designed to replace education personnel or can damage the social value and the quality of education. Besides, the AI Regulation should ensure that the development of AI in education does not reduce the role of teachers to mere providers of instructions but rather serves as a supporting tool for the teaching profession.

EdTech expansion and issues of intellectual property rights, data privacy of teachers:
The TUI points out that the development of the use of Artificial Intelligence in education has been accompanied by the often adverse expansion of Ed-tech companies in their influence in the education sector.",PUBLISHED,small,David,COM(2021)206,2021-07-09 12:48:06,withinfo,24212003,closed,,,
sv,2661969,SWE,trade_union,LO Sweden,larsson,LO har valt att lämna sin synpunkter i bifogad fil.,PUBLISHED,medium,linda,COM(2021)206,2021-07-09 12:44:49,withinfo,24212003,closed,,,
en,2661501,FIN,other,CSC - IT Center for Science,Tuomikorpi,"AI will have a significant impact on Europe’s competitiveness and wellbeing of citizens. It is crucial to support the development of European AI technologies and applications both financially, and by building an interoperable, horizontal ecosystem of European state-of-the-art data and HPC infrastructures. It is of utmost importance to make sure, that the use of AI does not jeopardise the safety or fundamental rights of Europeans. The AI Act proposal is an ambitious attempt towards this end. However, it must be carefully assessed to make sure that its provisions are feasible and that they do not leave too much room for interpretation. Human-centricity must be kept in mind as the key overarching principle for the development of AI. In addition, what is needed and what cannot be written in any regulation, is trust, transparency and collaboration between different stakeholders. 

Data is the raw material for AI. As a phenomenon it is rapidly evolving and hard to anticipate. Thus, there is a risk of overregulation that may hamper innovation. Should AI regulation be made at all, systems falling within its scope cannot be defined by existing technologies. Not only would such definition become soon outdated, it would also create a loophole: if a certain technology is not included in the definition of AI in the Regulation, it does not have to comply with it. This creates problematic situations, where doing some things would be prohibited if they are done using one specific technology but allowed if they are done with another one. Also, if the EC will have the power to update the definition of AI whenever necessary, there is no legal certainty as any change to the definition would also change the scope of the Regulation.

In light of the above, the definition of AI systems must be made generic enough, to be able to include emerging technologies without having to update the Regulation. The aim must be to regulate certain purposes for which technology is used, not the technologies as such. Regulation must also take into account the context in which AI is used and, for example, not limit the use of AI for research and innovation purposes, to avoid creating barriers for a flourishing data economy. 

CSC welcomes the EC’s risk-based approach whereby most of the provisions of the AI Act only concern prohibited or high-risk AI systems. However, such classification requires that the definitions and requirements of the prohibited and high-risk AI systems are formulated clearly and precisely, to avoid leaving too much room for interpretation. Vague formulations, such as ‘psychological harm’ in Art. 5.1(a), open the door for subjective and even arbitrary interpretations of the Regulation.

It is crucial to make sure that high-risk AI systems do not become de facto prohibited ones due to impossibly strict requirements imposed on them. For example, the requirements for the quality of training, validation and testing data in Art. 10.3 must be designed so that they can be met in practice. The aim of avoiding biased data is valid but the requirement of the data to be entirely free of errors is unrealistic and must therefore be re-assessed.

Considering the role of data as the fuel of AI, the AI Act must be closely aligned with the EU’s data regulation making sure, for example, that individuals are informed about the purposes of which their personal data is used. In general, it is essential that the use of AI is transparent. This includes transparency of algorithms which is crucial for the human oversight of AI. It must be noted, however, that effective human oversight requires adequately skilled professionals. Therefore, competence development in all fields and sectors is a key aspect to ensure not only the development of state-of-the-art AI systems in Europe but also their appropriate use.",PUBLISHED,large,Satu,COM(2021)206,2021-07-09 07:00:41,withinfo,24212003,closed,098297335667-27,,
en,2661481,BEL,company,DEKRA e.V. ,Deiters,"As an independent Testing, Inspection and Certification (TIC) company, we welcome the European Commission’s proposal for a Regulation laying down harmonized rules on Artificial Intelligence to ensure the safety and security of European consumers in the area of Artificial Intelligence. 
Third-party conformity assessments conducted by independent TIC companies guarantee not only the impartial and thorough testing, inspection and certification of any product entering the European market, but also of AI systems. The assessments offer the providers’ expertise and ensure that products meet the requirements of the relevant applicable legislation and high consumer protection. Through this, a level playing field is being created on which ground innovation can flourish. 
It is necessary to build a legislative framework suitable for the future and meeting, among others, highest standards in terms of consumer protection. Not including the highest standards as regards scrutiny and quality assurance of new products in the form of third-party assessments, due to the fear that notified bodies might not yet have the capacity to perform the conformity assessments of AI systems, seems counterproductive to the aforementioned goal. TIC companies have already shown their capacity to successfully innovate or adapt their services offering to the evolution of technology, as is currently the case in the area of cybersecurity. 
In particular, with regards to stand-alone AI systems with risk of adverse impact on fundamental rights, which are explicitly listed in Annex III, we do believe that internal control checks by the industry (with the exception of remote biometric identification systems that would be subject to third-party conformity assessment) are not sufficient and might even be impractical for significant parts of the AI innovation ecosystem. 
We are convinced, that there is a need to have qualified and independent resources to conduct conformity assessment on AI systems. This also includes providing the necessary infrastructure, such as tooling. Furthermore specialized experts need to be able to perform the required assessments with high certainty, very mature processes and organizational structures within the assessing organization to assure reliability and replicability of the assessments.
With this proposal, there will be no additional financial burden on small-scale providers, as the European Commission considered this and suggested offering adequate fees for small-scale providers when turning to notified bodies. 
As also highlighted by Commission officials, many manufacturers/providers would turn to notified bodies even if they are not obliged to do so. This is for reputational reasons and due to their lack of expertise to perform conformity assessments of their own.
Finally, as mentioned in the White Paper on AI published by the European Commission last year, “the carrying out of conformity assessments could be entrusted to notified bodies designated by Member States. (..) Independent assessment will increase trust and ensures objectivity. It could also facilitate the work of relevant competent authorities. The EU enjoys excellent testing and assessment centres and should develop its capacity also in the area of AI”
We further support,
• the development of a risk-classification of AI systems
• mandatory conformity assessments according to the relevant sectoral legislation when the AI system is a safety and/or security component of a product
• new ex ante re-assessments of conformity in case of substantial modifications to the AI systems
• that Annex III will need to get constantly updated to take new technology developments into account
• the non-discriminatory access to data for the validation of high-risk systems also for notified bodies through European Common Data Spaces, to be able to perform our duties. We call on the Commission to also grant this access for lower-risk systems, to allow notified bodies to perform their services

",PUBLISHED,large,Oliver,COM(2021)206,2021-07-08 14:05:32,withinfo,24212003,closed,52624015528-97,,
en,2661384,FRA,other,ETSI,DOR,"ETSI welcomes the draft Regulation on AI presented by the European Commission (EC) at https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206. 
The draft Regulation builds on the processes of the New Legislative Framework (NLF) putting harmonised European Standards into the focus of demonstrating compliance with the regulatory requirements. This is in line with the view of ETSI that ""The NLF should be used for technical regulation in new areas including AI and data.""
ETSI has analysed the draft AI Regulation in an activity involving the ETSI Board as well as ETSI's expert group on AI, set up  across all technical committees and groups working on AI, the OCG-AI group. ETSI has many activities in the area of AI, focusing on applications in or using ICT. A concise overview from our working groups is available on the OCG-AI's web site at https://portal.etsi.org//TB-SiteMap/OCG/OCG-AI-Co-ordination.
With this document ETSI provides feedback on the draft Regulation based on the broad spectrum of expert knowledge available in ETSI. This includes in depth knowledge of AI technologies and systems as well as expertise present in ETSI on European standardisation, e.g. from the work on harmonised standards in support of other technical regulation and on the basis of ETSI's ENAP process for the development and adoption of harmonised standards.
ETSI also thanks the EC for the established close dialogue and exchange on the draft AI Regulation. This dialogue is very helpful for ETSI in its role as a European Standardisation Organisation to prepare for the upcoming tasks of developing harmonised European standards on AI. 

(Full text attached)",PUBLISHED,medium,margot,COM(2021)206,2021-07-06 18:10:48,withinfo,24212003,closed,474710916419-15  ,,
en,2661333,SWE,public_authority,Region Västra Götaland,Burhoi,"AI has the potential to contribute significantly in several areas such as increased economic growth as well as solutions to environmental and social challenges. There are examples of AI enabling better diagnoses of diseases, reducing traffic accidents, streamlining industrial production, developing new drugs, and shortening red tape. At the same time, the risks of AI need to be considered. It is therefore positive that the proposal uses a risk-based approach to create a structured division between different types of AI systems and its use. As in the reply to the Public Consultation on the White Paper on AI, Region Västra Götaland highlights that the development and the use of AI need to be guided by norms and ethical principles and be based on the principle of human rights. The overall goal should be sustainable AI, meaning that AI applications should be ethical, secure, reliable, and transparent. Ethical and safety considerations cannot be an afterthought in AI applications but must be an integral part of the early design work.

The use of AI has the potential to create significant benefits in several areas through increased economic growth as well as solutions to environmental and societal challenges. It is therefore important that there is a balance between the consideration of risk on the one hand and development and innovation on the other. Thus, it is central that the proposal highlights the use of so-called regulatory sandboxes. It is an important part in promoting innovation and streamline regulatory compliance for future AI systems. Test and experimental environments, as well as regulatory sandboxes, are central to the development of reliable AI.

For AI to contribute with desired effects, data needs to be made available. It is with the help of large amounts of data that AI can be used, for example, in healthcare diagnoses and contribute to preventive health. A prerequisite is the possibility to share and use data in a secured manner. Furthermore, data needs to be of high quality.

In the proposal, parts of the activities under regional responsibility falls under areas of possible high-risk AI applications. It is therefore important that the right conditions are created to ensure that the public administrations have the right tools to make an adequate risk assessment of often complex AI value chains. It is of great importance that the consequences of the proposal for both public and private activities are analysed in detail. The possible increased administrative burden for regions and municipalities also needs to be analysed.

Furthermore, digital skills and competences are key factors. A prerequisite for sustainable introduction and application of trusted AI systems is the understanding and commitment to AI development. The need for digital skills is growing but there is simultaneously a shortage of digital skills and excellence in many parts of the EU, for example in Sweden. This is a challenge, not least for the development of AI. The proposal for a new AI regulation can be expected to further increase the need for digital competence in various businesses and industries. Resources need to be invested in skills development in both public and private sectors. Investments in lifelong learning, efforts to attract international talent and improved measures to match the supply and demand in the labour market are needed in the EU. Region Västra Götaland therefore welcomes the European Commission's new coordinated plan on AI. It is important to work strategically on measures for skills supply, digital skills and increased investment, and to make use of the already existing structures at local, regional, national and European level. Regions have long experience of supporting and collaborating with actors in different ecosystems and infrastructures at regional level. These already existing forms of cooperation and ecosystems should be considered in the future work in the field of digitalisation and AI.",PUBLISHED,large,Astrid,COM(2021)206,2021-07-05 16:17:54,withinfo,24212003,closed,83211143480-97,regional,authority
en,2660610,DNK,eu_citizen,,Ludvigsen,"The draft AI Act represents a huge step forward in the regulation of digital services and products. As a EU citizen and an academic, I want to do my part in making future legislation in the EU the best it can be, and I want to contribute with the expertise I have.

I here present my feedback for it, see the included PDF. It does not reflect on the opinion of my employer.

Sincerely, Kaspar
",PUBLISHED,,Kaspar Rosager,COM(2021)206,2021-07-02 13:24:05,withinfo,24212003,closed,,,
en,2660588,BEL,business_association,"European Coordination Committee of the Radiological, Electromedical and healthcare IT Industry (COCIR)",Eberstein,"Artificial Intelligence in healthcare is already a reality. Healthcare providers have embedded the technology into their workflows and decision-making processes. The introduction of AI in healthcare has brought improvements for patients, providers, payers, other healthcare stakeholders, and society at large, also in the fight against the COVID-19 pandemic.

It is essential to have a robust regulatory framework that provides certainty to all actors as new technologies are being developed and introduced into the market. Medical Device manufacturers have long-standing experience of operating in a highly regulated sector. COCIR members take the responsibility towards end-users, especially patients, very seriously, to place safe and performing products on the European market.

The attached document contains our detailed feedback to the Commission proposal for an Artificial Intelligence Act. We are looking forward to engaging with the EU institutions in the upcoming legislative process, together with our technical experts.",PUBLISHED,micro,Annika,COM(2021)206,2021-07-02 08:49:23,withinfo,24212003,closed,05366537746-69,,
en,2660543,CHE,business_association,Medtech & Pharma Platform Association (MPP),Secretariat,"The not-for-profit MPP Association welcomes the European Commission proposal for a Regulation laying down rules on AI. MPP supports efforts to optimize the regulatory framework to create the right conditions for healthcare innovation.

We are the opinion that risk categories for AI provide a useful base for the regulation of AI. In this area, we would welcome clarifications on the determination of the high-risk category in the proposed Regulation and how such category correlates to existing legal frameworks, especially with the Medical Devices and In Vitro Diagnostics Regulations (MDR & IVDR). Notably, there is a concern that the existence of different risk assessments under the AI and medical device legislations might create confusion. Under a risk-based regulatory system and in consideration of the scope of use, some AI-driven software may be categorized as low or intermediate risk under the MDR and IVDR, whereas the proposed Regulation may classify such products as high-risk. In this regard, it could be beneficial to consider aligning the risk levels described in the proposed Regulation with risk levels outlined in the MDR and IVDR for consistency. The recently entered into force MDR and upcoming IVDR provide an adequate framework for the regulation of Medical Device Software (MDSW) and it is important to make sure that the proposed Regulation is fully compatible with them.

In addition, MPP would like to share the following comments and questions:
•Recital 60: Does this imply that a pre-trained model cannot be used in an AI high-risk system as a software of unknown (or uncertain) pedigree (or provenance) (SOUP) or off-the-shelf (OTS), for which the collaboration of the supplier is not necessarily needed?
•Article 43 section 4: How does the process for pre-determined changes for MDSW work? Are the rules mentioned in this article supposed to come from the regulators or does the paragraph mean that the manufacturer can specify in the instructions the modification (e.g. in accuracy) that can occur without implying a ""substantial modification""?
•Annex II: Companies may develop internal products in the areas of high-risk AI systems. How does the EU intend to monitor the internal usage of AI systems that do not reach the market?
•Annex IV: Is the pre-specification in this Annex comparable to (SPS) Software as Medical Device (SaMD) pre-Specifications, Algorithm Change Protocol (ACP) from the US Food and Drug Administration (FDA) AI Discussion Paper on the regulatory framework for modification to AI /machine learning (AI/ML) based SaMD? Is there any possibility to harmonize those two approaches with the current proposed Regulation?
•Annex IV, 2(e): More clarification is needed on how human oversight is defined and applicable, examples from medical device software would be appreciated.
•The proposal states that it will consider harmonized technical requirements. Will standards such as IEC 62304, ISO13485, ISO14971 be integrated?
•How does the Commission plan to build the regulatory sandboxes to facilitate products following regulations MDR and IVDR? More specifically, can certain activities, like data collection with the purpose of building a model or tests for clinical evaluation, be performed in the context of these sandboxes? And what would be the regulatory steps to move from a sandbox to the market for such a product?
•How is the expansion of the AI system (e.g. scaling up to different devices, from Android to iDevice) organized in terms of submission?

The proposed penalties in case of infringement may impact the commitment of manufacturers in ensuring deep and intensive monitoring of their own AI systems in the post-market surveillance phase. To promote a shared culture of rigorous monitoring, incentives to the manufacturers that find incidents in their own AI system could be put in place. Such incentives could, for instance, materialize via decreased fines when infringements are found and address by manufacturers themselves.",PUBLISHED,micro,Medtech Pharma Platform,COM(2021)206,2021-06-30 18:43:49,withinfo,24212003,closed,234427831877-10,,
en,2660510,SWE,public_authority,City of Stockholm,,"The City of Stockholm welcomes the regulation laying down harmonised rules on artificial intelligence and amending certain union legislative acts. However, certain concerns arise with the current proposal for the Artificial Intelligence Act:

1.	There is a need to clearly specify which types of legal requirements that are mandatory for the different authorities, organisations and companies (etc) concerned. 
2.	Who is to define whether or not certain users or systems fall under the category of “high risk”?
3.	There is a need to conduct further risk analyses to sort out the consequences for those affected by the legislation on different levels, such as the public sector.
4.	It should be investigated in further detail which issues regarding AI that are suitable for regulation at EU regulation level and which other regulation level. 
5.	The AI Act should be developed in line with other current legislation and legislative proposal in the field of data sharing, data reliability and security.
6.	How is “common normative standards” defined?
7.	Many citizens are sceptical of data collection and data sharing and see it as an intrusion into their lives – proactive information campaigns on how AI works should be planned.
8.	It is unclear whether innovation within the area of cloud services will be encouraged. Will the AI Act encourage the creation of European cloud services to test AI and AI-complience? 

Please see attachment for further elaboration concerning these points mentioned above.",PUBLISHED,large,,COM(2021)206,2021-06-29 12:17:43,anonymous,24212003,closed,,local,authority
pt,2660495,PRT,consumer_organisation,DECO,Paquito,"I. Comentários na generalidade:
1. Numa apreciação geral, pode afirmar-se que a proposta de Regulamento sobre Inteligência Artificial (IA) fica muito aquém das expectativas das organizações de defesa de consumidor e ainda do próprio objetivo da Comissão de tornar a IA em algo em que os cidadãos considerem confiável.

Com efeito, as medidas ora estabelecidas são de alcance demasiado curto, aplicando-se a uma gama muito limitada de usos de IA e, consequentemente, a um conjunto igualmente limitado de problemas suscitados pela IA, não protegendo adequadamente os consumidores de eventuais (mas previsíveis) danos causados por produtos e serviços que integrem IA.

Torna-se por isso essencial alterar a redação desta proposta no sentido de permitir que tanto beneficie o indivíduo como a sociedade como um todo.  

2. Ao longo da proposta é possível identificar uma presunção da Comissão de que a indústria está e irá estar em conformidade com as regras. Ora, com o devido respeito, consideramos que esta é uma abordagem totalmente errada, por estarem em causa avaliações complexas relacionadas com o uso de alto risco de tecnologia de IA, que deveria também levar em consideração como a tecnologia impacta nos direitos e liberdades das pessoas.

A regulamentação dos usos da IA deve ser alicerçada num quadro jurídico sólido que garanta que o desenvolvimento desta tecnologia é feito no respeito dos direitos fundamentais dos consumidores e dos valores europeus. Neste ponto a União Europeia deve mostrar liderança e definir um padrão global.

II. Comentários na especialidade:
1. A proposta encontra-se focalizada em estabelecer regras para aplicações de IA de ""alto risco"", excluindo injustificadamente uma grande variedade de usos de IA que afetam diariamente os consumidores em suas vidas diárias, como é mero exemplo a avaliação de riscos para um seguro saúde, realizada com recurso a algoritmos.

Quanto maior for o potencial dos sistemas algorítmicos para causar danos, quanto mais rigorosos deverão ser os requisitos legais.

2. Igualmente, pretende-se proibir o uso de certas práticas de IA, como por exemplo práticas utilizadas para distorcer o comportamento de um indivíduo. No entanto, para este efeito, apenas é considerado o risco de ""dano físico e psicológico"", deixando de fora outros previsíveis danos ou prejuízos causados aos consumidores, como danos financeiros ou discriminação económica.

3. Acresce que esta proposta é totalmente omissa em relação a direitos do consumidor que deveriam ser especificamente acautelados, designadamente o direito de contestar o resultado de uma decisão algorítmica ou exigir a supervisão humana na tomada de decisão, por exemplo. Tal como é omissa em estabelecer soluções quando ocorrem conflitos. 

Neste ponto, entendemos que um Regulamento sobre IA deve incluir necessariamente um procedimento de reclamação e soluções eficazes, incluindo ações de tutela coletiva.

4. Mais entendemos que de um Regulamento desta natureza deverá ainda fazer parte a revisão de outras matérias relevantes, como por exemplo a responsabilidade civil do produtor e a segurança dos produtos, de forma a estabelecer um quadro jurídico abrangente que garanta um forte conjunto de direitos para os consumidores, nomeadamente a garantia que os produtos e serviços com IA oferecem segurança durante todo o seu tempo útil de vida; proibição de práticas algorítmicas injustas, garantindo que IA e ADM sejam usados de forma justa, transparente e responsável; um sistema de supervisão e fiscalização robusto; e regras de responsabilidade para garantir a compensação ao consumidor, em caso de danos decorrentes de produtos e serviços alimentados por IA.
",PUBLISHED,medium,Carla,COM(2021)206,2021-06-28 16:19:40,withinfo,24212003,closed,005887421809-56,,
en,2660159,DEU,trade_union,German Trade Union Confederation DGB,Suchy,"Feedback

Online Consultation
EU Commission 

ARTIFICIAL INTELLIGENCE ACT
21 April 2021



In principle, the German Trade Union Confederation – DGB – welcomes that the EU Commission generally classifies AI systems as high-risk in the context of labour and employment as well as in education or vocational training and thus makes them subject to special approval conditions. However, it is unclear whether work organised on digital platforms falls within the scope of this regulation. This must be ensured.

DGB critices the EU Commission’s planned massive restriction of the high-risk classification through ‘Annex III’ and calls for AI systems to be generally classified as high-risk if personal information in the employment relationship is affected. This concerns both the area of human resources administration (such as the initiation of employment relationships), including the involvement of social security systems and, in particular, the interaction of employees with AI systems in the work process (e.g. embodied intelligence). It is crucial to prevent delimitation issues that primarily relate to new forms of human-machine interaction or human-robot collaboration or imply algorithmic forms of control.

DGB also calls for the legal exclusion of analysis procedures in the area of HR that turn employees into objects by collecting information that cannot be deliberately controlled (“unacceptable risk” category). In order to prevent the misuse of personal data comprehensive regulations for the protection of employee data must be ensured and, in combination, the participation rights of workers’ representation bodies must be strengthened.


DGB criticises the fact that the EU Commission’s proposal does not include any process requirements for participation and co-determination options for the operational use of AI systems. 

DGB therefore calls für procedural regulation on operational use to enable preventive, non-discriminatory, gender-sensitive and holistic work design, including, in particular, an operational impact assessment (risk management system), the intended testing procedures (Article 7 (5)), the quality management system (Article 17) for sufficient transparency and traceability and continuous evaluation of the learning systems in the organisation and intervention options. Consideration of collective agreements can and should be integrated in a manner that is analogous to the GDPR (Article 88). The impact on operational work processes (employment prospects, profile changes, occupational health and safety, etc.) must be explicitly considered in the ‘risk management system’ required for high-risk applications. 


DGB calls for a right of trade unions to initialise legal actions in the first step for disclosure of the functionalities of the algorithms in AI systems used in companies and the code behind these algorithms. An enforceable right to disclosure for unions is necessary to also be able to examine the data collectively recorded by AI systems and thus avert any damage to employees. It must be ensured that sanctions for employees under labour law that could theoretically result from interaction with AI systems (especially when dealing with proposed decisions) must be excluded in a binding manner.


DGB takes issue with the fact that, according to the proposal of the EU Commission by Article 43 with reference to Annex III for implementation of the requirements for AI providers in the context of work, an establishment of independent bodies and corresponding audits for the area of work has not explicitly been provided for.

The DGB calls for independent AI agencies to be set up on the national level, specifically for the area of labour and employment, to support company stakeholders in consultation, testing, evaluation and complaints, and for these agencies to be equipped with sufficient resources.
",PUBLISHED,large,Oliver,COM(2021)206,2021-06-25 11:28:26,withinfo,24212003,closed,07595112423-87,,
es,2660134,ESP,public_authority,"COLEGIO DE REGISTRADORES DE LA PROPIEDAD, MERCANTILES Y DE BIENES MUEBLES DE ESPAÑA",,"UNA DE LAS DIMENSIONES DE LA INTELIGENCIA ARTIFICIAL ES LA RELATIVA A LA RESPONSABILIDAD POR LOS DAÑOS OCASIONADOS POR LOS ROBOTS. PARA DETERMINAR LA TITULARIDAD DE UN ROBOT O LAS CARGAS HIPOTECARIAS QUE PUEDAN EXISTIR SOBRE EL, ASI COMO QUIEN ES FINALMENTE RESPONSABLE DE LOS DAÑOS QUE CAUSEN, LOS REGISTRADORES ESPAÑOLES CREEMOS QUE LOS ROBOTS, LA IA Y SUS COMPONENTES DEBEN INSCRIBIRSE EN LOS REGISTROS JURÍDICOS.",PUBLISHED,large,,COM(2021)206,2021-06-24 16:01:18,anonymous,24212003,closed,,national,authority
de,2636257,AUT,business_association,Austrian Federal Economic Chamber (WKO),Schmalz,"WKÖ-Positionspapier zum Rechtsrahmen für Künstliche Intelligenz 

Die technische Entwicklung schreitet voran und KI-Anwendungen finden immer vielfältigere Einsatzfelder, daher sind eigene einheitliche Regelungen grundsätzlich zu begrüßen. Um ein Level-Playing-Field zu garantieren, muss jedes KI-System im Binnenmarkt denselben Vorgaben entsprechen. 

Die WKÖ begrüßt grundsätzlich den Plan der Europäischen Kommission („Kommission“), einen Rechtsrahmen für KI zu schaffen. Dieser muss jedoch sicherstellen, dass das Potenzial künstlicher Intelligenz in Zukunft weiter ausgeschöpft werden kann und Rechtssicherheit geschaffen wird, während gleichzeitig ein hohes Schutzniveau gewährleistet wird. 

Wir erachten es als besonders wichtig, dass bereits bestehende Normen und Vorgaben in einer Gesamtbetrachtung einbezogen werden um Doppel- und Überregulierung zu vermeiden. ",PUBLISHED,large,Florian,COM(2021)206,2021-06-22 16:51:08,withinfo,24212003,closed,10405322962-08,,
en,2636210,FIN,business_association,The Federation of Finnish Enterprises,Mikkilä,"The proposal for the Artificial Intelligence Act has admirable objectives but is riddled with issues, which render the regulation problematic.
     
Despite containing some right elements, the proposal is excessively burdensome from the perspective of smaller European enterprises developing, providing, or using AI systems, especially those classified as high-risk systems. Furthermore, given the fact that the evolution and deployment of AI solutions are still at early stages, it may not be advisable to pass legislation as detailed as is now proposed. The apparent risk is that a too heavy-handed approach to AI undermines the competitiveness of SMEs and hampers the growth of the European digital economy.

Therefore, we encourage the EU legislators to carefully take into consideration the regulation’s impacts on innovation and SMEs that provide and use AI systems.    

// The definition of AI system

The definition which emerges from the proposal is too vague and risks expanding the scope of the regulation to include also non-AI software. As this is surely not the intention of the regulation, a more exact definition is required.

// The definition of high-risk AI systems

The risk-based approach to regulating AI is well founded and welcome. However, the creation of a parallel legal framework to the EU product legislation in the form of the AI regulation is problematic, as it would increase both complexity and cost of compliance especially for SMEs. To avoid this, we would advise removing Annex II from the AI regulation and regulate the use of AI systems on a sector-specific basis in the existing EU product legislation. 

When it comes to Annex III, we are skeptical whether the use cases of AI systems outlined in point 4 (Employment, workers management and access to self-employment) meet the high-risk criteria. At the very least, their description would need to be more precise.

We urge the legislators to take further steps in ensuring that especially smaller providers of AI systems have the required certainty and understanding of whether their systems are considered high-risk or not. 

We are concerned about the power given to the Commission to adopt delegated acts to add new types of high-risk AI systems to the regulation. The proposed process does not provide enough predictability and legal certainty and, therefore, needs to be revised. 

// Requirements for high-risk AI systems and the position of small-scale providers and users

Given how extensive and detailed the requirements are, they are likely to overwhelm particularly smaller AI providers. This is unacceptable and goes against the EU’s Think Small First principle. We encourage the legislators to consider whether the requirements of high-risk AI systems could be made less stringent for small-scale providers.

The regulation should identify and define a category of ‘small-scale users’.

// AI regulatory sandboxes

We are in favour of the idea of AI regulatory sandboxes. However, we ask the legislators to form a stronger link between the sandbox scheme and the process through which the AI regulation is amended once it has entered into force. We would also suggest moderating the liabilities of SMEs that choose to take part and experiment in AI regulatory sandboxes. This would encourage them to join sandboxes in greater numbers.

// Codes of conduct and voluntary compliance

Although codes of conduct are welcome, they must not become barriers to markets for the smaller AI players or put them at a competitive disadvantage.

// Implementation and enforcement

We are concerned whether the Member States have the resources to implement the regulation competently, consistently, and even-handedly. 
    
The proposed sanctions seem unreasonably tough from the perspective of smaller AI players. We are particularly concerned about administrative fines, for there is a danger they may be disproportionately imposed on these smaller players.   

[For our full position, see the attachment]",PUBLISHED,medium,Joonas,COM(2021)206,2021-06-21 13:47:24,withinfo,24212003,closed,032592932156-20,,
en,2636199,GBR,company,Engine B,Hackett,"Engine B welcomes the EC’s proposals to harmonise the regulation of AI across the Union. Standardisation will help ensure that negative impacts of AI on humans are consistently prevented while supporting technology firms to engage confidently on a level and consistent playing field. 

Engine B is pleased to see the EC recognises the potential benefits from the safe and ethical adoption of AI, and welcomes simplification proposals. 

The proposed requirements on the safeguards for high-risk AI are good, and we especially welcome the recommendations around high quality data, and bias monitoring, detection and correction. The proposal to expand European common data spaces is particularly welcome; Engine B recommends that corporate data, especially relating to companies which have ceased to trade due to fraud, be included. We further recommend the EC align its data spaces with UK proposals to provide common data repositories for corporate data. 

Engine B welcomes the EC’s commitment to transparency of AI, the documentation requirement, and the requirement to retain ‘human in the loop’ for oversight and governance. We also fully support the requirement that AI systems should be required to convey their limitations and accuracy limits to users. 

Engine B fully supports the recommendation in paragraph 61 on standardisation. We encourage the EC to look into data standards and common data models in particular. The adoption of such common models enables a competitive marketplace where consumers of AI are better able to compare products for quality, and supports good quality governance of technology by making assessment of tools more consistent. By developing common data models in collaboration with technology providers, the EC could significantly aid development of an effective AI oversight industry which works for all. 

Engine B is also encouraged by the proposal for AI regulatory sandboxes, and believes these can be critical to furthering our knowledge of AI and its successful implementation. We especially welcome approaches which support SMEs and start-ups, and would recommend the EC encourage the adoption of common data models and standards within these sandboxes to further enable small companies to innovate.
Engine B also welcomes the proposal to encourage and facilitate the drawing up of codes of conduct. We are concerned though that the current proposal could lead to a proliferation of substandard codes of conduct. It is important for the users of AI and those impacted by it even in low-risk ways that design, implementation, and governance is high quality, and we recommend the EC use its market position to recognise and endorse high quality codes of conduct to which AI providers could voluntarily subscribe. 
 
However, Engine B has a concern that the definition of ‘high risk’ within the proposed regulation is too narrow. The current definition may not cover activities where the poor quality AI may have broad negative societal impacts but where it is hard in advance to predict who would be harmed. 

Our particular concern is for the use of AI in the production of financial and other corporate reporting information and in the provision of assurance services. If poor quality AI is adopted here, we could see major corporate and audit failures which have an enormous impact on markets. As companies increasingly use AI to produce estimates in their corporate reporting, and as major audit providers increasingly use AI to detect fraud and error in this reporting, the risks of AI grow. AI could be used to perpetrate fraud, it could introduce material errors, direct businesses towards poor sustainability behaviours in the belief that they are improving their environmental impact, or in assurance AI could fail to identify significant frauds or risk of disorderly collapse. We recommend amendments to include areas of application where misuse of AI could increase the risk of disorderly business collapse and its resulting economic detriment.
",PUBLISHED,small,Franki,COM(2021)206,2021-06-21 09:37:22,withinfo,24212003,closed,,,
en,2636017,BEL,trade_union,UNI Europa,,"The regulation fails to address the crucial role that social partners and collective bargaining play in the deployment of new technologies at the workplace. Participation of workers’ representatives in the development, implementation and governance of AI systems at the workplace is key to ensure the best protection of workers’ rights. Collective bargaining is an essential tool to address technological change.
An EU framework for AI needs to ensure the right to workers’ involvement, protection of collective bargaining and national legislation providing for better safeguards and collecting bargaining should not be undermined or overruled by EU regulation. We are concerned that this regulation could be used to introduce only minimum standards for workers’ protection. The EU regulation must not override national laws that provide more protection and safeguards for workers as this could lead to deregulation of labour and industrial relations.
We need stronger systematic involvement of employee representatives/trade unions in AI governance. The voluntary approach of the regulation is not enough. We want a structural involvement of unions/workers’ representatives in all stages of the design, development and deployment of AI systems. When AI systems are implemented in the workplace, trade union/workers’ representatives need to be involved in the process. Employees’ rights to information, consultation and participation (according to existing legal frameworks) must be respected and should be part of the mandatory compliance obligations. This is the only way to anticipate the impact on the quality of work, working conditions or dismissals due to automation.  Whilst the regulation only mentions “external experts and observers”, trade unions should be part of the governance of the EU AI Board to ensure a democratic process and the protection of workers’ rights.
The draft regulation mentions high-risk AI-systems in the field of employment, but the list of applications covered is limited to systems used for algorithmic management especially in HR, decision- making and for task allocation. However, other AI applications with possible consequences for employees do not fall under this scope. The list in Annex III is not complete as future AI systems that allow for an extended algorithmic management might not be covered. High-risk applications (especially for surveillance) in the employment area heavily impact health & safety and fundamental rights. The limitation of the scope of high-risk applications is not sufficient. We are concerned about AI systems to infringe privacy/data protection rights as employers can access workers’ data, allow surveillance to take place outside of company premises and outside of working hours, as it invades workers’ homes. Systems can cause bias, incorrect and discriminating AI decisions due to limited data. The precautionary principle should be applied and any AI system that impacts on workers/ working conditions needs to be categorized as high-risk and be subject to a 3rd-party impact assessment by competent authorities with the involvement of trade unions. 
It is vital for workers’ rights that biometric identification and monitoring of workers should not be allowed. Employers should not use biometric identification like face recognition for tracking and controlling employees, whether the workplace is publicly accessible or not. Especially in the context of HR, AI systems are not used to identify a person but to make presumptions and for categorizing employees. A ban of harmful surveillance practices (neuro-surveillance) is necessary. Collective bargaining is crucial to negotiate about the use of monitoring or surveillance technologies at work. It is important to address the collective data rights for workers, not only individual rights. As for human oversight, responsibility should be distributed across the process and its stakeholders. 

(Please find our detailed position on the regulation attached.)
",PUBLISHED,small,,COM(2021)206,2021-06-16 13:22:18,anonymous,24212003,closed,,,
en,2635987,DEU,other,DIN Deutsches Institut für Normung e.V.,GABLER,"We welcome the European Commission's proposal for a regulation that focuses on trustworthiness and product safety of high-risk artificial intelligence (AI) for the European Single Market. AI technology will only be used across the board if it is trustworthy and finds acceptance in society. Standards play an essential role in helping to achieve a broader acceptance of AI systems and bolster trust in them. For example, they can define criteria for quality, explainability, fairness, safety, security, privacy, and transparency. We therefore welcome the fact that the Commission assigns an essential role to standards in the safe placing of AI technology on the European Single Market. We further believe that the AIA proposal paves a way for European experts to significantly shape global standards on AI.

Please find our comments on the draft Artificial Intelligence Act in the attached PDF document.",PUBLISHED,large,Sibylle,COM(2021)206,2021-06-15 15:50:26,withinfo,24212003,closed,989808524267-58,,
en,2635975,FRA,academic_research_instittution,Artificial and Natural Intelligence Toulouse Institute (ANITI),Hidalgo,The attached document presents a critical summary and assessment of the proposed legislation prepared by the Chairs of the Artificial and Natural Intelligence Toulouse Institute (ANITI). The document discusses the limitations of the definition of AI used in the proposed regulation and the potential impact of the regulation in the geography of innovation and artificial intelligence and the competitiveness of Europe in digital technologies. ,PUBLISHED,small,Cesar,COM(2021)206,2021-06-15 11:34:18,withinfo,24212003,closed,,,
fr,2625382,FRA,ngo,Alliance VITA,Roux,"Alliance VITA tient à insister auprès de la Commission pour que celle-ci tienne davantage compte, dans sa proposition de règlement relatif à l’intelligence artificielle, de plusieurs principes-clés relatifs au respect des droits fondamentaux et de la dignité des personnes, en particulier dans le domaine de la médecine et des soins de santé. 

Il convient en effet de rappeler que la montée en puissance de l’Intelligence Artificielle (IA) induit de nombreux enjeux inédits à cet égard.
Ainsi, le développement de robots médicaux vient par exemple réinterroger les critères décisionnels et la place du médecin dans le champ des soins et des traitements médicaux. La survalorisation de l’IA conduirait à réduire l’intelligence humaine à sa facette rationnelle au détriment des intelligences corporelle, relationnelle et spirituelle indispensables à l’humanité. 

La collecte massive de données (Big Data) pose quant à elle la question de l’utilisation des données personnelles et celle du consentement éclairé des personnes. Comme le souligne le Conseil National de l’Ordre des Médecins (Médecins et patients dans le monde des data, des algorithmes et de l’intelligence artificielle, Janvier 2018), l’essor d’une société numérique débridée de toute contrainte pourrait accentuer les inégalités entre les citoyens : « Dans le monde des data et des algorithmes, les citoyens pourraient en arriver, en contrepartie de la facilité des accès à tout (…), à abdiquer d’eux-mêmes leurs libertés. Les populations les plus fragiles y seraient les plus vulnérables ». 

Le principe d’égale jouissance des droits et libertés par les individus ne peut être remis en question du fait de la vulnérabilité de certaines personnes, en particulier lorsque cette vulnérabilité s’avère renforcée par l’IA, comme le rappelle d’ailleurs à juste titre la notice explicative du projet règlement aux points 3.5 et 5.2.2.

Aussi, tel que libellé actuellement, le projet de règlement semble prendre insuffisamment en compte la nécessité d’une protection particulière et effective des droits fondamentaux des personnes, en particulier des personnes dont la vulnérabilité serait encore davantage accrue par le développement de l’IA.
L’attention portée à l’accessibilité des systèmes d’IA aux personnes handicapées (art. 69), de même que l’interdiction des systèmes d’IA qui viseraient à exploiter les vulnérabilités des personnes (art. 5) sont évidemment primordiales.

Il convient toutefois d’aller un pas plus loin en conditionnant effectivement l’ensemble des dispositions du futur règlement au respect du principe éthique fondamental selon lequel les technologies doivent être au service de la personne humaine et de la société. 

En particulier en matière médicale, les critères décisionnels et les algorithmes doivent être communs et lisibles. En tout état de cause, doit demeurer une place à la réflexion intuitive humaine et à une approche éthique des protocoles de recherche.

Le futur règlement doit également veiller à ce que le consentement de la personne soit nécessaire en toute circonstance. Il n’y a pas d’algorithmes pertinents sans données de qualité. Chaque individu est en permanence le générateur de données digitales et ce, la plupart du temps, à son insu. Il est donc primordial de redonner le pouvoir aux individus sur leurs données personnelles avec un libre arbitre.

Enfin, il convient d’intégrer dans le règlement une obligation de non-ingérence. Les potentialités prédictives des technologies peuvent gravement entraver la vie économique et sociale d’une personne dès lors que la probabilité de survenance d’une maladie l’empêcherait de conclure une police d’assurance ou d’obtenir un prêt bancaire, par exemple. Il est donc essentiel d’établir des limites claires et fermes à ne pas dépasser.",PUBLISHED,small,Caroline,COM(2021)206,2021-06-14 16:20:36,withinfo,24212003,closed,458242017765-22,,
en,2596917,GBR,ngo,Public Intelligence organisation,Mathew,,REJECTED,micro,Sherin,COM(2021)206,2021-06-13 13:21:42,withinfo,24212003,closed,,,
en,2522258,POL,eu_citizen,,Karczewska,"The Proposal for an Artificial Intelligence Act includes the following statement: ""A comprehensive ex-ante conformity assessment through internal checks, combined with a strong ex-post enforcement, could be an effective and reasonable solution for those systems, given the early phase of the regulatory intervention and the fact the AI sector is very innovative and expertise for auditing is only now being accumulated."" I would like to point out that both leading international associations that certify auditors have already published guidelines concerning auditing AI: The IIA’s Artificial Intelligence Auditing Framework (in 2017) and ISACA's Auditing Artificial Intelligence (in 2018, revised in 2021). So the expertise is already available.",PUBLISHED,,Joanna,COM(2021)206,2021-06-10 00:38:46,withinfo,24212003,closed,,,
en,2488672,CZE,company,SAZKA Group a.s. ,PEDERIVA,"Considering the space limits, we attach our contribution to the ""Artificial Intelligence - ethical and legal requirements"" consultation 

Kamil Ziegler 
Head of Institutional and Government Affairs ",PUBLISHED,medium,Antonella,COM(2021)206,2021-06-08 16:37:29,withinfo,24212003,closed,343059028884-80,,
en,2334478,ITA,eu_citizen,,,"Although among the purposes of the regulation it is mentioned ensuring people's right to get access to certain essential private services (e.g. financial services), the regulation includes among banking sector high-risk AI systems only those for creditworthiness assessment and omit those for customer due diligence required by EU AML/CFT Directive.
No need to say that AML/CFT risk scoring comes one step earlier than credit risk scoring (before applying for a loan a person should pass successfully through customer due diligence - CDD - that at a growing number of institutions is run by relying on AI systems) and the former can prevent people to get access to financial services more than the latter.
Given that people financial exclusion is an issue of material relevance (see EBA's steps to address ‘de-risking’ practices), and ineffective AI systems being used for CDD may prevent people (especially those belonging to vulnerable categories such immigrants, religious minorities) from getting access to basic financial services and thus increasing the financial exclusion, it is worth to consider the inclusion among category 5 in Annex III of the regulation 'AI systems used for CDD both for onboarding and transaction monitoring'. It follows that AML/CFT competent authorities should be vested with the responsibilities to verify financial institutions' compliance with the provisions laid down in the regulation at hand.
",PUBLISHED,,,COM(2021)206,2021-06-02 16:10:49,anonymous,24212003,closed,,,
en,2324448,NLD,company,The Value Engineers,Wieringa,"The proposed regulation is an excellent initiative but needs improvement to be applicable to the cases for which it is intended.

The definition of AI

The definition of AI in Article 3.1 is contingent on the current state of the art. The Commission has the right to update the list of techniques given in Annex I (Article 4) but on the basis of what criteria would it do so? Radically new techniques such as artificial organic brains would be very different from those techniques. How would the Commission motivate its addition to the list? A criterion for what is an artificial intelligence, is needed. 

Furthermore, the addition that AI systems can, given human-defined objectives, generate outputs such as content, predictions, recommendations, or decisions influencing their environments, does not help. Many systems can do so, including a cruise control of a car and a simple thermostat, which would not be regarded as AI systems.

Proposal: The systems for which this regulation is intended are machines that make decisions for which moral reasoning is required. Moral reasoning involves reasoning about possible benefits and harms to people and about conformance to moral principles. A cruise control does not do any moral reasoning. A system to decide about employee promotion does. 

The regulation does have to be more precise about what exactly moral reasoning is. Annex III gives a list of systems that we can take to be part of the definition. The commission can update this list (Article 7) and motivate its updates in terms of the requirement of moral reasoning needed to take decisions.

Human accountability

The proposed definition above has the advantage that it points out why this regulation is needed at all. It must be clear in advance of using an AI system who is responsible for the decisions made by, or with the help of, the system. This requires a strengthening of the requirement of human oversight in Article 14. If decisions that require moral reasoning are delegated to a machine, a natural person should be accountable for those decisions. This person should be designated before using the system.

Transparency

This in turn requires a strengthening of the requirement of transparency. In order for the responsible (accountable) person to be accountable, that person must have sufficient information about the decision made or recommended by the machine. In the case of data-driven AI systems, they must be aware of limitations of the data sets used, and they must understand the strengths and weaknesses of the choice of error rates in the learning algorithm. In addition, the accountable person must take responsibility for assessment of the similarity of the case at hand to the cases on which the algorithm was trained and tested — this requires moral reasoning too.

“Perfect” data sets

Article 10.3 requires data and training sets to be relevant, representative, free of errors and complete. There are no such data sets. Rather, the user of the system must be aware of imperfections in the training sets: The unavoidable prejudices in the training sample, unavoidable limits to the representativeness of the sample, and unavoidable limits to the construct validity of the measured variables. This is part of the transparency requirement above.

More detail about my improvement proposal can be found at https://www.thevalueengineers.nl/the-eu-ai-regulation-what-are-we-talking-about/ and https://www.thevalueengineers.nl/how-to-deal-with-bias-in-artificial-intelligence/. 
",PUBLISHED,micro,Roel,COM(2021)206,2021-05-12 10:13:45,withinfo,24212003,closed,,,
en,2256824,DEU,academic_research_instittution,NEC Laboratories Europe GmbH,,Test.,PUBLISHED,medium,,COM(2021)206,2021-05-05 12:08:23,anonymous,24212003,closed,,,
en,2256808,BEL,company,Agence du Numérique (AdN),Hublet,"Please find below our feedback on the AI impact assessment. 

- More classical algorithms such as linear regression have been around for many, many years and have almost never caused any damage or frustration. Artificial intelligence is a modern term that scares people, but in reality it is only an applied technology (mathematical, statistical,...) on data. And it is therefore the data and its use that is at the heart of the problem. The most important problem is the data itself and the purpose for which it is used. Technology is a means to an end. 

- Moreover AI is a vague and inconsistent term. The definition of artificial intelligence is a task that needs to be addressed as a matter of priority before to focus on risks. We prefer to talk about augmented intelligence rather than artificial intelligence.  And AI should be seen more as a technology of opportunities than of threat.  For example, it may be relevant to break down AI into sub-elements (data, software, prediction or classification,…), technological sub-domains or scope of use, and to apply a specific regulation to each of them with a priority or risk level. Thus, provide a more structured regulatory approach. 

- The definition of high-risk AI is sometimes too restrictive and does not sufficiently take into account the sectoral context of companies. The approach seems to be premature. It is important to understand the issues on the ground related to AI before regulating and hindering the growth of companies for reasons that do not apply. In order to minimise the constraints of AI regulations and avoid holding back business development, it is essential to focus on high risk at the European level. For low-risk AI, an AI certification system is more relevant than a regulatory system. Companies would then have the choice to certify their model to create a trust mechanism and reassure the end users. 
",PUBLISHED,small,Antoine,COM(2021)206,2021-05-05 09:08:46,withinfo,24212003,closed,,,
fr,2256463,FRA,eu_citizen,,lasou,"Un règlement européen qui va définir des exigences éthiques et juridiques pour les usages de l'IA en Europe.
écrit en anglais, 108 pages + les annexes, études d'impact etc.

A ma connaissance, vous vous adressez aux citoyens européens qui ne comprennent pas tous l'anglais, en particulier quand un jargon spécifique est employé pour définir des règles communes.

Pour ma part, bien que lisant l'anglais, je suis contre ce règlement, uniquement parce qu'il n'a pas été écrit dans une langue que je maîtrise suffisamment pour pouvoir me prononcer.

en publiant ces documents uniquement dans une seule langue, qui ne constitue plus une référence à un des pays de l'UE, vous créez de fait une inégalité dans le traitement, la compréhension et l'acceptation des règles que vous voulez fixer.
",PUBLISHED,,gauthier,COM(2021)206,2021-04-28 07:57:44,withinfo,24212003,closed,,,
en,2242340,DEU,company,SB Science Management UG (haftungsbeschränkt),,"Unfortunately, standardisation measures are still not very common in the life sciences R&D. In the same time, more and more AI solutions are finding their way into life science research and development. Therefore, standardisation between AI and the life sciences should be harmonised too.  A note should be included on the necessity of harmonisation with standardisation measures in the field of natural sciences when AI solutions enters lifesciences R&D. 
",PUBLISHED,micro,,COM(2021)206,2021-04-27 12:05:08,anonymous,24212003,closed,,,
bg,2242316,BGR,eu_citizen,,Кузманов,"Съвсем правилно е набелязан/избран вариант 3+ . Той има предпоставки да направи процеса на регулация най-лесен и бърз, при желание (🤓) на заинтересованите. А и в днешната реалност в ЕС изглежда най-лесно приложим.
Благодаря за вниманието 🙋🏻‍♂️❗",PUBLISHED,,Даниел,COM(2021)206,2021-04-27 08:42:55,withinfo,24212003,closed,,,
